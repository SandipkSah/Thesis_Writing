\documentclass[a4paper,11pt,oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,top=3cm,bottom=3cm,left=3cm,right=3cm]{geometry}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{helvet}
\usepackage[english]{babel}
\usepackage[style=numeric,language=english,sorting=none]{biblatex}
\usepackage{parskip}
\usepackage[margin=1cm]{caption}
\usepackage{booktabs}
\usepackage[pdftex]{graphicx}
\usepackage{subfigure}
\usepackage[pdftex]{hyperref}
\usepackage{amsmath}
\usepackage{array}

\pdfadjustspacing=1

\newcommand{\mylastname}{Sah}
\newcommand{\myfirstname}{Sandip Kumar}
\newcommand{\mynumber}{5589263}
\newcommand{\myname}{\myfirstname{} \mylastname{}}
\newcommand{\mytitle}{A fast implementation of deep neural-network potentials for molecular dynamics simulations of alloys}
\newcommand{\mysupervisor}{Prof. Dr. Felix Höfling}

\hypersetup{
  pdfauthor = {\myname},
  pdftitle = {\mytitle},
  colorlinks = {true},
  linkcolor = {blue}
}

\addbibresource{references.bib}

\begin{document}
\pagenumbering{roman}
\thispagestyle{empty}

\begin{flushright}
  \includegraphics[width=0.35\textwidth]{fub_logo_2.svg.png}
\end{flushright}
\vspace{10mm}


\vspace*{40mm}
\begin{center}
  \huge
  \textbf{\mytitle}
\end{center}
\vspace*{4mm}
\begin{center}
  \Large by
\end{center}
\vspace*{4mm}
\begin{center}
  \LARGE
  \textbf{\myname}
\end{center}
\vspace*{20mm}
\begin{center}
  \Large
  Master Thesis in Computational Science
\end{center}
\vfill
\begin{flushleft}
  \large
  Submission: \today \hfill Supervisor: \mysupervisor \\
  \rule{\textwidth}{1pt}
\end{flushleft}
\begin{center}
  Freie Universität Berlin $|$ Department of Mathematics and Computer Science\\
  Institute of Mathematics
\end{center}

\newpage
\thispagestyle{empty}

\begin{center}
  \Large \textbf{Statutory Declaration}
  \vspace*{8mm}
\end{center}

\begin{center}
  \begin{tabular}{|l|p{85mm}|}
    \hline
    Family Name, Given/First Name & \mylastname, \myfirstname \\
    Matriculation number          & \mynumber                 \\
    Kind of thesis submitted      & Master Thesis             \\
    \hline
  \end{tabular}
  \vspace*{8mm}
\end{center}

\subsection*{English: Declaration of Authorship}

I hereby declare that the thesis submitted was created and written
solely by myself without any external support. Any sources, direct
or indirect, are marked as such. I am aware of the fact that the
contents of the thesis in digital form may be revised with regard to
usage of unauthorized aid as well as whether the whole or parts of
it may be identified as plagiarism. I do agree my work to be entered
into a database for it to be compared with existing sources, where
it will remain in order to enable further comparisons with future
theses. This does not grant any rights of reproduction and usage,
however.

This document was neither presented to any other examination board
nor has it been published.

\subsection*{German: Erklärung der Autorenschaft (Urheberschaft)}

Ich erkläre hiermit, dass die vorliegende Arbeit ohne fremde Hilfe
ausschließlich von mir erstellt und geschrieben worden ist. Jedwede
verwendeten Quellen, direkter oder indirekter Art, sind als solche
kenntlich gemacht worden. Mir ist die Tatsache bewusst, dass der
Inhalt der Thesis in digitaler Form geprüft werden kann im Hinblick
darauf, ob es sich ganz oder in Teilen um ein Plagiat handelt. Ich
bin damit einverstanden, dass meine Arbeit in einer Datenbank
eingegeben werden kann, um mit bereits bestehenden Quellen
verglichen zu werden und dort auch verbleibt, um mit zukünftigen
Arbeiten verglichen werden zu können. Dies berechtigt jedoch nicht
zur Verwendung oder Vervielfältigung.

Diese Arbeit wurde noch keiner anderen Prüfungsbehörde vorgelegt
noch wurde sie bisher veröffentlicht.

\vspace{20mm}

\dotfill\\
Date, Signature

\newpage

\section*{Abstract}
A concise 200–300 word summary of:
\begin{itemize}
  \item Motivation (accelerating atomistic simulations using neural network potentials)
  \item Goal (integrating DeepMD potential calculation into HALMD)
  \item Method (extracting weights, replicating inference in C++/CUDA)
  \item Key results and performance (accuracy vs. speed trade-off)
  \item Conclusions
\end{itemize}

\newpage
\tableofcontents
\clearpage
\pagenumbering{arabic}

% ===============================
\section{Introduction}
\subsection{Motivation}

% Molecular dynamics (MD) simulations are a fundamental computational tool in materials science and condensed matter physics, providing atomistic insight into the structure, thermodynamics, and kinetics of matter. The accuracy of such simulations crucially depends on the quality of the underlying interatomic potential, which defines how atoms interact with one another. Traditionally, two main classes of potentials have been employed: empirical force fields and \emph{ab initio} methods.

% Classical potentials are computationally efficient and allow simulations of systems containing millions of atoms over long timescales. However, their analytical forms are typically limited in flexibility and fail to capture complex bonding environments, charge transfer, or many-body effects, leading to limited transferability beyond the conditions for which they were fitted. 

% In contrast, \emph{ab initio} molecular dynamics (AIMD), typically based on density functional theory (DFT), achieves remarkable accuracy by explicitly solving the electronic structure problem. Yet, this accuracy comes at a prohibitive computational cost, restricting simulations to only a few hundred atoms and short timescales on the order of picoseconds. Bridging the gap between these two extremes—achieving \emph{ab initio}-level accuracy at classical MD efficiency—remains a central challenge in computational materials science.

% Recent advances in machine learning (ML) and, in particular, deep neural networks (DNNs) have led to the emergence of \emph{neural network potentials} (NNPs), which learn the complex mapping between atomic configurations and their corresponding energies and forces from high-fidelity \emph{ab initio} data. Among these approaches, the Deep Potential Molecular Dynamics (DeepMD) framework has demonstrated exceptional accuracy and scalability by constructing descriptors that are invariant under translation, rotation, and permutation of atoms. DeepMD allows one to perform MD simulations with quantum-level accuracy while maintaining computational efficiency close to that of empirical potentials.

% However, despite these advances, integrating such data-driven potentials into high-performance MD frameworks remains challenging. Existing implementations of DeepMD are optimized for standalone usage and rely heavily on TensorFlow-based infrastructures, which are not always ideal for coupling with established GPU-accelerated MD engines. The High-Accuracy Large-scale Molecular Dynamics (HALMD) package provides a powerful alternative: it is a modular, GPU-accelerated simulation framework designed for high precision, extensibility, and large-scale simulations. 

% This motivates the present work, which aims to integrate the DeepMD potential evaluation pipeline directly into HALMD. By embedding a deep neural-network potential within HALMD’s efficient GPU architecture, this thesis seeks to achieve a fast and scalable MD implementation capable of simulating alloy systems with near \emph{ab initio} accuracy.

\subsection{Objectives and Scope}

% The primary objective of this thesis is to integrate a deep neural-network potential, trained using the Deep Potential Molecular Dynamics (DeepMD) framework, into the High-Accuracy Large-scale Molecular Dynamics (HALMD) software. The goal is to enable HALMD to compute interatomic forces and potential energies based on a machine-learned representation of the potential energy surface, thereby combining the quantum-level accuracy of \textit{ab initio} methods with the computational efficiency of classical molecular dynamics.

% To achieve this objective, the first step involves extracting the complete set of parameters—weights, biases, and embedding coefficients—from a trained DeepMD model. These parameters define the neural network architecture responsible for mapping local atomic environments to atomic energy contributions. By reproducing this mapping directly within HALMD, the force and energy predictions can be generated independently of the TensorFlow runtime, allowing for tight integration within HALMD’s C++ and CUDA-based computational framework.

% A major focus of this work is the faithful replication of the DeepMD inference process. This includes implementing the descriptor construction (based on the relative atomic coordinates and cutoff functions), the embedding network, and the fitting network. The network’s outputs are validated against the original DeepMD-kit implementation to ensure consistency in computed energies and forces for test configurations.

% Beyond functional correctness, this project emphasizes computational performance and scalability. The integration is designed to exploit HALMD’s existing GPU-accelerated infrastructure for neighbor-list construction and force computation. This ensures that the deep neural-network potential can be evaluated efficiently for large systems, maintaining simulation throughput comparable to conventional analytical potentials.

% The scope of this study is limited to the inference phase of the DeepMD model—i.e., the evaluation of energies and forces from a pre-trained network. Training of neural-network potentials, data generation from \textit{ab initio} calculations, and retraining of models are outside the scope of this work. The developed implementation is validated through simulations of alloy systems, where accuracy and computational performance are benchmarked against the reference DeepMD-kit outputs.

\subsection{Structure of the Thesis}

This thesis is organized into six main sections, each addressing a distinct aspect of the integration of deep neural-network potentials into the HALMD simulation framework for binary alloy systems.

\textbf{Section 1: Introduction} presents the motivation for employing machine-learned interatomic potentials in molecular dynamics, highlighting the limitations of traditional analytical models and the potential of deep learning to achieve \textit{ab initio}-level accuracy at a fraction of the computational cost. The objectives and scope of the work are defined, followed by an outline of the thesis structure.

\textbf{Section 2: Background} introduces the theoretical and computational foundations of the work. It provides an overview of the HALMD software, describing its modular design, GPU acceleration, and classical molecular dynamics formulation. The section also discusses the Deep Potential Molecular Dynamics (DeepMD) framework, including the principles of neural-network potentials, descriptor construction, and the overall architecture of the Deep Potential model for multi-species systems such as alloys.

\textbf{Section 3: Methodology} details the integration process of the DeepMD potential into HALMD. It explains the extraction of model parameters (weights, biases, and descriptors) from a trained DeepMD model, the reconstruction of the descriptor pipeline (R and G matrices), and the implementation of the neural network inference and force computation within HALMD. Special attention is given to the treatment of multiple atomic species and the efficient GPU-based execution of the potential evaluation.

\textbf{Section 4: Results} presents the outcomes of the integration. This includes validation of the energy and force predictions against reference DeepMD-kit calculations, performance benchmarks of the HALMD implementation on GPU hardware, and case studies of binary alloy systems. The accuracy, scalability, and computational efficiency of the implementation are systematically analyzed.

\textbf{Section 5: Discussion} interprets the results, addressing the accuracy–performance trade-offs, sources of deviation between HALMD and DeepMD outputs, and the numerical stability of the approach. It also highlights the implications of using neural-network potentials in multi-component systems.

\textbf{Section 6: Conclusions and Future Work} summarizes the key contributions of the thesis and discusses possible directions for future development. Potential extensions include optimized GPU kernels, support for additional descriptor types, and extension of the implementation to more complex alloy systems beyond binaries.

Overall, the structure of the thesis is designed to guide the reader from the motivation and theoretical foundations through the detailed implementation and validation of a high-performance, machine-learned potential within the HALMD framework.


% ===============================
\section{Background}
\subsection{HALMD Software}

% The High-Accuracy Large-scale Molecular Dynamics (HALMD) package is a high-performance, open-source molecular dynamics (MD) simulation framework developed to study the microscopic dynamics of liquids, glasses, and other condensed matter systems. HALMD is designed around a modular C++ architecture with a strong emphasis on numerical precision, extensibility, and efficient parallel computation on graphics processing units (GPUs). It provides a versatile platform for conducting large-scale classical molecular dynamics simulations involving millions of particles.

% HALMD employs the \textit{classical molecular dynamics} approach, in which atomic motion is governed by Newton’s equations of motion,
% \[
% m_i \frac{d^2 \mathbf{r}_i}{dt^2} = - \nabla_i U(\mathbf{r}_1, \mathbf{r}_2, \ldots, \mathbf{r}_N),
% \]
% where \( U \) is a predefined potential energy function describing interatomic interactions. The accuracy of a classical MD simulation is determined by the analytical form of this potential. HALMD provides several built-in potentials such as Lennard-Jones, Gaussian Core, and Yukawa models, and also supports tabulated pair potentials that can be customized by the user. These potentials are purely empirical and do not involve electronic structure calculations, distinguishing HALMD from \textit{ab initio} molecular dynamics (AIMD) methods, which derive forces directly from quantum-mechanical computations.

% One of HALMD’s defining features is its high-performance GPU implementation. The software utilizes NVIDIA’s CUDA and, more recently, SYCL for heterogeneous computing to accelerate computationally intensive tasks such as force evaluation, neighbor-list construction, and time integration. Through domain decomposition and spatial cell binning, HALMD efficiently scales to very large systems, achieving orders-of-magnitude speedups compared to single-core CPU implementations. It also supports mixed-precision arithmetic, applying double precision selectively to ensure long-term numerical stability without sacrificing performance.

% HALMD follows a modular design philosophy. Core components—such as integrators, potential functions, and observables—are implemented as interchangeable modules, which can be dynamically configured through a Lua scripting interface. This modularity facilitates the addition of new functionalities, such as custom interaction potentials or data analysis routines, without modifying the main codebase. Simulation data and trajectories are stored in the H5MD format, an HDF5-based standard that ensures compatibility with a wide range of analysis tools.

% These characteristics make HALMD an ideal foundation for extending molecular dynamics simulations with machine-learned interatomic potentials. In this work, HALMD serves as the host MD engine into which the Deep Potential Molecular Dynamics (DeepMD) model is integrated. The goal is to replace conventional analytical potentials with neural-network-based force fields that reproduce \textit{ab initio}-level accuracy while leveraging HALMD’s GPU-accelerated framework for computational efficiency.

\subsection{Neural Network Potentials}

% In recent years, machine learning techniques—particularly deep neural networks (DNNs)—have revolutionized the construction of interatomic potentials for molecular dynamics simulations. Traditional analytical potentials, while computationally efficient, are often limited in their ability to accurately capture many-body interactions and chemical complexities across diverse atomic environments. Neural Network Potentials (NNPs) overcome these limitations by learning the potential energy surface (PES) directly from \textit{ab initio} reference data, typically obtained from density functional theory (DFT) or \textit{ab initio} molecular dynamics (AIMD).

% In the NNP framework, a neural network is trained to approximate the mapping between an atomic configuration and its corresponding total energy and atomic forces. The total potential energy of a system is expressed as a sum of atomic energy contributions predicted by the network:
% \[
% E = \sum_i E_i(\mathcal{R}_i),
% \]
% where \(E_i\) denotes the atomic energy associated with atom \(i\), and \(\mathcal{R}_i\) represents its local atomic environment within a cutoff radius. This decomposition ensures extensivity and locality of the potential, allowing the model to scale efficiently with system size.

% The training of a neural network potential involves minimizing a composite loss function that enforces agreement with the reference \textit{ab initio} data for both energies and forces, and optionally for virial stresses:
% \[
% \mathcal{L} = p_E \frac{1}{N_E} \sum_{n} \left| E_n^{\text{NN}} - E_n^{\text{DFT}} \right|^2
% + p_F \frac{1}{N_F} \sum_{n,i} \left| \mathbf{F}_{i,n}^{\text{NN}} - \mathbf{F}_{i,n}^{\text{DFT}} \right|^2
% + p_V \frac{1}{N_V} \sum_{n} \left| \mathbf{V}_n^{\text{NN}} - \mathbf{V}_n^{\text{DFT}} \right|^2,
% \]
% where \(p_E\), \(p_F\), and \(p_V\) are weighting factors that control the relative importance of energy, force, and virial matching during training. Including forces in the loss function is critical, as it allows the model to capture local curvature of the PES and improves its transferability across different atomic configurations.

% Once trained, the neural network potential can replace the analytical interatomic potential in molecular dynamics simulations. During the inference stage, atomic coordinates are transformed into symmetry-preserving descriptors that are invariant under translation, rotation, and permutation of atoms of the same type. These descriptors serve as inputs to the neural network, which predicts atomic energies and corresponding forces in real time. The resulting simulations can therefore achieve \textit{ab initio}-level accuracy at computational costs comparable to classical MD.

% A variety of NNP architectures have been proposed, including Behler–Parrinello networks, Gaussian approximation potentials (GAP), SchNet, and Neural Equivariant Interatomic Potentials (NequIP). Among these, the Deep Potential Molecular Dynamics (DeepMD) framework has emerged as one of the most widely adopted and computationally efficient implementations, owing to its smooth descriptor formulation, scalability, and native GPU support. The following subsection focuses specifically on \textbf{DeepMD-kit version 2}, which forms the theoretical and computational foundation for the present work.


\subsection{Deep Potential Molecular Dynamics (DeepMD)}

% This work focuses exclusively on \textbf{Deep Potential Molecular Dynamics version 2 (DeepMD-kit v2)}, the second-generation implementation of the Deep Potential framework. DeepMD-kit v2 represents a major advancement over its predecessor, introducing smooth and transferable descriptor formulations, improved multi-species handling, and efficient GPU execution. These features make it particularly suitable for high-accuracy, large-scale molecular dynamics simulations of binary alloy systems within HALMD.

% DeepMD-kit v2 is a machine-learning-based approach that models interatomic interactions with near \textit{ab initio} accuracy at a computational cost comparable to classical molecular dynamics. It employs deep neural networks (DNNs) to learn the mapping between atomic configurations and their corresponding potential energies and forces from quantum-mechanical reference data, typically obtained from density functional theory (DFT).

% At the heart of the Deep Potential formalism lies the decomposition of the total potential energy into atomic energy contributions,
% \[
% E = \sum_i E_i(\mathcal{R}_i),
% \]
% where \(E_i\) is the atomic energy associated with atom \(i\), and \(\mathcal{R}_i\) denotes the local atomic environment of atom \(i\) within a cutoff radius \(r_c\). This locality assumption ensures linear scaling with the number of atoms and enables efficient parallelization on GPUs. Each atomic energy \(E_i\) is predicted by a neural network that takes as input a descriptor encoding the geometric arrangement of neighboring atoms in a manner invariant to translation, rotation, and permutation of atoms of the same type.

% In DeepMD-kit v2, the local environment of each atom is described using the \textit{Smooth Edition} (SE) descriptor, implemented in two variants: the angular form (\texttt{se\_e2\_a}) and the radial form (\texttt{se\_e2\_r}). These descriptors are built upon two intermediate geometric tensors—the \(R\)-matrix and the \(G\)-matrix—which capture both pairwise and many-body correlations. The \(R\)-matrix represents relative positions of neighboring atoms:
% \[
% R_{ij} = \mathbf{r}_j - \mathbf{r}_i,
% \]
% for all neighbors \(j\) within the cutoff radius \(r_c\). The pair distances \(r_{ij} = |R_{ij}|\) are modulated by a smooth cutoff function \(f_c(r_{ij})\) to ensure continuity of both energy and force at the cutoff boundary. From the \(R\)-matrix, a \(G\)-matrix is constructed to incorporate angular dependencies and normalize the geometric information, providing a rotationally and permutationally invariant representation of the local atomic environment.

% The Deep Potential architecture in version 2 consists of two coupled neural networks:
% \begin{enumerate}
%     \item \textbf{Embedding (Filter) Network:} Also referred to as the \textit{filter network}, this smaller feed-forward network acts individually on each neighbor of atom \(i\). It maps raw neighbor information (e.g., distances or angular components) into smooth, high-dimensional features that act as adaptive filters. Each atomic species is associated with its own filter network, enabling the model to capture distinct chemical interactions across multiple elements while preserving permutation invariance within a given species.

%     \item \textbf{Fitting Network:} A larger fully connected neural network that takes the aggregated descriptor vector \(\mathbf{D}_i\), constructed from the outputs of the filter network, and predicts the atomic energy \(E_i\). The same fitting network architecture is used for all atoms of a given species, while different parameter sets are used for different species, allowing accurate modeling of binary or multi-component alloy systems.
% \end{enumerate}

% The total potential energy is obtained as the sum of all atomic energies, and the atomic forces are derived as the negative gradients of the energy with respect to the atomic coordinates:
% \[
% \mathbf{F}_i = -\frac{\partial E}{\partial \mathbf{r}_i}.
% \]
% During training, these gradients are computed automatically through TensorFlow’s backpropagation, while during inference (the focus of this thesis) they are obtained via explicit application of the chain rule through the descriptor and neural network layers.

% A key strength of DeepMD-kit v2 is its explicit treatment of multi-species systems. Each atomic species is assigned its own filter (embedding) network, enabling the model to accurately capture both intra-species and inter-species interactions—such as Cu–Ag or Ni–Al bonds—while maintaining smoothness and transferability across configurations. This makes it especially powerful for modeling binary alloy systems.

% A trained DeepMD-kit v2 model is stored as a TensorFlow \texttt{frozen\_model.pb} file, accompanied by an \texttt{input.json} configuration file. The \texttt{.pb} file contains the trained parameters—weights, biases, and network topology—while the JSON file defines the descriptor type, cutoff radius, activation functions, and precision settings. During inference, atomic coordinates are transformed into descriptors, passed through the species-specific filter networks, and then processed by the fitting network to produce per-atom energies and forces.

% In this thesis, DeepMD-kit v2 serves as the machine-learned potential integrated into the HALMD simulation framework. All parameter extraction, descriptor reconstruction, and neural network inference follow the DeepMD-kit v2 specification precisely. By embedding this inference process into HALMD’s GPU-accelerated infrastructure, the present work enables efficient and scalable molecular dynamics simulations of binary alloy systems with near \textit{ab initio} accuracy.



% ===============================
\section{Methodology}

\subsection{Overview of Implementation}
% Outline of integration workflow between HALMD and DeepMD model.  
% Python-based weight extraction and C++ inference replication.

\subsection{Model Parameter Extraction}
\subsubsection{Frozen Model Structure}
% Description of \texttt{frozen\_model.pb} and \texttt{input.json}.  
% Layers: embedding network, fitting network, descriptor parameters.  

\subsubsection{Extraction Procedure}
% Using TensorFlow or \texttt{saved\_model\_cli} to inspect nodes.  
% Conversion of tensors to readable numpy arrays (weights, biases).  
% Mapping to descriptor and fitting network components.  
% Validation of shape and consistency.

\subsection{Coordinate System Extension}
% Representation of atomic environments in HALMD vs. DeepMD.  
% Neighbor list construction and cutoff scheme.  
% Transformation of Cartesian coordinates to local reference frames.

\subsection{Computation of R and G Matrices}
\subsubsection{R Matrix}
% Definition: pairwise relative position matrix for neighbors.  
% Mathematical formulation:
% \[
% R_{ij} = \mathbf{r}_j - \mathbf{r}_i
% \]
% Application of cut-off function \( f_c(r_{ij}) \).

\subsubsection{G Matrix}
% Construction from normalized pairwise distances.  
% Use of angular information and smoothing functions.  
% Comparison to Behler-Parrinello symmetry functions.

\subsection{Descriptor Computation}
% Role of descriptor as invariant representation of local environment.  
% DeepMD \texttt{se\_e2\_a} or \texttt{se\_e2\_r} descriptor formulation.  
% Equations governing embedding and mapping to fixed-length vectors.  
% Implementation of embedding network using extracted weights.

\subsection{Potential Energy Calculation}
% Definition of per-atom network:
% \[
% E_i = NN(\mathbf{D}_i)
% \]
% Combination of embedding and fitting networks.  
% Implementation of feed-forward inference using weights and biases.  
% Reproduction of DeepMD outputs for test configurations.

\subsection{Force Computation}
\subsubsection{Descriptor Derivative}
% Mathematical derivation:
% \[
% \frac{\partial \mathbf{D}_i}{\partial \mathbf{r}_j}
% \]
% Chain rule through R and G matrices.  
% Symmetry and permutation invariance.

\subsubsection{Network Derivative}
% Backpropagation of gradients:
% \[
% \mathbf{F}_i = - \frac{\partial E}{\partial \mathbf{r}_i}
% = - \sum_j \frac{\partial E}{\partial \mathbf{D}_j} 
% \frac{\partial \mathbf{D}_j}{\partial \mathbf{r}_i}
% \]
% Implementation in C++/CUDA kernels within HALMD.  
% Verification against DeepMD reference outputs.

% ===============================
\section{Results}
\subsection{Verification}
% Comparison of per-atom energy and force between HALMD implementation and DeepMD-kit outputs.  
% Metrics: RMSE, MAE, correlation plots.

\subsection{Performance Evaluation}
% Computational performance (CPU vs GPU).  
% Scaling with number of atoms and neighbor count.  
% Memory and throughput comparison with standard DeepMD runtime.

\subsection{Case Study: Binary Alloy System}
% Description of system (e.g., Cu–Ag or Cu–Au).  
% Simulation setup (temperature, timestep, ensemble).  
% Observed physical properties (radial distribution, energy stability).

% ===============================
\section{Discussion}
% Accuracy vs. performance trade-offs.  
% Numerical stability and precision issues.  
% Possible sources of discrepancy from DeepMD reference.  
% Lessons learned integrating ML potentials into MD engines.

% ===============================
\section{Conclusions and Future Work}
% Summary of contributions.  
% Limitations and optimization possibilities.  
% Future extensions (multi-species descriptor, GPU parallelization, hybrid QM/ML MD).
% Autodiff implementation, different Descriptors, possibly type-embedding


\newpage
\printbibliography

\end{document}
