\documentclass[aspectratio=169]{beamer}
\usetheme{Madrid}
\usecolortheme{default}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{physics}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{circuitikz}
\usetikzlibrary{positioning}
\usepackage[backend=biber,style=numeric]{biblatex}

\usepackage{tikz}
\usepackage{circuitikz}

% \title[DeepMD–HALMD Integration]{Integration of Deep Potential Molecular Dynamics (DeepMD v2) into HALMD for Multi-Species Alloy Systems}
\title[DeepMD–HALMD Integration]{Accurate implementation of DeepMD-v2 potential calculation in HALMD for single species, extension to multi-species and Automatic-Differentiation-Based Force Computation}

\author{Sandip Sah}
\institute{M.Sc. Computational Science, Freie Universität Berlin }
\date{January 07, 2026}

\addbibresource{ppt_ref.bib}


\begin{document}

%=====================================================
% \begin{frame}
% \titlepage
% \end{frame}
\begin{frame}
  \titlepage
  \vspace{1cm}
  \begin{center}
    \large \textbf{Supervisor:} Prof. Dr. Felix Höfling
  \end{center}
\end{frame}

%=====================================================
\begin{frame}[allowframebreaks]{Content}
\setcounter{tocdepth}{2}
\begin{multicols}{2}
\tableofcontents
\end{multicols}
\end{frame}

\section{Introduction}
%=====================================================
\subsection{Motivation}

\begin{frame}[allowframebreaks]{Motivation}
\begin{itemize}
    % \item MD relies on interatomic potentials to approximate quantum-mechanical interactions \cite{frenkel2023understanding}.
    
    \item Classical potentials are efficient but inaccurate , while \textit{ab initio} methods are accurate but computationally expensive \cite{daw1984eam,marx2009ab}.
    
    \item Machine-learned potentials (e.g., DeepMD) achieve near--DFT accuracy at classical MD cost \cite{behler2007generalized,wang2018deepmd}.
    
    \item Current DeepMD--HALMD integration is inaccurate and limited to single-species systems \cite{cruz2025deepmd}.
    
    % \item This thesis aims to accurate multi-species DeepMD simulations in HALMD.
\end{itemize}
\end{frame}



%=====================================================
\subsection{Objectives and Scope}

\begin{frame}{Objectives and Scope}
\begin{itemize}
    \item Extend the DeepMD integration in HALMD beyond the single-species implementation of Cruz to full \textit{multi-species DeepMD-v2} inference \cite{cruz2025deepmd}.
    
    \item Implement the missing core components of the DeepMD-v2 pipeline:
    \begin{itemize}
        \item periodic coordinate handling, ghost atoms, and multi-type neighbor lists,
        \item species-dependent filter and descriptor networks,
        \item normalization, scaling, smooth cutoffs, and force calculations.
    \end{itemize}
    \cite{wang2018deepmd,zeng2023deepmdv2}.
    
    \item Focus on the \textbf{inference stage} (energy and force evaluation) for multi-component materials using the DeepPot-SE descriptor;
    \item Model training and alternative descriptors are outside the scope.
\end{itemize}
\end{frame}


%=====================================================
\section{Background}
\subsection{HALMD}
\begin{frame}{HALMD: High-Accuracy Large-Scale MD Engine}
\begin{itemize}
    \item HALMD is a high-performance, open-source MD framework for large-scale simulations, optimized for numerical accuracy and GPU acceleration \cite{colberg2011highly}.
    
    \item Atomic motion follows Newton’s equations of motion:
    \[
        m_i \frac{d^2 \mathbf r_i}{dt^2} = - \nabla_i U(\mathbf r_1, \mathbf r_2, \ldots, \mathbf r_N),
    \]
    
    \item Traditionally uses efficient  interatomic potentials, but are limited for complex chemistry \cite{daw1984eam,brenner2002reactive}.
    
    \item This thesis extends HALMD with multi-species DeepMD-v2 force fields, enabling near \textit{ab initio} accuracy at classical MD cost \cite{wang2018deepmd,zeng2023deepmdv2}.
\end{itemize}
\end{frame}



\subsection{DeepMD v2}



\begin{frame}{Neural Network Potentials and Deep Potential MD(DeepMD)-v2}
% \begin{frame}[allowframebreaks]{Neural Network Potentials \& DeepMD-v2}
\begin{itemize}
    \item \textbf{Neural Network Potentials} learn interatomic interactions from \textit{ab initio} data, achieving near--DFT accuracy at classical MD cost \cite{behler2007generalized,noe2020mlreview}.
    
    \item Energy is written as a sum of atomic contributions:
    \begin{equation}
        E = \sum_i E_i(\mathcal{R}_i),
    \end{equation}
        
    ensuring locality and linear scaling \cite{behler2007generalized}.
    
    \item \textbf{DeepMD-v2}: modern NNP with smooth, symmetry-preserving descriptors, explicit multi-species support, and efficient GPU execution \cite{wang2018deepmd,zeng2023deepmdv2}.
    
    \item Species-specific embedding and fitting networks predict energies; forces follow from
    \begin{equation}
        \mathbf F_i = -\frac{\partial E}{\partial \mathbf r_i}.
    \end{equation}
\end{itemize}
\end{frame}



%=====================================================

\section{Methodology}
\subsection{Overview of Implementation}
\begin{frame}{Overview of Implementation}
\begin{itemize}
    \item DeepMD-v2 is integrated into HALMD through a modular, end-to-end inference pipeline.

    \item Model parameters are extracted from \texttt{frozen\_model.pb} and \texttt{input.json} and converted into a compact HDF5 format for efficient loading.

    \item HALMD reconstructs local atomic environments using the exact DeepMD-v2 convention:
    \begin{itemize}
        \item periodic coordinate wrapping,
        \item ghost-cell extension,
        \item species-ordered neighbor selection.
    \end{itemize}

    \item From the reconstructed environment, HALMD computes descriptors and evaluates species-specific neural networks to obtain atomic energies and forces.
\end{itemize}
\end{frame}

\begin{frame}{DeepMD--HALMD Energy Evaluation Pipeline}
    \begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        node distance=0.5cm and 0.6cm,
        box/.style={
            draw,
            rectangle,
            rounded corners,
            align=center,
            minimum width=6cm,
            minimum height=1.0cm,
            inner sep=4pt
        },
        arrow/.style={->, thick}
    ]

    % Row 1
    \node[box] (input) {%
    \textbf{Input from HALMD MD Step}\\
    Coordinates $r_i$, species $t_i$, box $\{L_x,L_y,L_z\}$
    };

    \node[box] (env) [right=of input] {%
    \textbf{Coordinate Extension}\\
    Wrap $\rightarrow$ Ghost $\rightarrow$ extended coordinates 
    };

    % Row 2
    \node[box] (R) [below=of env] {%
    \textbf{$R$ Matrix + Normalization}\\
    Compute $R^i_{j}$, normalize
    };

    \node[box] (filter) [left=of R] {%
    \textbf{Filter Networks}\\
    $G^i_{j} =N_{\theta}^{(a,b)}(R^i_{j})$
    };

    % % Row 3
    % \node[box] (G) [below=of filter] {%
    % \textbf{$G$ Matrix Construction}\\
    % Assemble $G_{ij}$
    % };

    \node[box] (D) [below=of filter] {%
    \textbf{Descriptor Construction}\\
    Compute $D^i$ from $G^i$ and $R^i$
    };

    % Row 4
    \node[box] (nn) [right=of D] {%
    \textbf{DeepMD Neural Networks}\\
    $E_i = NN_s(D^i)$
    };

    \node[box] (energy) [below=of nn] {%
    \textbf{Energy Evaluation}\\
    $E_{\text{total}} = \sum_i E^i$
    };



    % Arrows
    \draw[arrow] (input) -- (env);
    \draw[arrow] (env) -- (R);
    \draw[arrow] (R) -- (filter);
    % \draw[arrow] (filter) -- (G);
    \draw[arrow] (filter) -- (D);
    \draw[arrow] (D) -- (nn);
    \draw[arrow] (nn) -- (energy);


    \end{tikzpicture}
    \caption{DeepMD--HALMD energy evaluation pipeline implemented in this thesis.}
    \end{figure}


\end{frame}


\subsection{Model Parameter Extraction}
\begin{frame}{Model Parameter Extraction}
\begin{itemize}
    \item DeepMD-v2 models are stored as \texttt{frozen\_model.pb} with hyperparameters in \texttt{input.json} \cite{wang2018deepmd,zeng2023deepmdv2}.

    \item All parameters are extracted from the TensorFlow graph and converted into a compact HDF5 format for efficient inference in HALMD.

    \item The extracted model contains:
    \begin{itemize}
        \item descriptor attributes (cutoffs, normalization, species mapping),
        \item species-dependent embedding (filter) networks,
        \item species-dependent fitting networks,
        \item final outputs: energies, forces, and virials.
    \end{itemize}

    \item The hierarchical HDF5 model enables full multi-species DeepMD-v2 inference in HALMD, without TensorFlow, extending earlier single-species implementations \cite{cruz2025deepmd}.
\end{itemize}
\end{frame}


\subsection{Coordinate System Extension}

\begin{frame}{Coordinate System Extension}
\begin{itemize}
    \item \textbf{Problem:}  
    HALMD’s native neighbor handling uses the \textit{minimum-image convention}, sufficient for classical MD, but incompatible with DeepMD’s descriptor construction \cite{colberg2011highly,wang2018deepmd,zeng2023deepmdv2}.

    \item \textbf{DeepMD Requirement:}  
    Descriptors are computed from \emph{absolute atomic coordinates} inside the primary cell, requiring explicit:
    \begin{itemize}
        \item coordinate wrapping,
        \item ghost-cell periodic expansion,
        \item species-aware neighbor organization,
        \item descriptor normalization.
    \end{itemize}

    \item \textbf{Implemented Solution:}  
    A new DeePMD-style environment constructor that exactly reproduces the DeepMD-v2 preprocessing pipeline, replacing HALMD’s original environment construction \cite{cruz2025deepmd}.

    \item \textbf{Impact:}  
    Enables correct descriptor evaluation and full compatibility with multi-species DeepMD-v2 models.
\end{itemize}
\end{frame}


\begin{frame}{Explicit Coordinate Wrapping}
\begin{itemize}
    \item DeepMD requires all atoms to be mapped into the primary cell using fractional coordinates:
    \begin{equation}
        \mathbf{s}_i = \mathbf{r}_i \mathbf{H}^{-1}, \qquad
        \tilde{\mathbf{s}}_i = \mathbf{s}_i - \lfloor \mathbf{s}_i \rfloor, \qquad
        \tilde{\mathbf{r}}_i = \tilde{\mathbf{s}}_i \mathbf{H}.
    \end{equation}

    \item This procedure is mandatory for general (non-orthorhombic) simulation cells and matches DeepMD’s internal pipeline \cite{wang2018deepmd}.
\end{itemize}

\vspace{0.3cm}
% \begin{center}
%     \begin{figure}[H]
%     \centering
%     \resizebox{1\textwidth}{!}{%
%     \begin{circuitikz}[transform shape]
%     \tikzstyle{every node}=[font=\tiny]

%     % Outer reference region
%     \draw [dashed] (-2,2) rectangle (2,-2);

%     % Primary simulation cell
%     \draw (-1,1) rectangle (1,-1);

%     % --- Real atoms r_i ---
%     \draw  (-1.5,1.5) circle (0.08cm) node[left] {$r_0$, $\tilde r_0$};
%     \draw  (1.5,1.5) circle (0.08cm) node[right] {$r_3$, $\tilde r_3$};
%     \draw  (-1.5,-1.5)  circle (0.08cm) node[left] {$r_1$, $\tilde r_1$};
%     \draw [dashed] (3.2,-0.5) circle (0.08cm) node[right] {$r_2$};

%     % --- Fractional coordinates s_i ---
%     \draw [dashed] (-0.75,0.75) circle (0.08cm) node[left] {$s_0$, $\tilde s_0$};
%     \draw [dashed] (0.75,0.75)  circle (0.08cm) node[below] {$s_3$, $\tilde s_3$};
%     \draw [dashed] (-0.75,-0.75)  circle (0.08cm) node[left] {$s_1$,$\tilde s_1$};
%     \draw [dashed] (1.6,-0.25) circle (0.08cm) node[right] {$s_2$};

%     % --- Wrapped fractional coordinates
%     \draw (0.6,-0.25) circle (0.08cm) node[above] {$\tilde s_2$};

%     \draw (1.2,-0.5) circle (0.08cm) node[above] {$\tilde r_2$};

%     % --- Mapping arrows ---
%     \draw[->, thin, gray] (-1.5,1.5) -- (-0.75,0.75);
%     \draw[->, thin, gray] (1.5,1.5) -- (0.75,0.75);
%     \draw[->, thin, gray] (-1.5,-1.5) -- (-0.75,-0.75);
%     \draw[->, thin, gray] (3.2,-0.5) -- (1.6,-0.25);

%     \draw[->, thin, blue] (1.6,-0.25) -- (0.6,-0.25);

%     \draw[->, thin, red] (0.6,-0.25) -- (1.2,-0.5);
%     \draw[->, thin, red] (-0.75,0.75) -- (-1.5,1.5);
%     \draw[->, thin, red] (0.75,0.75) -- (1.5,1.5);
%     \draw[->, thin, red] (-0.75,-0.75) -- (-1.5,-1.5);

%     % --- Axis guides ---
%     \draw [dashed] (-2,0) -- (2,0);
%     \draw [dashed] (0,-2) -- (0,2);

%     % --- Dimensions ---
%     \draw [<->, >=Stealth] (-2,2) -- (-2,-2) node[pos=0.5, fill=white] {$L_y=4$};
%     \draw [<->, >=Stealth] (-2,2) -- (2,2) node[pos=0.5, fill=white] {$L_x=4$};

%     \draw [<->, >=Stealth] (-1,1) -- (-1,-1) node[pos=0.5, fill=white] {2};
%     \draw [<->, >=Stealth] (-1,1) -- (1,1) node[pos=0.5, fill=white] {2};

%     \end{circuitikz}
%     }%
%     \caption{Explicit coordinate wrapping illustrated in stages: real coordinates $r_i$, fractional coordinates $s_i=\mathbf r_i \mathbf H^{-1}$, and wrapped fractional coordinates $\tilde s_i$.}
%     \label{fig:coordinate_wrapping}
%     \end{figure}
% \end{center}
\end{frame}


\begin{frame}{Explicit Coordinate Wrapping visualization}

\vspace{0.3cm}

    \begin{figure}[H]
    \centering
    \resizebox{0.40\textwidth}{!}{%
    \begin{circuitikz}[transform shape]
    \tikzstyle{every node}=[font=\tiny]

    % Outer reference region
    \draw [dashed] (-2,2) rectangle (2,-2);

    % Primary simulation cell
    \draw (-1,1) rectangle (1,-1);

    % --- Real atoms r_i ---
    \draw  (-1.5,1.5) circle (0.08cm) node[left] {$r_0$, $\tilde r_0$};
    \draw  (1.5,1.5) circle (0.08cm) node[right] {$r_3$, $\tilde r_3$};
    \draw  (-1.5,-1.5)  circle (0.08cm) node[left] {$r_1$, $\tilde r_1$};
    \draw [dashed] (3.2,-0.5) circle (0.08cm) node[right] {$r_2$};

    % --- Fractional coordinates s_i ---
    \draw [dashed] (-0.75,0.75) circle (0.08cm) node[left] {$s_0$, $\tilde s_0$};
    \draw [dashed] (0.75,0.75)  circle (0.08cm) node[below] {$s_3$, $\tilde s_3$};
    \draw [dashed] (-0.75,-0.75)  circle (0.08cm) node[left] {$s_1$,$\tilde s_1$};
    \draw [dashed] (1.6,-0.25) circle (0.08cm) node[right] {$s_2$};

    % --- Wrapped fractional coordinates
    \draw (0.6,-0.25) circle (0.08cm) node[above] {$\tilde s_2$};

    \draw (1.2,-0.5) circle (0.08cm) node[above] {$\tilde r_2$};

    % --- Mapping arrows ---
    \draw[->, thin, gray] (-1.5,1.5) -- (-0.75,0.75);
    \draw[->, thin, gray] (1.5,1.5) -- (0.75,0.75);
    \draw[->, thin, gray] (-1.5,-1.5) -- (-0.75,-0.75);
    \draw[->, thin, gray] (3.2,-0.5) -- (1.6,-0.25);

    \draw[->, thin, blue] (1.6,-0.25) -- (0.6,-0.25);

    \draw[->, thin, red] (0.6,-0.25) -- (1.2,-0.5);
    \draw[->, thin, red] (-0.75,0.75) -- (-1.5,1.5);
    \draw[->, thin, red] (0.75,0.75) -- (1.5,1.5);
    \draw[->, thin, red] (-0.75,-0.75) -- (-1.5,-1.5);

    % --- Axis guides ---
    \draw [dashed] (-2,0) -- (2,0);
    \draw [dashed] (0,-2) -- (0,2);

    % --- Dimensions ---
    \draw [<->, >=Stealth] (-2,2) -- (-2,-2) node[pos=0.5, fill=white] {$L_y=4$};
    \draw [<->, >=Stealth] (-2,2) -- (2,2) node[pos=0.5, fill=white] {$L_x=4$};

    \draw [<->, >=Stealth] (-1,1) -- (-1,-1) node[pos=0.5, fill=white] {2};
    \draw [<->, >=Stealth] (-1,1) -- (1,1) node[pos=0.5, fill=white] {2};

    \end{circuitikz}
    }%
    \caption{Explicit coordinate wrapping illustrated in stages: real coordinates $r_i$, fractional coordinates $s_i=\mathbf r_i \mathbf H^{-1}$, and wrapped fractional coordinates $\tilde s_i$.}
    \label{fig:coordinate_wrapping}
    \end{figure}

\end{frame}


\begin{frame}{Ghost-Cell Periodic Extension}
\begin{itemize}
    \item \textbf{Problem:}  
    HALMD’s neighbor list applies the minimum-image convention, which is sufficient for classical MD but does \emph{not} provide the full periodic environment required by DeepMD descriptors \cite{colberg2011highly,wang2018deepmd,zeng2023deepmdv2}.

    \item \textbf{DeepMD Requirement:}  
    Every atom must retain a complete neighborhood within the cutoff radius \( r_c \), including neighbors across periodic boundaries.

    \item \textbf{Implemented Solution:}  
    Full \textbf{ghost-cell tiling} — explicit replication of the simulation cell in all spatial directions before neighbor construction.

    \item \textbf{Impact:}  
    Ensures descriptor correctness and identical environments for atoms near boundaries and in the interior, enabling full DeepMD-v2 compatibility.
\end{itemize}
\end{frame}


\begin{frame}[fragile]{Ghost-Cell Construction}
\begin{itemize}
    \item Periodic images are generated from wrapped coordinates:
    \begin{equation}
        \mathbf{r}_i^{(\mathbf{s})}
        =
        \tilde{\mathbf{r}}_i
        +
        s_x L_x\, \mathbf{e}_x
        +
        s_y L_y\, \mathbf{e}_y
        +
        s_z L_z\, \mathbf{e}_z,
        \quad
        s_\alpha \in [-n_{\mathrm{buff},\alpha},\, n_{\mathrm{buff},\alpha}]
    \end{equation}

    \item Buffer size ensures full cutoff coverage:
    \[
        n_{\mathrm{buff},\alpha} = \Big\lceil \frac{r_c}{L_\alpha} \Big\rceil
    \]

    \item For cubic cells with \( n_{\mathrm{buff}} = 1 \):  
    \(3^3 = 27\) replicated cells.
\end{itemize}




\end{frame}





\begin{frame}[fragile]{Ghost-Cell construction visualization}
\vspace{0.25cm}
\begin{center}
    \begin{figure}[H]
    \centering
    \begin{circuitikz}[scale=0.4, transform shape]
    \tikzstyle{every node}=[font=\small]

    % ============================
    % Parameters of the unit cell
    % ============================
    \def\Lx{5}   % width  = 7.5 - 2.5
    \def\Ly{5}   % height = 11 - 6

    % ============================
    % Definition of ONE cell
    % ============================
    \newcommand{\cell}[3][]{%
    \draw[#1] (2.5,11) rectangle (7.5,6);
    \filldraw[#1, fill=black] (3.25,6.5) circle (0.08cm) node {$\tilde{r}^{#2,#3}_{1}$};
    \filldraw[#1, fill=black] (3.25,10.5) circle (0.08cm) node {$\tilde{r}^{#2,#3}_{0}$};
    \filldraw[#1, fill=black] (7,10.5)   circle (0.08cm) node {$\tilde{r}^{#2,#3}_{3}$};
    \filldraw[#1, fill=black] (5.5,8)    circle (0.08cm) node {$\tilde{r}^{#2,#3}_{2}$};
    }

    % ============================
    % Draw center + 8 neighbors
    % ============================
    \foreach \i in {-1,0,1}{
    \foreach \j in {-1,0,1}{
        \begin{scope}[shift={(\i*\Lx,\j*\Ly)}]
        \ifnum\i=0
            \ifnum\j=0
            \cell{\i}{\j}
            \else
            \cell[dotted]{\i}{\j}
            \fi
        \else
            \cell[dotted]{\i}{\j}
        \fi
        \end{scope}
    }
    }

    \end{circuitikz}
    \caption{Periodic replication of the computational cell.}
    \end{figure}

\end{center}
\end{frame}


%=====================================================

\subsection{Construction of Configuration Matrix $R$}

\begin{frame}[allowframebreaks]{Construction of the Configuration Matrix $R$}

\begin{itemize}
    \item \textbf{Inputs:} wrapped central coordinate $\tilde{\mathbf r}_i$ and ghost-cell neighbour images $\mathbf r_j^{(\mathbf s)}$.

    \item \textbf{Relative displacement:}
    \begin{equation}
    \mathbf r_{ij} = \mathbf r_j^{(\mathbf s)} - \tilde{\mathbf r}_i,
    \qquad
    r_{ij} = \|\mathbf r_{ij}\|.
    \end{equation}

    \item \textbf{Smooth cutoff (DeepMD):}
    \begin{equation}
    s(r)=
    \begin{cases}
    \frac{1}{r}, & r<r_s,\\[2pt]
    \frac{1}{r}\Big[x^3(-6x^2+15x-10)+1\Big], & r_s\le r<r_c,\\[2pt]
    0, & r\ge r_c,
    \end{cases}
    \qquad
    x=\frac{r-r_s}{r_c-r_s}.
    \end{equation}

    \item \textbf{Neighbour feature row (4 components):}
    \begin{equation}
    (R^i)_j=\Big[
    s(r_{ij}),\;
    s(r_{ij})\frac{x_{ij}}{r_{ij}},\;
    s(r_{ij})\frac{y_{ij}}{r_{ij}},\;
    s(r_{ij})\frac{z_{ij}}{r_{ij}}
    \Big].
    \end{equation}

    \item \textbf{Species-fixed layout via \texttt{sel}:}
    \begin{equation}
    N_c^{(b)}=\text{max neighbour slots for species }b,\qquad
    \texttt{sel}=[N_c^{(1)},\dots,N_c^{(B)}],
    \end{equation}
    \begin{equation}
    N_c^{(\mathrm{total})}=\sum_{b=1}^{B}N_c^{(b)},\qquad
    R^i\in\mathbb R^{N_c^{(\mathrm{total})}\times 4}.
    \end{equation}
    Neighbours are grouped by species and truncated/zero-padded to match \texttt{sel}.

    \item \textbf{Normalization (training statistics):}
    \begin{equation}
    \hat R^i_{k\alpha}=\frac{R^i_{k\alpha}-t_{\mathrm{avg},\alpha}}{t_{\mathrm{std},\alpha}},
    \qquad
    \hat s_{ij}=\hat R^i_{j,0}\;\;(\text{input to embedding net}).
    \end{equation}
\end{itemize}

\end{frame}



\subsection{Construction of embedding matrix $G$}
\begin{frame}[allowframebreaks]{Construction of the Embedding Matrix $G$}
\begin{itemize}
    \item The DeepPot-SE descriptor maps each normalized inverse distance
    \begin{equation}
        \hat{s}_{ij} = \hat{R}^i_{j,0}
    \end{equation}
    to a learned embedding vector using a species-pair–specific filter network.

    \item For each central–neighbor species pair $(a,b)$, DeepMD-v2 defines a dedicated neural network
    \begin{equation}
        N^{(a,b)}_\theta : \mathbb{R} \rightarrow \mathbb{R}^M,
    \end{equation}
    producing the embedding row
    \begin{equation}
        G^i_{j:} = N^{(a,b(j))}_\theta(\hat{s}_{ij}).
    \end{equation}

    \item All embedding rows are stacked to form
    \begin{equation}
        G^i \in \mathbb{R}^{N_c^{\text{(total)}} \times M},
        \quad
        N_c^{\text{(total)}} = \sum_b N_c^{(b)}.
    \end{equation}

    \item Different neighbor species use different filter networks, enabling chemically accurate multi-species modeling.

    \item HALMD stores the full network family as
    \begin{equation}
        \texttt{networks}[a][b] \equiv N^{(a,b)}_\theta,
    \end{equation}
    and selects the correct one at runtime for each neighbor.
\end{itemize}
\end{frame}



\subsection{Descriptor Computation}
\begin{frame}[allowframebreaks]{Descriptor Computation}
\begin{itemize}
    \item The DeepPot-SE descriptor combines geometric information $\hat{R}^i$ and learned embeddings $G^i$ to form the final descriptor $D^i$ used to predict atomic energy.

    \item After neighbor construction, the normalized geometric matrix is
    \begin{equation}
        \hat{R}^i \in \mathbb{R}^{N_c^{\text{(total)}} \times 4},
    \end{equation}
    containing inverse distances and angular components.

    \item Each row is embedded via a species-pair neural network:
    \begin{equation}
        G^i_{j:} = N^{(a,b(j))}_\theta(\hat{s}_{ij}),
        \qquad
        G^i \in \mathbb{R}^{N_c^{\text{(total)}} \times M}.
    \end{equation}

    \item The quadratic descriptor is then constructed as
    \begin{equation}
        D^i
        =
        \frac{1}{(N_c^{\text{(total)}})^2}
        \bigl((G^i)^{T}\hat{R}^i\bigr)
        \bigl((\hat{R}^i)^{T}G^{i,\text{trunc}}\bigr).
    \end{equation}

    \item Flattening $D^i$ produces the final descriptor vector
    \begin{equation}
        \mathbf D^i \in \mathbb{R}^{MM'},
    \end{equation}
    which is passed to the fitting network to predict energy and forces.
\end{itemize}
\end{frame}



\subsection{Potential Energy calculation}
\begin{frame}[allowframebreaks]{Potential Energy Evaluation}
\begin{itemize}
    \item DeepMD expresses the total energy as a sum of atomic energies:
    \begin{equation}
        E_{\text{tot}} = \sum_{i=1}^N E^i.
    \end{equation}

    \item For each atom $i$ of species $s_i$, the atomic energy is obtained from a
    \textbf{species-dependent fitting network}:
    \begin{equation}
        E^i = \mathrm{NN}_{s_i}(\mathbf D^i) + b_{s_i}.
    \end{equation}

    \item DeepMD-v2 defines one fitting network per species; HALMD reconstructs
    all networks directly from the frozen TensorFlow graph.

    \item The descriptor vector $\mathbf D^i$ is guaranteed to match the exact
    structure expected by each fitting network (ordering, dimension, normalization).

    \item The new HALMD implementation extends previous work from single-species
    to full \textbf{multi-species} DeepMD-v2 inference.
\end{itemize}
\end{frame}

\section{Results}
\subsection{Energy agreement between DeepMD and HALMD}


\begin{frame}{Energy Agreement}
\begin{itemize}
    \item HALMD reproduces DeepMD energies to single-precision floating-point accuracy.

    
    \item Verified for:
    \begin{itemize}
        \item Cu (Model A),
        \item HEA,
        \item Garnet,
        \item Cu (Model B with different statistics).
    \end{itemize}

    \item Agreement holds across:
    \begin{itemize}
        \item increasing system sizes,
        \item chemically complex multi-species systems,
        \item different normalization statistics.
    \end{itemize}

    \item Confirms correctness of:
    \begin{itemize}
        \item descriptor construction,
        \item embedding networks,
        \item fitting networks,
        \item energy bias terms.
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Energy Agreement Across All Models and Species}

\begin{center}
\scriptsize
\begin{tabular}{lccccc}
\hline
\textbf{Model} & \textbf{Species} & \textbf{Atoms} & \textbf{DeepMD (eV)} & \textbf{HALMD (eV)} & \textbf{Match} \\
\hline
Cu A & 1 & 4   & $-6673.84$   & $-6673.84$   & $\checkmark$ \\
Cu A & 1 & 32  & $-53412.4$   & $-53412.4$   & $\checkmark$ \\
Cu A & 1 & 108 & $-180303$    & $-180303$    & $\checkmark$ \\
Cu A & 1 & 256 & $-427425$    & $-427425$    & $\checkmark$ \\
\hline
Cu B & 1 & 4   & $-7.7443$    & $-7.7443$    & $\checkmark$ \\
Cu B & 1 & 32  & $-89.4412$   & $-89.4412$   & $\checkmark$ \\
Cu B & 1 & 108 & $-335.96$    & $-335.96$    & $\checkmark$ \\
Cu B & 1 & 256 & $-836.414$   & $-836.414$   & $\checkmark$ \\
\hline
HEA & 5 & 16  & $-66.7551$ & $-66.7551$ & $\checkmark$ \\
HEA & 5 & 128 & $-895.498$ & $-895.498$ & $\checkmark$ \\
HEA & 5 & 432 & $-3490.86$ & $-3490.86$ & $\checkmark$ \\
\hline
Garnet & 5 & 161 & $-108445$ & $-108445$ & $\checkmark$ \\
\hline
\end{tabular}
\end{center}

\vspace{0.5em}
\begin{itemize}
\item Agreement holds for monoatomic and multi-species systems.
\item Verified across different training statistics and descriptor normalizations.
\item Confirms correctness of full DeepMD-v2 inference in HALMD.
\end{itemize}

\end{frame}


\section{Force Computation}


\subsection{Overview}
\begin{frame}{Overview}
\begin{itemize}
    \item In DeepMD, atomic forces are obtained from the energy by
    \begin{equation}
        \mathbf F_i = -\frac{\partial E_{\text{tot}}}{\partial \mathbf r_i}.
    \end{equation}

    \item The total energy depends on atomic coordinates through multiple stages:
    \[
        \mathbf r 
        \;\rightarrow\; \hat R
        \;\rightarrow\; D 
        % \;\rightarrow\; D 
        \;\rightarrow\; E.
    \]

    \item Force evaluation therefore requires a structured application of the chain rule:
    \begin{equation}
            \frac{\partial E}{\partial \mathbf r}
            =
            \frac{\partial E}{\partial D}
            \frac{\partial D}{\partial \hat R}
            \frac{\partial \hat R}{\partial \mathbf r}.
    \end{equation}

    

    \item HALMD reproduces the DeepMD-v2 derivative pipeline by combining:
    \begin{itemize}
        \item automatic differentiation for all neural networks,
        \item analytic derivatives for geometric and descriptor operations.
    \end{itemize}

    \item This yields forces that match DeepMD-v2 outputs to floating-point precision.
\end{itemize}
\end{frame}


\subsection{Automatic differentiation}

\begin{frame}{Automatic Differentiation (AD): How It Works}

\begin{itemize}
    \item \textbf{Automatic Differentiation (AD)} computes exact derivatives of a program
    by applying the chain rule to each elementary operation.

    \item Key insight: any computation can be decomposed into simple operations
    \[
        +,\; \times,\; \sin,\; \cos,\; \ldots
    \]
    and the chain rule is applied locally to each operation.

    \item Example computation:
    \[
        z = x\,y + \sin(x)
    \]
    is evaluated as a sequence of primitives:
    \[
        a = x y, \quad b = \sin(x), \quad z = a + b.
    \]

    \item AD differentiates this program automatically, without symbolic formulas
    and without numerical approximations.
\end{itemize}

\end{frame}

\begin{frame}{AD: Forward-Mode}

\begin{itemize}
    \item \textbf{Forward-mode AD}
    \begin{itemize}
        \item Propagates derivatives together with values.
        \item Each variable carries its derivative:
        \[
            (x, \dot x),\ (y, \dot y),\ (a, \dot a), \ldots
        \]
        \item One program run gives the derivative w.r.t.\ one input.
        \item Cost scales with number of inputs.
    \end{itemize}

    % \medskip
    % \item \textbf{Reverse-mode AD} (backpropagation)
    % \begin{itemize}
    %     \item First run computes all intermediate values.
    %     \item Second run propagates sensitivities backward.
    %     \item One run yields the full gradient of a scalar output.
    %     \item Cost scales with number of outputs.
    % \end{itemize}

    % \medskip
    % \item This makes reverse-mode AD ideal for
    % \[
    %     E = f(\mathbf{x}), \quad \mathbf{x} \in \mathbb{R}^n,
    % \]
    % which is exactly the structure of neural networks.
\end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]{Forward-Mode AD: Worked Example}

\textbf{Goal:} Compute the derivative of
\[
z = x\,y + \sin(x)
\]
with respect to $x$.

\medskip

\textbf{Step 1: Decompose computation into primitives}
\[
\begin{aligned}
a &= x\,y \\
b &= \sin(x) \\
z &= a + b
\end{aligned}
\]

\medskip

\textbf{Step 2: Attach derivatives to every variable}
\[
(x,\dot x),\ (y,\dot y),\ (a,\dot a),\ (b,\dot b),\ (z,\dot z)
\]

Seed the input derivative:
\[
\dot x = 1, \qquad \dot y = 0.
\]

\medskip

\textbf{Step 3: Propagate derivatives using chain rule}

\[
\begin{aligned}
a &= x y, & \dot a &= y\,\dot x + x\,\dot y = y \\
b &= \sin(x), & \dot b &= \cos(x)\,\dot x = \cos(x) \\
z &= a + b, & \dot z &= \dot a + \dot b = y + \cos(x)
\end{aligned}
\]

\medskip

\textbf{Result:}
\[
\boxed{
\frac{\partial z}{\partial x} = y + \cos(x)
}
\]

\end{frame}

\begin{frame}{AD: Reverse-Mode}
    \begin{itemize}
        \item First run computes all intermediate values.
        \item Second run propagates sensitivities backward.
        \item One run yields the full gradient of a scalar output.
        \item Cost scales with number of outputs.
    \end{itemize}

\medskip
    This makes reverse-mode AD ideal for
    \[
        E = f(\mathbf{x}), \quad \mathbf{x} \in \mathbb{R}^n,
    \]
    which is exactly the structure of neural networks.
\end{frame}



\begin{frame}{Automatic Differentiation (AD) in HALMD}

\begin{itemize}
    \item DeepMD force evaluation requires derivatives through:
    geometry $\rightarrow$ embedding $\rightarrow$ descriptor $\rightarrow$ fitting network.

    \item \textbf{Analytic derivatives} are used for all geometric operations.

    \item \textbf{Automatic differentiation} is used for all neural networks.

    \medskip
    \item \textbf{Current HALMD implementation: Forward-mode AD}
    \begin{itemize}
        \item Used for embedding networks:
        \[
            \hat{s}_{ij} \rightarrow G^i_j, \qquad
            \frac{\partial G^i_j}{\partial \hat{s}_{ij}}.
        \]
        \item Used in fitting network to compute
        \[
            \frac{\partial E_i}{\partial D_{i,\alpha}}.
        \]
    \end{itemize}

    \medskip

    \medskip
    \item HALMD uses a \textbf{hybrid strategy}:
    analytic geometry + AD for neural networks.
\end{itemize}

\end{frame}




% \begin{frame}[allowframebreaks]{AD: Application in HALMD Force Computation}
% \begin{itemize}
%     \item Force computation in DeepMD requires derivatives through geometry,
%     embedding networks, descriptor construction, and fitting networks.

%     \item Analytic differentiation is feasible for geometric operations i.e $\hat R$.
%     \item nonlinear activations and residual connections makes challenging in neural networks.

%     \item HALMD therefore uses \textbf{AD: Application in HALMD Force Computation} for all
%     neural-network components.

%     \item \textbf{Current HALMD implementation: Forward-mode AD}
%     \begin{itemize}
%         \item Efficient when the number of inputs is small.
%         \item Used for embedding networks: 
%         \[
%             \hat{s}_{ij} \rightarrow G^i_j, \quad 
%             \frac{\partial G^i_j}{\partial \hat{s}_{ij}}.
%         \]
%         \item Also used to compute 
%         $\partial E_i / \partial D_{i,\alpha}$ in the fitting network.
%     \end{itemize}

%     % \item \textbf{Future improvement: Reverse-mode AD}
%     % \begin{itemize}
%     %     \item Optimal for scalar outputs with many inputs.
%     %     \item Would greatly accelerate fitting-network differentiation.
%     % \end{itemize}

%     \item HALMD uses a \textbf{hybrid approach}:
%     \begin{itemize}
%         \item analytic derivatives for geometry,
%         \item AD for neural networks.
%     \end{itemize}
% \end{itemize}
% \end{frame}



\subsection{Derivative of geomety matrix $R$}
\begin{frame}{Derivatives of the Geometry Matrix $R$}
\begin{itemize}
    \item Forces require derivatives of each geometry row
    \begin{equation}
        \frac{\partial R^{(i)}_{j\alpha}}{\partial \mathbf r_i}.
    \end{equation}

    \item Each row depends on the relative displacement
    \begin{equation}
        \mathbf r_{ij} = \mathbf r_j - \mathbf r_i, \quad
        r_{ij} = \|\mathbf r_{ij}\|.
    \end{equation}

    \item Geometry features:
    \begin{equation}
        R^{(i)}_{j0} = s(r_{ij}), \qquad
        R^{(i)}_{j\alpha} = s(r_{ij}) \frac{d_\alpha}{r_{ij}}.
    \end{equation}

    \item All derivatives follow directly from the multivariate chain rule.
\end{itemize}
\end{frame}



\subsection{Derivative of embedding matrix $G$}
\begin{frame}{Derivative of the Embedding Matrix $G$}
\begin{itemize}
    \item Each embedding row is produced by a species-dependent network:
    \begin{equation}
        G^{(i)}_{j:} = N^{(a,b)}_\theta(\hat s_{ij}),
        \qquad
        \hat s_{ij} = \hat R^{(i)}_{j0}.
    \end{equation}

    \item Therefore, $G^{(i)}$ depends on geometry only through
    the single scalar $\hat s_{ij}$.

    \item Its derivative reduces to a one–dimensional neural-network derivative:
    \begin{equation}
        \frac{\partial G^{(i)}_{jp}}{\partial \hat R^{(i)}_{j0}}
        =
        \frac{d\, N^{(a,b)}_{\theta,p}}{d \hat s}(\hat s_{ij}),
        \qquad p = 1,\dots,M.
    \end{equation}

    \item All other geometric components do \textbf{not} affect $G$:
    \begin{equation}
        \frac{\partial G^{(i)}_{jp}}{\partial \hat R^{(i)}_{j\alpha}} = 0,
        \qquad \alpha \neq 0.
    \end{equation}
\end{itemize}
\end{frame}


\subsection{descriptor derivative}
% \subsection{Descriptor Structure \& Construction}

\begin{frame}{Descriptor Structure and Construction}
\begin{itemize}
    \item For each atom $i$, the descriptor is built from:
    \[
        \hat R^{(i)} \in \mathbb{R}^{N_c \times 4},
        \qquad
        G^{(i)} \in \mathbb{R}^{N_c \times M}.
    \]

    \item \textbf{Intermediate contraction:}
    \[
        C^{(i)} = \frac{1}{N_c} \big(G^{(i)}\big)^{\mathsf T} \hat R^{(i)},
        \qquad
        C^{(i)} \in \mathbb{R}^{M \times 4}.
    \]

    \item \textbf{Quadratic, permutation-invariant descriptor:}
    \[
        D^{(i)}_{\mathrm{ext}} = C^{(i)} \big(C^{(i)}\big)^{\mathsf T}
        \in \mathbb{R}^{M \times M}.
    \]

    \item \textbf{Truncated descriptor used by the fitting network:}
    \[
        D^{(i)} = D^{(i)}_{\mathrm{ext}}[:,1:M']
        \;=\;
        C^{(i)} \big(C^{(i)}\big)^{\mathsf T} P
        \in \mathbb{R}^{M \times M'}.
    \]
\end{itemize}
\end{frame}



\begin{frame}{Descriptor Derivative: Key Structure}
\begin{itemize}
    \item Descriptor depends on geometry via two paths:
    \[
        \hat R^i \rightarrow C^i \rightarrow D^i,
        \qquad
        \hat R^i \rightarrow G^i \rightarrow C^i \rightarrow D^i.
    \]

    \item Matrix chain rule:
    \[
        \frac{\partial\,\mathrm{vec}(D^i)}{\partial\,\mathrm{vec}(\hat R^i)}
        =
        \underbrace{
        \frac{\partial\,\mathrm{vec}(D^i)}{\partial\,\mathrm{vec}(G^i)}
        \frac{\partial\,\mathrm{vec}(G^i)}{\partial s^i}
        \frac{\partial s^i}{\partial\,\mathrm{vec}(\hat R^i)}
        }_{\text{embedding path}}
        +
        \underbrace{
        \frac{\partial\,\mathrm{vec}(D^i)}{\partial\,\mathrm{vec}(\hat R^i)}
        }_{\text{direct geometry path}}.
    \]
\end{itemize}
\end{frame}

\begin{frame}{Fitting Network Derivative}
\begin{itemize}
    \item Atomic energy:
    \[
        E^i = \mathrm{NN}_{s_i}\!\bigl(\mathrm{vec}(D^i)\bigr)
    \]

    \item Jacobian required for forces:
    \[
        J^i =
        \frac{\partial E^i}{\partial D^i}
        =
        \frac{\partial E^i}{\partial\,\mathrm{vec}(D^i)}
        \in \mathbb{R}^{M \times M'}.
    \]

    \item Deep nonlinear mapping →  
          \textbf{automatic differentiation (AD)}.
\end{itemize}
\end{frame}


\begin{frame}{Final Force Assembly}
\begin{itemize}
    \item Total force:
    \[
        \mathbf F_i = - \sum_j \frac{\partial E^j}{\partial \mathbf r_i}.
    \]

    \item Final factored force expression:
    \[
    \boxed{
    \mathbf F_i
    =
    -\sum_j
    J^j
    \Bigg(
    \underbrace{
    \frac{\partial\,\mathrm{vec}(D_j)}{\partial\,\mathrm{vec}(G_j)}
    \frac{\partial\,\mathrm{vec}(G_j)}{\partial s_j}
    \frac{\partial s_j}{\partial\,\mathrm{vec}(\hat R_j)}
    }_{\text{via embedding } (\hat R \!\rightarrow\! s \!\rightarrow\! G \!\rightarrow\! D)}
    +
    \underbrace{
    \frac{\partial\,\mathrm{vec}(D_j)}{\partial\,\mathrm{vec}(\hat R_j)}
    }_{\text{direct geometry } (\hat R \!\rightarrow\! D)}
    \Bigg)
    \frac{\partial\,\mathrm{vec}(\hat R_j)}{\partial \mathbf r_i}
    }
    \]
\end{itemize}
\end{frame}


\begin{frame}{Force Computation: Hybrid Structure}
\begin{itemize}
    \item \textbf{Neural part:}
    \[
        \frac{\partial\,\mathrm{vec}(D)}{\partial\,\mathrm{vec}(G)},
        \quad
        \frac{\partial\,\mathrm{vec}(G)}{\partial s}
        \quad \text{(via AD)}
    \]

    \item \textbf{Analytic geometry:}
    \[
        \frac{\partial s}{\partial\,\mathrm{vec}(\hat R)},
        \quad
        \frac{\partial\,\mathrm{vec}(\hat R)}{\partial \mathbf r}
    \]

    \item Fully explicit, stable and DeepMD-v2–equivalent force pipeline.
\end{itemize}
\end{frame}






%=====================================================


\section{Conclusion}

\begin{frame}{Conclusion \& Outlook}

\begin{itemize}
    \item First fully explicit \textit{DeepMD-v2 implementation in HALMD} with multi-species descriptors, energies, and force derivatives.

    \item Verified energy agreement for monoatomic and complex multi-species systems enables
    \textit{accurate large-scale ML-driven molecular dynamics} without framework overhead.

    \item This work enables:
    \begin{itemize}
        \item faster ML-based simulations,
        \item improved HPC scalability,
        \item closer integration of ML and physics-based modeling.
    \end{itemize}

    \item With completed forces and further optimisation, HALMD can become a
    \textit{high-performance platform for next-generation materials simulation}.
\end{itemize}

\end{frame}



\begin{frame}[allowframebreaks]{References}
\printbibliography
\end{frame}


%=====================================================
\begin{frame}
\centering
\Huge Thank You!
\end{frame}

\end{document}
