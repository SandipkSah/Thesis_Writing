\documentclass[a4paper,11pt,oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,top=3cm,bottom=3cm,left=3cm,right=3cm]{geometry}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{helvet}
\usepackage[english]{babel}
\usepackage[style=numeric,language=english,sorting=none]{biblatex}
\usepackage{parskip}
\usepackage[margin=1cm]{caption}
\usepackage{booktabs}
\usepackage[pdftex]{graphicx}
\usepackage{subfigure}
\usepackage[pdftex]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}

\pdfadjustspacing=1

\newcommand{\mylastname}{Sah}
\newcommand{\myfirstname}{Sandip Kumar}
\newcommand{\mynumber}{5589263}
\newcommand{\myname}{\myfirstname{} \mylastname{}}
\newcommand{\mytitle}{A fast implementation of deep neural-network potentials for molecular dynamics simulations of alloys}
\newcommand{\mysupervisor}{Prof. Dr. Felix Höfling}

\hypersetup{
  pdfauthor = {\myname},
  pdftitle = {\mytitle},
  colorlinks = {true},
  linkcolor = {black}
}

\addbibresource{references.bib}

\begin{document}
\pagenumbering{roman}
\thispagestyle{empty}

\begin{flushright}
  \includegraphics[width=0.35\textwidth]{fub_logo_2.svg.png}
\end{flushright}
\vspace{10mm}

\vspace*{40mm}
\begin{center}
  \huge
  \textbf{\mytitle}
\end{center}
\vspace*{4mm}
\begin{center}
  \Large by
\end{center}
\vspace*{4mm}
\begin{center}
  \LARGE
  \textbf{\myname}
\end{center}
\vspace*{20mm}
\begin{center}
  \Large
  Master Thesis in Computational Science
\end{center}
\vfill
\begin{flushleft}
  \large
  Submission: {09 January 2026} \hfill Supervisor: \mysupervisor \\
  \rule{\textwidth}{1pt}
\end{flushleft}
\begin{center}
  Freie Universität Berlin $|$ Department of Mathematics and Computer Science\\
  Institute of Mathematics
\end{center}

\newpage
\thispagestyle{empty}

\begin{center}
  \Large \textbf{Statutory Declaration}
  \vspace*{8mm}
\end{center}

\begin{center}
  \begin{tabular}{|l|p{85mm}|}
    \hline
    Family Name, Given/First Name & \mylastname, \myfirstname \\
    Matriculation number          & \mynumber                 \\
    Kind of thesis submitted      & Master Thesis             \\
    \hline
  \end{tabular}
  \vspace*{8mm}
\end{center}

\subsection*{English: Declaration of Authorship}

I hereby declare that the thesis submitted was created and written
solely by myself without any external support. Any sources, direct
or indirect, are marked as such. I am aware of the fact that the
contents of the thesis in digital form may be revised with regard to
usage of unauthorized aid as well as whether the whole or parts of
it may be identified as plagiarism. I do agree my work to be entered
into a database for it to be compared with existing sources, where
it will remain in order to enable further comparisons with future
theses. This does not grant any rights of reproduction and usage,
however.

This document was neither presented to any other examination board
nor has it been published.

\subsection*{German: Erklärung der Autorenschaft (Urheberschaft)}

Ich erkläre hiermit, dass die vorliegende Arbeit ohne fremde Hilfe
ausschließlich von mir erstellt und geschrieben worden ist. Jedwede
verwendeten Quellen, direkter oder indirekter Art, sind als solche
kenntlich gemacht worden. Mir ist die Tatsache bewusst, dass der
Inhalt der Thesis in digitaler Form geprüft werden kann im Hinblick
darauf, ob es sich ganz oder in Teilen um ein Plagiat handelt. Ich
bin damit einverstanden, dass meine Arbeit in einer Datenbank
eingegeben werden kann, um mit bereits bestehenden Quellen
verglichen zu werden und dort auch verbleibt, um mit zukünftigen
Arbeiten verglichen werden zu können. Dies berechtigt jedoch nicht
zur Verwendung oder Vervielfältigung.

Diese Arbeit wurde noch keiner anderen Prüfungsbehörde vorgelegt
noch wurde sie bisher veröffentlicht.

\vspace{20mm}

\dotfill\\
Date, Signature

\newpage

\section*{Abstract}
A concise 200–300 word summary of:
\begin{itemize}
  \item Motivation (accelerating atomistic simulations using neural network potentials)
  \item Goal (integrating DeepMD potential calculation into HALMD)
  \item Method (extracting weights, replicating inference in C++/CUDA)
  \item Key results and performance (accuracy vs. speed trade-off)
  \item Conclusions
\end{itemize}

\newpage
\tableofcontents
\clearpage
\pagenumbering{arabic}

% ===============================
\section{Introduction}
\subsection{Motivation}

Molecular dynamics (MD) simulations play a central role in materials science by providing atomistic insight into the behavior of complex systems. Their predictive power depends on the interatomic potential used to approximate the underlying potential energy surface (PES). Classical empirical potentials are efficient but often too rigid to describe complex bonding environments, whereas \textit{ab initio} methods offer high accuracy at prohibitive computational cost. Machine-learned interatomic potentials---particularly the Deep Potential Molecular Dynamics (DeepMD) framework---bridge this gap by achieving near--quantum mechanical accuracy with classical-MD performance.

The work by Andrés Cruz, titled \textit{``Deep Neural Networks Potentials for Scalable Molecular Dynamics Simulations on Accelerator Hardware''} \cite{cruz2025deepmd}, represents an important step toward integrating Deep Potential models into the high-performance GPU-accelerated simulation engine HALMD. His work reconstructs the DeePMD-kit inference pipeline inside HALMD, extracts network weights from a trained TensorFlow model, and validates energy and force predictions for a \textit{single-species Copper system}. In particular, his implementation focuses on the \textit{two-body embedding DeepPot-SE descriptor} and reproduces the filter and fitting networks \textit{only for a monoatomic system}, making it suitable for single metal.

However, the implementation in \cite{cruz2025deepmd} remains limited to the simplest case of DeepMD-v2 architectures. It does not include multi-species support or the full range of descriptor and network features available in DeepMD-kit v2. 
% \begin{itemize}
%     \item multi-species support,
%     \item species-dependent neighbor counts $N_c(a)$,
%     \item type embedding networks,
%     \item species-aware embedding matrices $G_i$,
%     \item the general multi-species DeepPot-SE (se\_e2\_a) descriptor,
%     \item or complex multi-body interactions required for alloys or chemically diverse systems.
% \end{itemize}

Additionally, several intermediate steps present in the full DeepMD-v2 computational graph---such as normalization layers, ghost body extension for periodicity in boundary condition, species-wise descriptor partitioning, and certain derivative pathways---were simplified or omitted in the previous implementation. Cruz's work successfully established a working single-species DeepMD integration within HALMD.

\textit{This work builds directly on the work of Cruz \cite{cruz2025deepmd} and advances it substantially.} The primary contributions of the present work are:
\begin{enumerate}
    \item Generalizing the HALMD Deep Potential integration to \textit{multi-species, multi-body DeepMD-v2 models}, enabling simulations of binary and multicomponent alloy systems.
    \item Reconstructing descriptor and fitting networks for the \textit{full multi-species DeepPot-SE architecture}, including multi-body descriptor terms.
    \item Improving the \textit{accuracy} of the HALMD implementation by adding several computational steps missing in the previous work, such as proper cutoff normalization, multi-species descriptor algebra, and the full force backpropagation pipeline.
\end{enumerate}

Through these extensions, the present thesis transforms HALMD from supporting a prototype single-component DeepMD potential into a \textit{fully general, high-accuracy, multi-species DeepMD-v2 engine}. This significantly broadens HALMD's applicability, enabling large-scale molecular dynamics simulations of technologically relevant multicomponent materials at near--quantum mechanical accuracy.

\subsection{Objectives and Scope}

The objective of this thesis is to extend and generalize the Deep Potential (DP) implementation in HALMD beyond the single-species, two-body descriptor developed by Cruz \cite{cruz2025deepmd}. While Cruz's work successfully demonstrated that HALMD can evaluate a DeePMD-v2 model for a monoatomic copper system, several components required for full multi-species DeepMD-v2 inference were missing. Most notably, the previous implementation did not include (i) the periodic coordinate extension and neighbor-list construction used in DeepMD \cite{wang2018deepmd}, (ii) normalization and scaling layers defined in the DP-v2 framework \cite{zeng2023deepmdv2}, (iii) species-dependent descriptors, or (iv) descriptor and filter weights that depend simultaneously on the central and neighbor species, as introduced in the multi-species DeepPot-SE descriptor \cite{zeng2023deepmdv2}.

This thesis addresses these limitations by implementing the complete multi-species DeepMD-v2 inference pipeline, including accurate periodic handling of atomic environments. Specifically, the thesis pursues the following objectives:

\begin{enumerate}

    \item \textbf{Implement coordinate normalization, ghost-cell extension, and multi-type neighbor list construction.}  
    The previous implementation did not include the canonical DeePMD preprocessing steps for periodic systems---wrapping coordinates into the primary simulation cell, generating ghost atoms to cover the cutoff radius, and constructing species-grouped neighbor lists---as described in the DeePMD methodology \cite{wang2018deepmd, zeng2023deepmdv2}.  
    Implementing these steps ensures that HALMD reproduces the correct local environments required by the DeepPot-SE descriptors under periodic boundary conditions.

    \item \textbf{Generalize the descriptor pipeline to multi-species systems.}  
    This includes implementing species-dependent neighbor counts $N_c(a)$, species-aware embedding matrices $G_i$, and the full multi-species DeepPot-SE (se\_e2\_a) descriptor introduced in DP-v2 \cite{zeng2023deepmdv2}.

    \item \textbf{Implement species-dependent filter networks.}  
    Extend the single-species embedding and filter networks used in \cite{cruz2025deepmd} to support arbitrary numbers of atomic types, following the species-indexed filter network formulation defined in the DP-v2 architecture \cite{zeng2023deepmdv2}.

    \item \textbf{Reproduce the full DeepMD-v2 inference procedure with higher accuracy.}  
    Incorporate several computational steps omitted in previous work, including normalization layers, descriptor scaling operations, smooth cutoff functions, and full force backpropagation through descriptor derivatives, consistent with the DeepMD-v2 formulation \cite{zeng2023deepmdv2, wang2018deepmd}.

    \item \textbf{Validate energy and force predictions for multi-component systems.}  
    Compare results from HALMD against the DeePMD-kit reference implementation for multi-species models and quantify numerical deviations, following standard validation procedures established in DeepMD literature \cite{wang2018deepmd, zeng2023deepmdv2}.

\end{enumerate}

\noindent\textbf{Scope.}  
This thesis focuses exclusively on the inference stage of DeepMD-v2, i.e., the computation of energies and forces from pre-trained models. Model training is outside the scope of this work. The implementation targets the multi-species DeepPot-SE descriptor and does not cover other descriptor families such as end-to-end or message-passing potentials. The work provides support for multi-species atomic systems relevant to alloys and chemically complex materials, while the underlying molecular dynamics algorithms (integrators, thermostats, barostats) rely on HALMD's existing infrastructure.



% % ===============================
\section{Background}
% \subsection{HALMD Software}

% The High-Accuracy Large-scale Molecular Dynamics (HALMD) package is a high-performance, open-source molecular dynamics (MD) simulation framework developed to study the microscopic dynamics of liquids, glasses, and other condensed matter systems. HALMD is designed around a modular C++ architecture with a strong emphasis on numerical precision, extensibility, and efficient parallel computation on graphics processing units (GPUs)\cite{colberg2011highly}. It provides a versatile platform for conducting large-scale classical molecular dynamics simulations involving millions of particles.

% HALMD employs the \textit{classical molecular dynamics} approach, in which atomic motion is governed by Newton’s equations of motion,
% \[
% m_i \frac{d^2 \mathbf{r}_i}{dt^2} = - \nabla_i U(\mathbf{r}_1, \mathbf{r}_2, \ldots, \mathbf{r}_N),
% \]
% where \( U \) is a predefined potential energy function describing interatomic interactions. The accuracy of a classical MD simulation is determined by the analytical form of this potential. HALMD provides several built-in potentials such as Lennard-Jones, Gaussian Core, and Yukawa models, and also supports tabulated pair potentials that can be customized by the user. These potentials are purely empirical and do not involve electronic structure calculations, distinguishing HALMD from \textit{ab initio} molecular dynamics (AIMD) methods, which derive forces directly from quantum-mechanical computations.

% One of HALMD’s defining features is its high-performance GPU implementation. The software utilizes NVIDIA’s CUDA and, to accelerate computationally intensive tasks such as force evaluation, neighbor-list construction, and time integration. Through domain decomposition and spatial cell binning, HALMD efficiently scales to very large systems, achieving orders-of-magnitude speedups compared to single-core CPU implementations. It also supports mixed-precision arithmetic, applying double precision selectively to ensure long-term numerical stability without sacrificing performance.

% HALMD follows a modular design philosophy. Core components—such as integrators, potential functions, and observables—are implemented as interchangeable modules, which can be dynamically configured through a Lua scripting interface. This modularity facilitates the addition of new functionalities, such as custom interaction potentials or data analysis routines, without modifying the main codebase. Simulation data and trajectories are stored in the H5MD format, an HDF5-based standard that ensures compatibility with a wide range of analysis tools.

% These characteristics make HALMD an ideal foundation for extending molecular dynamics simulations with machine-learned interatomic potentials. In this work, HALMD serves as the host MD engine into which the Deep Potential Molecular Dynamics (DeepMD) model is integrated. The goal is to replace conventional analytical potentials with neural-network-based force fields that reproduce \textit{ab initio}-level accuracy while leveraging HALMD’s GPU-accelerated framework for computational efficiency.


\subsection{HALMD Software}

The High-Accuracy Large-scale Molecular Dynamics (HALMD) package is a high-performance, open-source molecular dynamics (MD) simulation framework designed to study the microscopic dynamics of liquids, glasses, and other condensed matter systems. HALMD is built around a modular C++ architecture with a strong emphasis on numerical precision, extensibility, and efficient parallel computation on graphics processing units (GPUs) \cite{colberg2011highly}. It provides a versatile platform for conducting large-scale classical molecular dynamics simulations involving millions of particles.

HALMD employs the \textit{classical molecular dynamics} approach, in which atomic motion is governed by Newton’s equations of motion,
\[
m_i \frac{d^2 \mathbf{r}_i}{dt^2}
= - \nabla_i U(\mathbf{r}_1, \mathbf{r}_2, \ldots, \mathbf{r}_N),
\]
where \( U \) is a predefined analytical potential describing interatomic interactions. The accuracy of a classical MD simulation depends on the fidelity of this potential. HALMD supports several built-in empirical pair potentials, such as Lennard–Jones, Gaussian Core, and Yukawa models, and additionally allows users to supply tabulated pair potentials. These empirical models do not involve quantum-mechanical calculations and thus differ from \textit{ab initio} molecular dynamics (AIMD), where forces are computed directly from electronic structure methods.

One of HALMD’s defining strengths is its high-performance GPU backend. The software employs NVIDIA’s CUDA to accelerate computationally intensive tasks such as force evaluation, neighbor-list construction, and time integration \cite{colberg2011highly}, and more recent versions extend support to heterogeneous computing platforms through SYCL \cite{halmdGithub}. Spatial domain decomposition and cell binning enable efficient scaling on multi-GPU systems, achieving orders-of-magnitude speedups compared to CPU-only implementations. HALMD also supports mixed-precision arithmetic, selectively applying double precision to critical operations to ensure long-term numerical stability without compromising performance.

HALMD follows a modular design philosophy. Core components—integrators, interaction potentials, neighbor-list builders, and observables—are implemented as interchangeable modules configurable through a Lua scripting interface \cite{halmdManual}. This modularity makes it straightforward to extend HALMD with new functionality, including custom potentials, analysis routines, and data exporters. Simulation data are stored in the H5MD format \cite{debuyl2014h5md}, an HDF5-based standard widely supported by analysis tools in computational physics and chemistry.

These characteristics make HALMD an ideal platform for integrating machine-learned interatomic potentials. Its GPU-accelerated, modular architecture provides the necessary infrastructure to replace conventional analytical potentials with neural-network-based force fields. In this thesis, HALMD serves as the host MD engine into which the Deep Potential Molecular Dynamics (DeepMD) model is embedded. The goal is to achieve \textit{ab initio}-level accuracy within HALMD’s efficient large-scale simulation framework by implementing a full multi-species DeepMD-v2 inference pipeline.

% \subsection{Neural Network Potentials}

% In recent years, machine learning techniques—particularly deep neural networks (DNNs)—have revolutionized the construction of interatomic potentials for molecular dynamics simulations. Traditional analytical potentials, while computationally efficient, are often limited in their ability to accurately capture many-body interactions and chemical complexities across diverse atomic environments. Neural Network Potentials (NNPs) overcome these limitations by learning the potential energy surface (PES) directly from \textit{ab initio} reference data, typically obtained from density functional theory (DFT) or \textit{ab initio} molecular dynamics (AIMD).

% In the NNP framework, a neural network is trained to approximate the mapping between an atomic configuration and its corresponding total energy and atomic forces. The total potential energy of a system is expressed as a sum of atomic energy contributions predicted by the network:
% \[
% E = \sum_i E_i(\mathcal{R}_i),
% \]
% where \(E_i\) denotes the atomic energy associated with atom \(i\), and \(\mathcal{R}_i\) represents its local atomic environment within a cutoff radius. This decomposition ensures extensivity and locality of the potential, allowing the model to scale efficiently with system size.

% The training of a neural network potential involves minimizing a composite loss function that enforces agreement with the reference \textit{ab initio} data for both energies and forces, and optionally for virial stresses:
% \[
% \mathcal{L} = p_E \frac{1}{N_E} \sum_{n} \left| E_n^{\text{NN}} - E_n^{\text{DFT}} \right|^2
% + p_F \frac{1}{N_F} \sum_{n,i} \left| \mathbf{F}_{i,n}^{\text{NN}} - \mathbf{F}_{i,n}^{\text{DFT}} \right|^2
% + p_V \frac{1}{N_V} \sum_{n} \left| \mathbf{V}_n^{\text{NN}} - \mathbf{V}_n^{\text{DFT}} \right|^2,
% \]
% where \(p_E\), \(p_F\), and \(p_V\) are weighting factors that control the relative importance of energy, force, and virial matching during training. Including forces in the loss function is critical, as it allows the model to capture local curvature of the PES and improves its transferability across different atomic configurations.

% Once trained, the neural network potential can replace the analytical interatomic potential in molecular dynamics simulations. During the inference stage, atomic coordinates are transformed into symmetry-preserving descriptors that are invariant under translation, rotation, and permutation of atoms of the same type. These descriptors serve as inputs to the neural network, which predicts atomic energies and corresponding forces in real time. The resulting simulations can therefore achieve \textit{ab initio}-level accuracy at computational costs comparable to classical MD.

% A variety of NNP architectures have been proposed, including Behler–Parrinello networks, Gaussian approximation potentials (GAP), SchNet, and Neural Equivariant Interatomic Potentials (NequIP). Among these, the Deep Potential Molecular Dynamics (DeepMD) framework has emerged as one of the most widely adopted and computationally efficient implementations, owing to its smooth descriptor formulation, scalability, and native GPU support. The following subsection focuses specifically on \textbf{DeepMD-kit version 2}, which forms the theoretical and computational foundation for the present work.


\subsection{Neural Network Potentials}

In recent years, machine learning techniques—particularly deep neural networks (DNNs)—have revolutionized the construction of interatomic potentials for molecular dynamics simulations. Traditional analytical potentials, while computationally efficient, are often limited in their ability to accurately capture many-body interactions and chemical complexities across diverse atomic environments. Neural Network Potentials (NNPs) overcome these limitations by learning the potential energy surface (PES) directly from \textit{ab initio} reference data \cite{behler2007generalized, noe2020mlreview}.

In the NNP framework, a neural network is trained to approximate the mapping between an atomic configuration and its corresponding total energy and atomic forces. The total potential energy of a system is commonly expressed as a sum of atomic contributions \cite{behler2007generalized}:
\[
E = \sum_i E_i(\mathcal{R}_i),
\]
where \(E_i\) denotes the atomic energy associated with atom \(i\), and \(\mathcal{R}_i\) represents its local environment within a cutoff radius. This decomposition ensures extensivity and locality and enables efficient scaling with system size.


Training a neural network potential involves minimizing a loss function that combines
the errors in energies, forces, and optionally virials. Following the formulation used in 
DeePMD-kit~\cite{wang2018deepmd, zeng2023deepmdv2}, the loss is written as
\begin{equation}
\mathcal{L} = p_E L_E + p_F L_F + p_V L_V ,
\end{equation}
where the terms are defined as
\begin{align}
L_E &= \frac{1}{N_E}
\sum_{n=1}^{N_E} 
\left( E_n^{\mathrm{NN}} - E_n^{\mathrm{DFT}} \right)^2, \\
L_F &= \frac{1}{3 N_F}
\sum_{n=1}^{N_F} \sum_{i=1}^{N_{\text{atoms}}}
\left\| \mathbf{F}_{i,n}^{\mathrm{NN}} - \mathbf{F}_{i,n}^{\mathrm{DFT}} \right\|^2, \\
L_V &= \frac{1}{9 N_V}
\sum_{n=1}^{N_V}
\left\| \mathbf{V}_n^{\mathrm{NN}} - \mathbf{V}_n^{\mathrm{DFT}} \right\|^2 .
\end{align}
The factors \(1/3\) and \(1/9\) arise from averaging the force and virial errors
over their three and nine Cartesian components, respectively, ensuring consistent 
normalization across all contributions to the loss.

During inference, atomic coordinates are transformed into symmetry-preserving descriptors that are invariant under translation, rotation, and permutation of atoms of the same type \cite{bartok2013representing, zaheer2017deepsets}. These descriptors serve as input to the neural network, which predicts atomic energies and corresponding forces in real time, achieving \textit{ab initio}-level accuracy at computational cost comparable to classical MD.

A variety of neural-network-based interatomic potentials have been proposed, including Behler–Parrinello networks \cite{behler2007generalized}, Gaussian Approximation Potentials (GAP) \cite{bartok2015gap}, SchNet \cite{schutt2017schnet}, and the E(3)-equivariant NequIP model \cite{batzner2022nequib}. Among these, the Deep Potential Molecular Dynamics (DeepMD) framework has emerged as one of the most widely adopted and computationally efficient implementations due to its smooth descriptor formulation and native GPU acceleration. The following subsection focuses specifically on \textbf{DeepMD-kit version 2}, which forms the theoretical and computational foundation for the present work.


% \subsection{Deep Potential Molecular Dynamics (DeepMD)}

% This work focuses exclusively on \textbf{Deep Potential Molecular Dynamics version 2 (DeepMD-kit v2)}, the second-generation implementation of the Deep Potential framework. DeepMD-kit v2 represents a major advancement over its predecessor, introducing smooth and transferable descriptor formulations, improved multi-species handling, and efficient GPU execution. These features make it particularly suitable for high-accuracy, large-scale molecular dynamics simulations of binary alloy systems within HALMD.

% DeepMD-kit v2 is a machine-learning-based approach that models interatomic interactions with near \textit{ab initio} accuracy at a computational cost comparable to classical molecular dynamics. It employs deep neural networks (DNNs) to learn the mapping between atomic configurations and their corresponding potential energies and forces from quantum-mechanical reference data, typically obtained from density functional theory (DFT).

% At the heart of the Deep Potential formalism lies the decomposition of the total potential energy into atomic energy contributions,
% \[
% E = \sum_i E_i(\mathcal{R}_i),
% \]
% where \(E_i\) is the atomic energy associated with atom \(i\), and \(\mathcal{R}_i\) denotes the local atomic environment of atom \(i\) within a cutoff radius \(r_c\). This locality assumption ensures linear scaling with the number of atoms and enables efficient parallelization on GPUs. Each atomic energy \(E_i\) is predicted by a neural network that takes as input a descriptor encoding the geometric arrangement of neighboring atoms in a manner invariant to translation, rotation, and permutation of atoms of the same type.

% In DeepMD-kit v2, the local environment of each atom is described using the \textit{Smooth Edition} (SE) descriptor, implemented in two variants: the angular form (\texttt{se\_e2\_a}) and the radial form (\texttt{se\_e2\_r}). These descriptors are built upon two intermediate geometric tensors—the \(R\)-matrix and the \(G\)-matrix—which capture both pairwise and many-body correlations. The \(R\)-matrix represents relative positions of neighboring atoms:
% \[
% R_{ij} = \mathbf{r}_j - \mathbf{r}_i,
% \]
% for all neighbors \(j\) within the cutoff radius \(r_c\). The pair distances \(r_{ij} = |R_{ij}|\) are modulated by a smooth cutoff function \(f_c(r_{ij})\) to ensure continuity of both energy and force at the cutoff boundary. From the \(R\)-matrix, a \(G\)-matrix is constructed to incorporate angular dependencies and normalize the geometric information, providing a rotationally and permutationally invariant representation of the local atomic environment.

% The Deep Potential architecture in version 2 consists of two coupled neural networks:
% \begin{enumerate}
%     \item \textbf{Embedding (Filter) Network:} Also referred to as the \textit{filter network}, this smaller feed-forward network acts individually on each neighbor of atom \(i\). It maps raw neighbor information (e.g., distances or angular components) into smooth, high-dimensional features that act as adaptive filters. Each atomic species is associated with its own filter network, enabling the model to capture distinct chemical interactions across multiple elements while preserving permutation invariance within a given species.

%     \item \textbf{Fitting Network:} A larger fully connected neural network that takes the aggregated descriptor vector \(\mathbf{D}_i\), constructed from the outputs of the filter network, and predicts the atomic energy \(E_i\). The same fitting network architecture is used for all atoms of a given species, while different parameter sets are used for different species, allowing accurate modeling of binary or multi-component alloy systems.
% \end{enumerate}

% The total potential energy is obtained as the sum of all atomic energies, and the atomic forces are derived as the negative gradients of the energy with respect to the atomic coordinates:
% \[
% \mathbf{F}_i = -\frac{\partial E}{\partial \mathbf{r}_i}.
% \]
% During training, these gradients are computed automatically through TensorFlow’s backpropagation, while during inference (the focus of this thesis) they are obtained via explicit application of the chain rule through the descriptor and neural network layers.

% A key strength of DeepMD-kit v2 is its explicit treatment of multi-species systems. Each atomic species is assigned its own filter (embedding) network, enabling the model to accurately capture both intra-species and inter-species interactions—such as Cu–Ag or Ni–Al bonds—while maintaining smoothness and transferability across configurations. This makes it especially powerful for modeling binary alloy systems.

% A trained DeepMD-kit v2 model is stored as a TensorFlow \texttt{frozen\_model.pb} file, accompanied by an \texttt{input.json} configuration file. The \texttt{.pb} file contains the trained parameters—weights, biases, and network topology—while the JSON file defines the descriptor type, cutoff radius, activation functions, and precision settings. During inference, atomic coordinates are transformed into descriptors, passed through the species-specific filter networks, and then processed by the fitting network to produce per-atom energies and forces.

% In this thesis, DeepMD-kit v2 serves as the machine-learned potential integrated into the HALMD simulation framework. All parameter extraction, descriptor reconstruction, and neural network inference follow the DeepMD-kit v2 specification precisely. By embedding this inference process into HALMD’s GPU-accelerated infrastructure, the present work enables efficient and scalable molecular dynamics simulations of binary alloy systems with near \textit{ab initio} accuracy.

\subsection{Deep Potential Molecular Dynamics (DeepMD)}

This work focuses on \textbf{Deep Potential Molecular Dynamics version~2 (DeepMD-kit v2)}, the second-generation implementation of the Deep Potential framework. DeepMD-kit v2 represents a major advancement over the original DeepMD formulation~\cite{wang2018deepmd}, introducing improved descriptor smoothness, enhanced multi-species handling, and a refined software architecture that enables highly efficient GPU execution~\cite{zeng2023deepmdv2}. These features make DeepMD-kit v2 particularly suitable for modeling chemically complex systems such as binary alloys within HALMD.

Deep Potential Molecular Dynamics is a machine-learning approach that approximates interatomic interactions with near \textit{ab initio} accuracy while retaining computational efficiency comparable to classical MD. A neural network is trained on quantum-mechanical reference data (typically from DFT or AIMD) to map atomic configurations to energies and forces. The total potential energy of the system is decomposed into atomic energy contributions~\cite{wang2018deepmd, zeng2023deepmdv2}:
\[
E = \sum_{i} E_i(\mathcal{R}_i),
\]
where \(E_i\) is the atomic energy of atom \(i\), and $\mathcal{R}_i$ denotes its local environment within a cutoff radius \(r_c\). This locality assumption yields linear scaling in the number of atoms and enables efficient parallelization on GPUs.

A defining element of DeepMD is the use of symmetry-preserving descriptors that are invariant under translations, rotations, and permutations of atoms of the same type. DeepMD-kit v2 employs the \textit{Smooth Edition} (SE) descriptor family, provided in angular (\texttt{se\_e2\_a}) variants~\cite{zeng2023deepmdv2}. It is constructed from two matrices:
\begin{itemize}
    \item the \textbf{$R$-matrix}, containing relative position vectors \( R_{ij} = \mathbf{r}_j - \mathbf{r}_i \) for neighbors \(j\) within the cutoff radius;
    \item the \textbf{$G$-matrix}, a normalized representation incorporating angular information and many-body geometric correlations.
\end{itemize}
Distances $r_{ij} = |R_{ij}|$ are modulated by a smooth cutoff function to ensure continuity of energies and forces at the cutoff boundary~\cite{wang2018deepmd}.

The Deep Potential model in both DP-v1 and DP-v2 consists of two neural networks: an \textit{embedding (filter) network} and a \textit{fitting network}~\cite{wang2018deepmd, zeng2023deepmdv2}.

\begin{enumerate}
    \item \textbf{Embedding (Filter) Network.}  
    This smaller feed-forward network transforms raw neighbor information into high-dimensional filter features. In DP-v2, a distinct embedding network is assigned to each atomic species, allowing the model to capture chemically specific interactions (e.g., Cu--Ag, Ni--Al) while preserving permutation invariance within each species~\cite{zeng2023deepmdv2}. The embedding network processes each neighbor independently.

    \item \textbf{Fitting Network.}  
    The outputs of the embedding networks are aggregated into a descriptor vector $\mathbf{D}_i$, which is passed to a larger fully connected fitting network that predicts the atomic energy $E_i$. Each species has its own fitting network parameters, enabling accurate modeling of multi-component materials.
\end{enumerate}

Forces are obtained as the negative gradients of the total energy:
\[
\mathbf{F}_i = -\frac{\partial E}{\partial \mathbf{r}_i}.
\]
During training, these derivatives are computed automatically through TensorFlow’s backpropagation engine. During inference---which is the focus of this thesis---the gradients are obtained by applying the chain rule explicitly through the descriptor and neural-network layers, following the DP-v2 computational graph~\cite{zeng2023deepmdv2}.

DeepMD-kit v2 introduces several enhancements that are essential for accurate multi-species modeling. These include species-dependent embedding networks, species-specific filter weights, descriptor normalization layers, and refined smooth cutoff schemes~\cite{zeng2023deepmdv2}. Together, these improvements enable DP-v2 to handle chemically diverse systems with greater smoothness, transferability, and numerical stability compared to its predecessor.

A trained DeepMD model is stored as a TensorFlow \textit{frozen\_model.pb} file along with an accompanying \texttt{input.json} file specifying descriptor types, cutoff radii, hyperparameters, and precision settings. During inference, atomic coordinates are transformed into descriptors, processed by species-specific embedding networks, and fed into the fitting network to produce atomic energies and forces.

In this thesis, DeepMD-kit v2 serves as the machine-learned potential that is fully integrated into HALMD. All parameter extraction, descriptor reconstruction, and neural-network inference follow the DP-v2 specification precisely. By embedding this inference process into HALMD's GPU-accelerated architecture, the present work enables large-scale simulations of multi-species systems, such as binary alloys, with near \textit{ab initio} accuracy.



% ===============================
\section{Methodology}

\subsection{Overview of Implementation}

% The integration of the Deep Potential Molecular Dynamics (DeepMD) model into the High-Accuracy Large-scale Molecular Dynamics (HALMD) framework constitutes the core technical contribution of this thesis. The goal of the implementation is to enable HALMD to evaluate interatomic forces and potential energies directly from a trained DeepMD-kit v2 model—without relying on TensorFlow or the original Python runtime—thus allowing fully GPU-accelerated molecular dynamics simulations of binary alloy systems within HALMD’s native C++ and CUDA environment.

% The implementation process follows a modular workflow that bridges the DeepMD and HALMD architectures. It consists of two main components: a Python-based parameter extraction stage and a C++/CUDA-based inference stage.


\subsection{Model Parameter Extraction}
\subsubsection{Frozen Model Structure}
% Description of \texttt{frozen\_model.pb} and \texttt{input.json}.  
% Layers: embedding network, fitting network, descriptor parameters.  
\subsubsection{Extraction Procedure}

DeepMD-kit stores trained neural network potentials as a TensorFlow \texttt{frozen\_model.pb} file, accompanied by an \texttt{input.json} file containing model hyperparameters. The \texttt{.pb} file encodes all numerical weights, biases, and auxiliary tensors in the form of TensorFlow computation graph nodes, while the JSON file specifies the descriptor type, cutoff radius, number of neurons per layer, activation functions, and species layout. Following the methodology of DeepMD-kit v1 and v2 \cite{wang2018deepmd, zeng2023deepmdv2}, this thesis extracts all necessary model parameters from these files and converts them into an HDF5 representation suitable for efficient inference inside HALMD.

The extraction process begins by loading the TensorFlow graph definition from the \texttt{.pb} file. All tensors of type \texttt{Const} are scanned, and those whose node names match descriptor or fitting-network layers are decoded using TensorFlow’s low-level \texttt{tensor\_util.MakeNdarray}. The DeepMD model contains one set of parameters per atomic species, and the species ordering in the output strictly follows the ordering defined in the DeepMD input configuration. For the work presented here, the descriptor type is always the angular Smooth Edition descriptor \texttt{se\_e2\_a}, which DeepMD-kit v2 uses for multi-species models requiring both radial and angular correlation encoding.

\paragraph{Descriptor Parameters.}
For each species, the descriptor section consists of a stack of fully connected layers with user-specified neuron counts, activation functions, and optional residual time-step connections. The extraction script identifies each descriptor layer via a naming pattern of the form
% \[
% \texttt{filter\_type\_}<s>/\texttt{matrix\_}<k>_<t_n>, \qquad
% \texttt{filter\_type\_}<s>/\texttt{bias\_}<k>_{t_n},
% \]
\[
\mathtt{filter\_type\_}\langle s \rangle/\mathtt{matrix\_}\langle k \rangle\_\langle  t_n \rangle, \qquad
\mathtt{filter\_type\_}\langle s \rangle/\mathtt{bias\_}\langle k \rangle \_{\langle t_n \rangle},
\]
where \(s\) indexes the species, \(k\) is the layer index, and \(t_n\) enumerates neighbor-type channels. For each layer, the script records:
\begin{itemize}
    \item weight matrices,
    \item bias vectors,
    \item number of neurons,
    \item activation function (typically \texttt{tanh} or \texttt{linear} in the present work),
    \item the presence of a residual network branch.
\end{itemize}
DeepMD-v2 allows descriptor layers to use residual updates when \texttt{resnet\_dt=true}. In that case, a per-layer time-step tensor is extracted and marked in the output structure, though the use of residual updates depends on the model’s training configuration.

\paragraph{Embedding (Filter) Network.}
In the DeepMD architecture, the descriptor network is conceptually equivalent to the “filter” or embedding network described in \cite{wang2018deepmd, zeng2023deepmdv2}. Each species has its own filter-network parameters to account for species-dependent geometric correlations. The extracted filter-network weights are reshaped into a row-major layout compatible with HALMD’s GPU evaluation kernels.

\paragraph{Fitting Network.}
For each species, the fitting network (also called the main network) predicts the atomic energy contribution \(E_i\). The extraction process retrieves:
\begin{itemize}
    \item all intermediate fitting layers,
    \item species-dependent activation functions,
    \item weight and bias tensors for each layer,
    \item the species-specific atomic energy bias term \texttt{bias\_atom\_e},
    \item the parameters of the final output layer.
\end{itemize}
The fitting-network layers are identified using a template of the form
\[
\texttt{layer\_LL\_type\_}<k>_<s>/\texttt{matrix}, \qquad 
\texttt{layer\_LL\_type\_}<k>_<s>/\texttt{bias},
\]
and similarly for the final layer \texttt{final\_layer\_type\_<s>}. The final layer uses a fixed activation function, typically \texttt{linear}, consistent with the DeepMD energy formulation.

\paragraph{Normalization Parameters.}
DeepMD-v2 introduces descriptor normalization tensors \texttt{t\_avg} and \texttt{t\_std}, which are required to maintain numerical stability and descriptor smoothness. These tensors are extracted from the nodes
\[
\texttt{descrpt\_attr/t\_avg}, \qquad
\texttt{descrpt\_attr/t\_std},
\]
and stored in the HDF5 output so that HALMD can apply the same normalization as the original DeepMD model.

\paragraph{Organization and Output Format.}
All extracted parameters are written into a structured HDF5 file using a hierarchical layout:
\begin{itemize}
    \item global descriptor constants (cutoff radii, smoothing radii, normalization tensors),
    \item per-species descriptor network parameters,
    \item per-species fitting network parameters.
\end{itemize}
This structure mirrors the multi-species design of DeepMD-kit v2 and makes the extracted parameters directly usable by HALMD for inference. Unlike the single-species extraction used in the previous HALMD implementation \cite{cruz2025deepmd}, the present work extracts and stores fully species-resolved descriptor and fitting-network data, which are required for accurate multi-species DeepMD-v2 inference.

The resulting HDF5 file serves as the unified model representation for HALMD. During the simulation, HALMD loads this file, reconstructs the descriptor and neural networks using GPU-optimized data structures, and performs inference without relying on TensorFlow. This enables the Deep Potential model to be evaluated natively within HALMD’s simulation loop with high performance and full multi-species support.



\subsection{Coordinate System Extension }
\label{sec:env_construction}

A central contribution of this thesis is the redesign of HALMD’s environment–construction pipeline so that it follows the exact conventions required by DeepMD-v2. The earlier implementation by Cruz \cite{cruz2025deepmd} relied on HALMD’s built-in periodic boundary handling and neighbour list, which correctly satisfy the minimum-image convention used in classical MD \cite{colberg2011highly}. However, this approach provides only the displacements between atoms and does not reproduce the full periodic environment expected by the Deep Potential (DP) descriptor pipeline. DeepMD-v2 uses a stricter definition of the local atomic environment, requiring explicit coordinate wrapping, ghost-cell expansion, species-aware neighbour grouping, and descriptor normalization \cite{wang2018deepmd, zeng2023deepmdv2}. These steps were not included in the previous HALMD implementation and form the foundation of the extended environment-construction procedure in this thesis.

The new DeePMD-style environment constructor introduced in this work consists of the following components:

\paragraph{1. Explicit coordinate wrapping.}
DeepMD assumes that all atomic coordinates are wrapped into the primary simulation cell before any descriptor computation. Although HALMD internally tracks particle images, its neighbour-displacement logic does not enforce wrapped coordinates. The new implementation applies explicit wrapping:
\[
\tilde{r}_{i\alpha} = r_{i\alpha} - L_\alpha \left\lfloor \frac{r_{i\alpha}}{L_\alpha} \right\rfloor,
\]
ensuring strict consistency with DeepMD’s periodic coordinate convention.

\paragraph{2. Ghost-cell periodic extension.}
DeepMD constructs local atomic environments by tiling the simulation box with enough periodic images to completely cover the descriptor cutoff radius \(r_c\). This requirement is stricter than HALMD’s neighbour list, which only returns the nearest periodic image of each atom. To satisfy the descriptor definition, this thesis implements full ghost-cell extension:
\[
\mathbf{r}_i^{(\mathbf{s})} = \mathbf{r}_i + s_x L_x \mathbf{e}_x + s_y L_y \mathbf{e}_y + s_z L_z \mathbf{e}_z,
\quad
s_\alpha \in [-n_{\mathrm{buff},\alpha}, n_{\mathrm{buff},\alpha}],
\]
where \(n_{\mathrm{buff},\alpha} = \lceil r_c / L_\alpha \rceil\).  
This guarantees that atoms near the boundary have complete environments identical to those produced by DeepMD-kit.

\paragraph{3. Species-aware neighbour grouping via the \texttt{sel} vector.}
DeepMD-v2 requires that neighbours be grouped first by species and only then sorted by distance, with the number of neighbours per species fixed by the descriptor configuration:
\[
\texttt{sel} = [N_c(a_0), N_c(a_1), \dots].
\]
HALMD’s native neighbour list is species-blind and distance-only. The extended implementation introduced here constructs:
\[
\text{neighbors\_by\_type}[a] = \{\, j ~|~ t_j = a,\ r_{ij} < r_c \,\},
\]
sorts each group by distance, and fills descriptor rows in the exact ordering expected by the DP-v2 architecture.  
This step is essential for multi-species DeepPot-SE descriptors.

\paragraph{4. Environment-matrix construction interface (without descriptor math).}
In the previous HALMD implementation, the environment was computed directly using the neighbour list and minimum-image displacements. In this thesis, environment construction is redesigned to operate on the wrapped and ghost-extended coordinates, and on the species-grouped neighbour lists described above.  
The computation of the descriptor tensors themselves (such as the R and G matrices) is performed in a later stage and is documented separately in Section~3.4.

\paragraph{5. Descriptor normalization using \texttt{t\_avg} and \texttt{t\_std}.}
DeepMD-v2 applies per-feature normalization using statistics computed during training:
\[
X' = \frac{X - t_\mathrm{avg}}{t_\mathrm{std}},
\]
to improve numerical stability. Cruz’s implementation did not include this step. The extended HALMD implementation integrates normalization directly into the environment-construction pipeline, ensuring full compatibility with the DP-v2 descriptor specification.

\paragraph{Summary of improvements.}
Relative to the previous work, the environment construction introduced in this thesis adds:
\begin{itemize}
    \item strict DeePMD-style coordinate wrapping,
    \item full periodic ghost-cell expansion,
    \item species-aware neighbour grouping according to the DP \texttt{sel} configuration,
    \item a consistent environment-construction workflow aligned with DeepMD-v2,
    \item descriptor normalization compatible with training-time statistics.
\end{itemize}

These extensions form the necessary foundation for computing the R and G descriptor matrices (Section~3.4) and enable HALMD to accurately evaluate multi-species DeepMD-v2 models.





% \subsection{Computation of R and G Matrices}
% \subsubsection{R Matrix}

\subsection{Computation of the \texorpdfstring{$R$}{R} and \texorpdfstring{$G$}{G} Matrices}

The DeepPot-SE (DP-SE) descriptor is constructed in two stages.  
First, the geometric matrix~$R$ encodes neighbour geometry using inverse-distance--based quantities.  
Second, the embedding matrix~$G$ transforms these geometric inputs through species-dependent neural networks.  
Together, these matrices form the basis of the final DeepMD-v2 descriptor.

% Throughout this section we employ the standard DeepMD-v2 terminology:
% \begin{itemize}
%     \item raw inverse distance: \( s_{ij} = 1 / r_{ij} \),
%     \item switched inverse distance: \( \tilde{s}_{ij} = s_{ij} f_{\mathrm{sw}}(r_{ij}) \),
%     \item normalized inverse distance: \( \hat{s}_{ij} \),
%     \item raw geometric row: \( R_{ij} \),
%     \item normalized geometric row: \( \hat{R}_{ij} \),
%     \item embedding vector: \( G_{ij} \).
% \end{itemize}

The implementation presented here reproduces the DeepMD-v2 specification exactly
\cite{wang2018deepmd,zeng2023deepmdv2} and corrects limitations of the earlier
HALMD prototype~\cite{cruz2025deepmd}.

% ---------------------------------------------------------
\subsubsection{R-Matrix Construction}
\label{sec:R_matrix_construction}

The geometric matrix \( R \) encodes the relative positions of selected neighbours of each
central atom and is designed to be invariant under translation, rotation, and permutation of
atoms of the same species \cite{wang2018deepmd,zeng2023deepmdv2}.  
Each neighbour contributes one row to the raw \(R\)-matrix.

\paragraph{Relative displacement and inverse-distance quantities.}
For a central atom \(i\) and neighbour \(j\),
\[
\mathbf{r}_{ij} = \mathbf{r}_j - \mathbf{r}_i,
\qquad
r_{ij} = \lVert \mathbf{r}_{ij} \rVert.
\]
DeepMD defines the raw geometric row as
\[
R_{ij}
=
\left[
    s_{ij},\;
    \frac{x_{ij}}{r_{ij}^{2}},\;
    \frac{y_{ij}}{r_{ij}^{2}},\;
    \frac{z_{ij}}{r_{ij}^{2}}
\right]
f_{\mathrm{sw}}(r_{ij}),
\]
where \( s_{ij} = 1 / r_{ij} \) and  
\[
\tilde{s}_{ij} = s_{ij} f_{\mathrm{sw}}(r_{ij})
\]
is the cutoff-weighted inverse distance used in the descriptor.

The switching function \(f_\mathrm{sw}(r)\) smoothly suppresses neighbour contributions near the cutoff:
\[
f_{\mathrm{sw}}(r)=
\begin{cases}
1, & r \le r_{\mathrm{sm}}, \\[4pt]
x^{3}(-6x^{2} + 15x - 10) + 1,
& r_{\mathrm{sm}} < r < r_c, \\[4pt]
0, & r \ge r_c,
\end{cases}
\qquad
x = \dfrac{r - r_{\mathrm{sm}}}{r_c - r_{\mathrm{sm}}}.
\]

\paragraph{Species-aware neighbour selection.}
DeepMD-v2 requires that neighbours be:
\begin{enumerate}
    \item grouped by species,
    \item sorted by distance within each species group,
    \item truncated according to the model’s \texttt{sel} vector.
\end{enumerate}

The previous HALMD implementation used a single flat neighbour list.  
The present work introduces complete species-aware ordering, ensuring perfect compatibility
with the DeepMD-v2 descriptor format.

\paragraph{Periodic ghost-cell extension.}
DeepMD constructs environments using extended periodic images, not the minimum-image
convention.  
This work reproduces the ghost-cell procedure described in Section~3.3, guaranteeing
that \( \mathbf{r}_{ij} \) is computed exactly as in DeepMD-kit.

\paragraph{Normalization.}
DeepMD-v2 applies component-wise normalization to each row of~\(R\) using model parameters
\texttt{t\_avg} and \texttt{t\_std}:
\[
\hat{R}_{ij,\alpha}
=
\frac{
    R_{ij,\alpha} - (t_{\mathrm{avg}})_{\alpha}
}{
    (t_{\mathrm{std}})_{\alpha}
}.
\]
In particular, the normalized inverse distance
\[
\hat{s}_{ij} = \hat{R}_{ij,0}
\]
is the scalar fed into the embedding networks.

\paragraph{Summary.}
The updated R-matrix construction introduces:
\begin{itemize}
    \item species-aware neighbour selection,
    \item periodic ghost-cell extension,
    \item exact DP-SE switching behaviour,
    \item normalization using \(t_{\mathrm{avg}}\) and \(t_{\mathrm{std}}\).
\end{itemize}

These improvements produce geometric descriptors fully consistent with DeepMD-v2.

% ---------------------------------------------------------
\subsubsection{G-Matrix (Embedding Matrix) Construction}
\label{sec:G_matrix_construction}

The second stage of the descriptor constructs the embedding matrix \( G \) by passing  
each normalized inverse distance \( \hat{s}_{ij} \) through a small neural network known as a
\emph{filter network}.
The output is a vector
\[
G_{ij} \in \mathbb{R}^{M},
\]
where \(M\) is the embedding dimension.

\paragraph{Central--neighbour species dependence.}
For a central atom of species \(a\) and neighbour of species \(b\), DeepMD-v2 defines a  
distinct embedding network:
\[
G_{ij}
=
N^{(a,b)}_{\theta}(\hat{s}_{ij}).
\]
This species-pair dependence is essential for modelling cross-species interactions (e.g.,
Cu--Ni, Ni--Al).  
The previous HALMD implementation \cite{cruz2025deepmd} used only a
single network per central species:
\[
G_{ij} = N^{(a)}_{\theta}(\hat{s}_{ij}),
\]
which cannot represent multi-component models.

The present work extends HALMD to store and evaluate:
\[
\texttt{neural\_networks[a][b]}
\equiv N^{(a,b)}_{\theta},
\]
thus implementing the full DeepMD-v2 filter-network structure.

\paragraph{Runtime network selection.}
During environment construction, the species-ordered neighbour list assigns a neighbour
type index \(b\) to each row \(j\).  
Thus the embedding evaluation becomes:
\[
G_{ij}
=
N^{(a,b_{(j)})}_{\theta}\!\left( \hat{s}_{ij} \right).
\]

\paragraph{Padding behaviour.}
If the required neighbour count for species \(b\) exceeds the number of available neighbours,
DeepMD pads the remaining rows using the \emph{last valid} embedding vector.  
The updated HALMD implementation reproduces this behaviour exactly.

\paragraph{Derivative propagation.}
For forces, HALMD must compute
\[
\frac{\partial G_{ij,k}}{\partial \hat{s}_{ij}}
\qquad\text{and}\qquad
\frac{\partial \hat{s}_{ij}}{\partial \mathbf{r}_{ij}},
\]
which are combined via the chain rule.

The updated filter-network module evaluates
\[
\frac{\partial G_{ij}}{\partial \hat{s}_{ij}}
\]
using automatic differentiation inside \texttt{complete\_result()}, ensuring correct
multi-species derivative propagation.  
The previous implementation could not compute neighbour-specific derivatives because it used
only one filter network per species.

\paragraph{Summary.}
The improved G-matrix implementation now includes:
\begin{itemize}
    \item distinct filter networks \(N^{(a,b)}_\theta\) for every species pair,
    \item runtime selection of the correct network for each neighbour,
    \item correct handling of normalized inverse distances \(\hat{s}_{ij}\),
    \item optimized reuse of padded embeddings,
    \item multi-species derivative propagation for accurate forces.
\end{itemize}

These modifications bring HALMD into full alignment with the DeepMD-v2 specification and
enable accurate simulations of multi-component systems.

\subsection{Descriptor Computation}

The computation of the atomic descriptor follows the DeepPot-SE (\texttt{se\_e2\_a})
formulation of DeepMD-kit v2, in which the local environment of each atom is
encoded through a combination of geometric information (\(R\)-matrix) and
species-aware embedding functions (\(G\)-matrix).  The resulting descriptor enters
the fitting network that predicts the atomic energy \(E_i\) and must therefore
reproduce the DeepMD-v2 construction exactly.

In the original work of Cruz~\cite{cruz2025deepmd}, the descriptor pipeline was
implemented for a \emph{single-species} system with a fixed neighbour capacity
\(N_c^{(a)}\) for each central species \(a\).  
All descriptor matrices---the geometric matrix \(R\), embedding matrix \(G\), and
quadratic descriptor matrix \(D\)---were dimensioned accordingly.  
However, DeepMD-v2 allocates neighbour slots on a \emph{per-species basis}, as
defined by the \texttt{sel} field of the model:
\[
N_c = \sum_{b \in \mathcal{S}} N_c^{(b)},
\]
where \(N_c^{(b)}\) is the maximum number of neighbours of species \(b\) for a
central atom of species \(a\).  
To accommodate this structure, the present work replaces the single neighbour
capacity with the multi-species total capacity
\[
N_c^{\mathrm{(total)}} = \sum_b N_c^{(b)},
\]
and enlarges all descriptor matrices accordingly.

\paragraph{R-matrix construction.}
After periodic extension and neighbour selection (Section~3.4), the
coordinate-computation module returns the \emph{normalized geometric matrix}
\[
\hat{R} \in \mathbb{R}^{N_c^{\mathrm{(total)}} \times (1+d)},
\]
whose rows contain the normalized inverse distance
\(\hat{s}_{j}\) and the normalized coordinate components.
Unlike the earlier HALMD implementation, the rows of \(\hat{R}\) are now grouped
into species blocks:
\[
\underbrace{N_c^{(1)}}_{\text{species 1}}
\quad
\underbrace{N_c^{(2)}}_{\text{species 2}}
\quad \cdots \quad
\underbrace{N_c^{(B)}}_{\text{species B}},
\]
matching the DeepMD-v2 descriptor layout.

\paragraph{Species-aware G-matrix.}
DeepMD-v2 employs \emph{distinct filter networks} \(N^{(a,b)}_\theta\) for each
central–neighbour species pair \((a,b)\).  
For each neighbour row \(j\), the neighbour-species block determines the
appropriate network, and the embedding vector is computed as
\[
G_j = N^{(a,b(j))}_{\theta}\!\left( \hat{s}_{j} \right),
\]
where \(\hat{s}_{j} = \hat{R}_{j,0}\) is the normalized inverse distance
(Section~3.4).  
This replaces the original single-network evaluation \(N^{(a)}_{\theta}(\hat{s}_j)\)
used by Cruz and enables correct modelling of multi-component alloys.

The resulting matrix
\[
G \in \mathbb{R}^{N_c^{\mathrm{(total)}} \times M}
\]
is fully species-aware and follows the DeepMD-v2 specification.

\paragraph{Descriptor assembly.}
DeepMD constructs the descriptor as the quadratic form
\[
D
=
\frac{1}{(N_c^{\mathrm{(total)}})^2}\,
\left(G^\mathsf{T} \hat{R}\right)
\left(\hat{R}^\mathsf{T} G_{\mathrm{trunc}}\right),
\]
where \(G_{\mathrm{trunc}}\) contains the first \(M\) columns of \(G\), following
the standard \texttt{se\_e2\_a} truncation.  
The original HALMD implementation used the single-species normalization
\((N_c^{(a)})^2\); here it is updated to the correct
\((N_c^{\mathrm{(total)}})^2\), which is necessary for exact agreement with the
TensorFlow inference.

The descriptor matrix \(D\) is finally flattened row-major to form the input to the
fitting network.

\paragraph{Summary of improvements.}
Compared to Cruz’s single-species implementation, the updated descriptor computation:
\begin{itemize}
    \item generalizes the neighbour axis to multi-species blocks as defined by the
          \texttt{sel} vector;
    \item evaluates species-dependent embedding networks \(N^{(a,b)}_\theta\) for all
          central–neighbour species pairs;
    \item expands the geometric and embedding matrices to the full multi-species
          capacity \(N_c^{\mathrm{(total)}}\);
    \item uses the correct DeepMD-v2 normalization \((N_c^{\mathrm{(total)}})^2\);
    \item employs normalized geometric inputs \(\hat{R}\) and normalized inverse distances
          \(\hat{s}_j\) consistent with Section~3.4.
\end{itemize}
This brings HALMD’s descriptor pipeline into full alignment with the DeepMD-v2
specification and enables accurate multi-species inference.


\subsection{Potential Energy Calculation}
\label{sec:potential_energy}

In the Deep Potential (DeeP) formalism, the total potential energy of a system is
expressed as a sum of atomic energy contributions,
\begin{equation}
    E = \sum_{i=1}^{N} E_i,
\end{equation}
where each atomic energy $E_i$ is obtained by evaluating a species-dependent
fitting neural network acting on the descriptor vector $\mathbf{D}_i$ of atom $i$.
For an atom of species $s_i$, the mapping is
\begin{equation}
    E_i = \mathrm{NN}_{s_i}(\mathbf{D}_i),
\end{equation}
where $\mathrm{NN}_{s_i}$ denotes the fitting network associated with species
$s_i$. HALMD evaluates this network using the trained weights extracted from the
\texttt{frozen\_model.pb} file, including DeepMD-v2 activation functions, residual
connections, and timestep scaling.

\subsubsection*{Baseline Implementation Prior to This Work}

The original HALMD implementation by Cruz supported:
\begin{itemize}
    \item \textbf{single-species} DeepMD potentials,
    \item evaluation of a single fitting network for all atoms,
    \item complete DeepMD feed-forward inference (activations, skip connections, Jacobian computation),
    \item accurate reproduction of DeepMD energies for monoatomic systems.
\end{itemize}

However, it lacked the mechanisms required for DeepMD-v2 multi-species inference:
\begin{itemize}
    \item no species-dependent fitting networks,
    \item no species-aware descriptor (i.e.\ no use of $\hat{R}$ or species-partitioned $G$),
    \item no support for the per-species atomic energy offset (\texttt{bias\_atom\_e}),
    \item no coupling between multi-species descriptors and the correct fitting network.
\end{itemize}

Consequently, HALMD could not reproduce DeepMD-v2 predictions for alloy systems.

\subsubsection*{Extensions Introduced in This Thesis}

This work generalizes the HALMD potential evaluation pipeline to fully support
DeepMD-v2 multi-species potentials.

\paragraph{1.\quad Species-dependent fitting networks.}

DeepMD-v2 defines one fitting network per atomic species.  
The new implementation stores a vector of networks,
\[
    \{\mathrm{NN}_0,\ \mathrm{NN}_1,\ \ldots,\ \mathrm{NN}_{S-1}\},
\]
and selects the appropriate one according to the central atom’s species,
\[
    E_i = \mathrm{NN}_{s_i}(\mathbf{D}_i).
\]
This required extending the \texttt{deepmd} operator so that species information
is forwarded to both the descriptor and fitting stages.

\paragraph{2.\quad Inclusion of the atomic energy bias term.}

DeepMD-v2 models may include a constant per-species energy offset
\texttt{bias\_atom\_e}.  
HALMD now adds this term explicitly,
\begin{equation}
    E_i = \mathrm{NN}_{s_i}(\mathbf{D}_i) + b_{s_i},
\end{equation}
ensuring numerical agreement with DeepMD-kit.

\paragraph{3.\quad Integration with multi-species descriptors.}

The descriptor introduced in Section~3.4 makes use of:
\begin{itemize}
    \item the normalized geometric matrix $\hat{R}$,
    \item species-partitioned neighbour groups,
    \item species-pair embedding networks $G^{(a,b)}$,
    \item per-species normalization tensors $(t_{\mathrm{avg}}, t_{\mathrm{std}})$.
\end{itemize}

so that the fitting network receives the correct DeepMD-v2 descriptor vector
$\mathbf{D}_i$.

\paragraph{4.\quad Reproduction of DeepMD-v2 energies.}

With all extensions enabled, HALMD reproduces DeepMD-v2 energies with
floating-point accuracy:
\begin{itemize}
    \item per-atom energies match the TensorFlow implementation,
    \item species offsets are correctly incorporated,
    \item descriptor and fitting network outputs coincide with DeepMD-kit,
    \item total energies of multi-component systems agree exactly with reference values.
\end{itemize}

\subsubsection*{Final Formulation}

The atomic energy computed in HALMD now takes the DeepMD-v2 form
\begin{equation}
    E_i = \mathrm{NN}_{s_i}(\mathbf{D}_i) + b_{s_i},
\end{equation}
and the total potential energy is
\begin{equation}
    E_{\mathrm{tot}} = \sum_{i=1}^{N} E_i.
\end{equation}
This matches DeepMD-v2 inference for all tested single- and multi-species systems.

\section{Force Computation}
\label{sec:forces}

In the Deep Potential (DeeP) framework, forces are obtained as the negative 
gradient of the total potential energy with respect to atomic coordinates,
\[
    \mathbf{F}_i = -\frac{\partial E}{\partial \mathbf{r}_i}.
\]
Because the energy is produced through a multi-stage mapping involving geometric 
quantities (\(R\)), species-dependent embedding networks (\(G\)), and a 
species-dependent fitting network, the force computation requires evaluating a 
sequence of nested derivatives. 

DeepMD expresses this as a structured chain rule passing through:

\[
\mathbf{r}
\;\xrightarrow{\text{geometry}}\; 
R
\;\xrightarrow{\text{embedding}}\;
G
\;\xrightarrow{\text{fitting network}}\;
E,
\]
so that each force component involves contributions from all atoms appearing in 
the local environment of the corresponding descriptor.

The remainder of this section describes the derivative computation in HALMD.  
We begin with an overview of automatic differentiation (AD), which is used to 
evaluate all neural-network derivatives. We then detail the analytic derivatives 
of the geometry matrix \(R\), the descriptor \(D\), and finally show how these 
quantities are combined with the fitting-network derivatives to assemble the 
total atomic forces.


\subsection{Automatic Differentiation (AD)}
\label{sec:autodiff}

As outlined in the beginning of this section, the computation of atomic forces
in DeepMD requires propagating derivatives through a sequence of mappings
involving geometric transformations, embedding networks, descriptor algebra,
and finally a species-dependent fitting network.  
While the geometric parts admit closed-form analytic expressions, the neural
networks used in DeepMD-v2 contain nonlinear activations, residual connections,
and timestep scalings that make analytical differentiation impractical.  
For these components HALMD employs \emph{automatic differentiation} (AD).


Automatic differentiation is a computational technique for evaluating
derivatives of functions represented as compositions of elementary operations
\cite{griewank2008evaluating, baydin2018automatic}.  
Unlike symbolic differentiation, AD avoids expression explosion, and unlike
finite differences, it does not introduce truncation error.  
Instead, derivatives are accumulated by applying the chain rule locally at
every primitive operation.

Two modes of AD are central:
\begin{itemize}
    \item \textbf{Forward-mode AD}: propagates derivatives together with the
    computation, efficient when the number of inputs is small.

    \item \textbf{Reverse-mode AD}: propagates derivatives backward from a scalar
    output, efficient when the number of outputs is one.  
    This is the mechanism underlying backpropagation
    \cite{rumelhart1986learning} and the mode used in modern ML frameworks such
    as TensorFlow \cite{abadi2016tensorflow} and PyTorch \cite{paszke2019pytorch}.
\end{itemize}

Reverse-mode AD is particularly well suited to DeepMD, since both the embedding
networks $G^{(a,b)}$ and the fitting networks $\mathrm{NN}_{s_i}$ have scalar
inputs or scalar outputs, making derivative evaluation highly efficient.


HALMD uses Boost.Autodiff \cite{falcou2023boostautodiff} to compute all
derivatives internal to neural networks:
\begin{enumerate}
    \item For each neighbour $j$, the filter network $G^{(a,b)}$ provides both
    the embedding vector $G_{ij}$ and the derivative
    $\partial G_{ij}/\partial\hat{s}_{ij}$.

    \item For each atom $i$, the fitting network provides both the predicted
    atomic energy $E_i$ and the Jacobian
    $\partial E_i / \partial D_{i,\alpha}$.

    \item All remaining derivatives—those involving the geometric mapping
    $R^\ast(\mathbf{r})$, the scaled distances $\hat{s}_{ij}$, and the descriptor
    construction $D(G,R^\ast)$—are evaluated analytically and implemented
    explicitly in HALMD.
\end{enumerate}


AD therefore supplies exactly the neural-network parts of the chain rule,
while the analytical components handle the geometric and descriptor terms.
This hybrid strategy achieves:
\begin{itemize}
    % \item exact numerical agreement with the DeepMD-v2 reference implementation,
    \item efficient multi-species derivative propagation,
    \item clean separation between neural and geometric derivatives.
\end{itemize}

The following subsections detail each analytic component of the force
derivation: derivatives of the geometric mapping ($R^\ast$), descriptor
derivatives, fitting-network derivatives, and finally the assembly of the total
force.

\subsubsection{Derivatives of the Raw Geometry Matrix \texorpdfstring{$R^\ast$}{R*}}
\label{sec:R_derivative}

The computation of atomic forces requires the derivative of each row of the
raw geometry matrix \( R^\ast \) with respect to the coordinates of the
central atom.  
Since \( R^\ast \) depends on both the pairwise displacement
\( \mathbf{r}_{ij} \) and its magnitude \( r_{ij} \), its derivative naturally
splits into \emph{radial} and \emph{angular} components, which are combined
to obtain the full geometric derivative.

\paragraph{Structure of the raw geometry row.}
Recall from Section~\ref{sec:R_matrix_construction} that each neighbour \(j\)
contributes the row
\[
R^\ast_{ij}
=
\Bigl[
    \tilde{s}_{ij},\;
    \tilde{s}_{ij} \frac{x_{ij}}{r_{ij}},\;
    \tilde{s}_{ij} \frac{y_{ij}}{r_{ij}},\;
    \tilde{s}_{ij} \frac{z_{ij}}{r_{ij}}
\Bigr],
\]
where  
\[
\tilde{s}_{ij} = s_{ij} f_{\mathrm{sw}}(r_{ij}), 
\qquad
s_{ij} = \frac{1}{r_{ij}}.
\]

\subparagraph{Relative displacement and unit direction.}
For the central atom \(i\),
\[
\mathbf{r}_{ij} = \mathbf{r}_j - \mathbf{r}_i,
\qquad
r_{ij} = \lVert \mathbf{r}_{ij} \rVert,
\qquad
\hat{\mathbf{r}}_{ij} = \frac{\mathbf{r}_{ij}}{r_{ij}}.
\]

% -------------------------------------------------------
\paragraph{(1) Radial derivative: \texorpdfstring{$\partial r_{ij}/\partial \mathbf{r}_i$}{dr/dri}}
\label{sec:Rderiv_radial_distance}

The central-atom derivative of the pair distance is
\begin{equation}
\frac{\partial r_{ij}}{\partial \mathbf{r}_i}
=
-\,\hat{\mathbf{r}}_{ij},
\label{eq:dr_dri_clean}
\end{equation}
indicating that increasing \( \mathbf{r}_i \) in the direction of
\( \hat{\mathbf{r}}_{ij} \) decreases the separation.

% -------------------------------------------------------
\paragraph{(2) Radial derivative of the switched inverse distance}
\label{sec:Rderiv_radial_s}

The switched inverse distance is
\[
\tilde{s}_{ij} = \frac{1}{r_{ij}} f_{\mathrm{sw}}(r_{ij}).
\]
Differentiating w.r.t.\ \( r_{ij} \) yields
\begin{equation}
\frac{\partial \tilde{s}_{ij}}{\partial r_{ij}}
=
-\,\frac{1}{r_{ij}^{2}} f_{\mathrm{sw}}(r_{ij})
+
\frac{1}{r_{ij}}\,
\frac{\mathrm{d} f_{\mathrm{sw}}}{\mathrm{d} r}(r_{ij}).
\label{eq:dsdr_clean}
\end{equation}
Combining Eq.~\eqref{eq:dsdr_clean} with Eq.~\eqref{eq:dr_dri_clean}:
\[
\left(\frac{\partial R^\ast_{ij,0}}{\partial \mathbf{r}_i}\right)_{\mathrm{rad}}
=
-\,\frac{\partial \tilde{s}_{ij}}{\partial r_{ij}} \, \hat{\mathbf{r}}_{ij}.
\]

% -------------------------------------------------------
\paragraph{(3) Radial derivative of angular components}
\label{sec:Rderiv_radial_components}

For
\[
S_{\alpha}
=
\tilde{s}_{ij} \frac{d_{\alpha}}{r_{ij}},
\qquad
d_{\alpha} \in \{x_{ij}, y_{ij}, z_{ij}\},
\]
the derivative w.r.t.\ the distance is
\begin{equation}
\frac{\partial S_{\alpha}}{\partial r_{ij}}
=
\frac{d_{\alpha}}{r_{ij}^{2}}
\frac{\partial \tilde{s}_{ij}}{\partial r_{ij}}
-
2\,\tilde{s}_{ij} \frac{d_{\alpha}}{r_{ij}^{3}}.
\label{eq:radial_Salpha_clean}
\end{equation}
Thus,
\[
\left(\frac{\partial S_{\alpha}}{\partial \mathbf{r}_i}\right)_{\mathrm{rad}}
=
-\,\frac{\partial S_{\alpha}}{\partial r_{ij}}
\hat{\mathbf{r}}_{ij}.
\]

% -------------------------------------------------------
\paragraph{(4) Angular derivative: explicit coordinate dependence}
\label{sec:Rderiv_angular}

The components \( d_{\alpha} = x_{j,\alpha} - x_{i,\alpha} \)
depend directly on \( \mathbf{r}_i \).  
Holding \(r_{ij}\) fixed:
\[
\frac{\partial d_{\alpha}}{\partial \mathbf{r}_i}
= -\,\mathbf{e}_{\alpha},
\]
leading to the angular contribution
\begin{equation}
\left(\frac{\partial S_{\alpha}}{\partial \mathbf{r}_i}\right)_{\mathrm{ang}}
=
-\,\frac{\tilde{s}_{ij}}{r_{ij}}\,\mathbf{e}_{\alpha}.
\label{eq:angular_Salpha_clean}
\end{equation}
The isotropic term \( R^\ast_{ij,0} \) has no angular component.

% -------------------------------------------------------
\paragraph{(5) Full derivative of the raw geometry row}
\label{sec:Rderiv_full}

Combining radial and angular parts:
\begin{equation}
\frac{\partial R^\ast_{ij,\alpha}}{\partial \mathbf{r}_i}
=
\left(\frac{\partial R^\ast_{ij,\alpha}}{\partial \mathbf{r}_i}\right)_{\mathrm{rad}}
+
\left(\frac{\partial R^\ast_{ij,\alpha}}{\partial \mathbf{r}_i}\right)_{\mathrm{ang}},
\qquad
\alpha = 0,1,2,3.
\label{eq:Rstar_full_clean}
\end{equation}

% -------------------------------------------------------
\paragraph{(6) Derivative of the normalized geometry row}
\label{sec:Rderiv_norm}

Normalization (Section~\ref{sec:R_matrix_construction}) gives
\[
\hat{R}_{ij,\alpha}
=
\frac{
    R^\ast_{ij,\alpha} - (t_{\mathrm{avg}})_{\alpha}
}{
    (t_{\mathrm{std}})_{\alpha}
}.
\]
Thus,
\begin{equation}
\frac{\partial \hat{R}_{ij,\alpha}}{\partial \mathbf{r}_i}
=
\frac{1}{(t_{\mathrm{std}})_{\alpha}}
\frac{\partial R^\ast_{ij,\alpha}}{\partial \mathbf{r}_i}.
\label{eq:Rnorm_full_clean}
\end{equation}

% -------------------------------------------------------
\paragraph{Summary.}
The derivative of the geometry matrix \( R^\ast \) includes:
\begin{itemize}
    \item a \textbf{radial component} from the dependence on \( r_{ij} \),
    \item an \textbf{angular component} from explicit displacement derivatives,
    \item a \textbf{normalization derivative} for compatibility with DeepMD-v2.
\end{itemize}
These analytic formulas reproduce the exact geometric derivative pipeline of
DeepMD-v2 and provide the geometric contribution needed for force evaluation.




\subsection{Descriptor Derivative}
\subsection{Fitting Network Derivative}
\subsection{Final Force Assembly}

% ===============================
\section{Results}
\subsection{Verification}
% Comparison of per-atom energy and force between HALMD implementation and DeepMD-kit outputs.  
% Metrics: RMSE, MAE, correlation plots.

\subsection{Performance Evaluation}
% Computational performance (CPU vs GPU).  
% Scaling with number of atoms and neighbor count.  
% Memory and throughput comparison with standard DeepMD runtime.

\subsection{Case Study: Binary Alloy System}
% Description of system (e.g., Cu–Ag or Cu–Au).  
% Simulation setup (temperature, timestep, ensemble).  
% Observed physical properties (radial distribution, energy stability).

% ===============================
\section{Discussion}
% Accuracy vs. performance trade-offs.  
% Numerical stability and precision issues.  
% Possible sources of discrepancy from DeepMD reference.  
% Lessons learned integrating ML potentials into MD engines.

% ===============================
\section{Conclusions and Future Work}
% Summary of contributions.  
% Limitations and optimization possibilities.  
% Future extensions (multi-species descriptor, GPU parallelization, hybrid QM/ML MD).
% Autodiff implementation, different Descriptors, possibly type-embedding


\newpage
\nocite{colberg2011highly}
\printbibliography


\end{document}
