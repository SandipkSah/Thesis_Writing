\documentclass[a4paper,11pt,oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,top=3cm,bottom=3cm,left=3cm,right=3cm]{geometry}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{helvet}
\usepackage[english]{babel}
\usepackage[style=numeric,language=english,sorting=none]{biblatex}
\usepackage{parskip}
\usepackage[margin=1cm]{caption}
\usepackage{booktabs}
\usepackage[pdftex]{graphicx}
\usepackage{subfigure}
\usepackage[pdftex]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{float} % make sure this is in your preamble


\pdfadjustspacing=1

\newcommand{\mylastname}{Sah}
\newcommand{\myfirstname}{Sandip Kumar}
\newcommand{\mynumber}{5589263}
\newcommand{\myname}{\myfirstname{} \mylastname{}}
% \newcommand{\mytitle}{A fast implementation of deep neural-network potentials for molecular dynamics simulations of alloys}
\newcommand{\mytitle}{Accurate implementation of DeepMD-v2 Potential calculation in HALMD for single species, extension to multi-species and Automatic-Differentiation-Based Force Computation}
\newcommand{\mysupervisor}{Prof. Dr. Felix Höfling}

\hypersetup{
  pdfauthor = {\myname},
  pdftitle = {\mytitle},
  colorlinks = {true},
  linkcolor = {black}
}

\addbibresource{references.bib}

\begin{document}
\pagenumbering{roman}
\thispagestyle{empty}

\begin{flushright}
  \includegraphics[width=0.35\textwidth]{fub_logo_2.svg.png}
\end{flushright}
\vspace{10mm}

\vspace*{40mm}
\begin{center}
  \huge
  \textbf{\mytitle}
\end{center}
\vspace*{4mm}
\begin{center}
  \Large by
\end{center}
\vspace*{4mm}
\begin{center}
  \LARGE
  \textbf{\myname}
\end{center}
\vspace*{20mm}
\begin{center}
  \Large
  Master Thesis in Computational Science
\end{center}
\vfill
\begin{flushleft}
  \large
  Submission: {09 January 2026} \hfill Supervisor: \mysupervisor \\
  \rule{\textwidth}{1pt}
\end{flushleft}
\begin{center}
  Freie Universität Berlin $|$ Department of Mathematics and Computer Science\\
  Institute of Mathematics
\end{center}

\newpage
\thispagestyle{empty}

\begin{center}
  \Large \textbf{Statutory Declaration}
  \vspace*{8mm}
\end{center}

\begin{center}
\begin{tabular}{lp{85mm}}
    \hline
    Family Name, Given/First Name & \mylastname, \myfirstname \\
    Matriculation number          & \mynumber                 \\
    Kind of thesis submitted      & Master Thesis             \\
    \hline
  \end{tabular}
  \vspace*{8mm}
\end{center}



\section*{Declaration of Independence}


\bigskip

I hereby confirm that I have completed this thesis independently and without the use of sources and
aids other than those specified. This thesis has not been submitted in the same or a similar form as an
examination at any other university.

I have identified all quotations, both direct and indirect, as such. Literal quotations have been marked
with quotation marks (“...”). I have also cited Internet sources used, stating their full address and the
time of last access.

I take note:

I am obliged to comply with the rules of good scientific practice. Violations will be subject to
disciplinary action. In particular, attempts to cheat will automatically be assessed as “fail” (5.0)
(§ 19 (3) sentence 1 RSPO). The adoption of external ideas / results / concepts etc. without
appropriate identification (plagiarism) is generally an attempt to deceive. Depending on the severity,
further sanctions are possible up to the final failure of the examination and thus the degree program
(§ 19 (3) sentence 3 and 4 RSPO).

The grade may be lowered (including a failing grade) due to a lack of independence if a significant
amount of other persons' explanations are copied, even if they are identified as such and no attempt to
deceive has been made.

\bigskip

I did

\begin{itemize}
  \item[$\square$] not use generative AI to write my thesis.
  \item[$\blacksquare$] use generative AI to write my thesis (please note the information on the
  extended declaration of independence).
\end{itemize}

\vspace{1.5cm}

\noindent
Date:\hfill Signature:

\section*{Extended Declaration of Independence}


\bigskip

Following operations were performed in my thesis using generative AI:

\bigskip


\begin{center}
\begin{tabular}{|p{7cm}|p{5cm}|}
\hline
\textbf{Type and location of AI assistance} & \textbf{AI tool used} \\
\hline
Paraphrasing of section Introduction and Background
& ChatGPT~4.0 \\
\hline
Summarizing overview of methodology
& ChatGPT~4.0 \\
\hline
Formula Organizaation inside sector Force Computation
& ChatGPT~4.0 \\
\hline
Linguistic correction of the entire text
& Grammarly \\
\hline
\end{tabular}
\end{center}


\bigskip

I understand that using AI-generated text or other content does not guarantee its quality. I take full
responsibility for all machine-generated content that I use and am liable for any errors, distortions,
or incorrect references, as well as any violations of data protection, copyright law, or plagiarism.
Furthermore, I assure you that my editorial influence prevails in this work.

I confirm that I did not use any AI-based tools prohibited by the examination regulations.

\vspace{1.5cm}

\noindent
Date:\hfill Signature:





\newpage
\section*{Abstract}
% A concise 200–300 word summary of:
% \begin{itemize}
%   \item Motivation (accelerating atomistic simulations using neural network potentials)
%   \item Goal (integrating DeepMD potential calculation into HALMD)
%   \item Method (extracting weights, replicating inference in C++/CUDA)
%   \item Key results and performance (accuracy vs. speed trade-off)
%   \item Conclusions
% \end{itemize}

\newpage
\setcounter{tocdepth}{2}
\tableofcontents
\clearpage
\pagenumbering{arabic}

% ===============================


\section{Introduction}
\subsection{Motivation}

Molecular dynamics (MD) simulations play a central role in materials science by providing atomistic insight into the behavior of complex systems. Their predictive power depends on the interatomic potential used to approximate the underlying potential energy surface (PES). Classical empirical potentials are efficient but often too rigid to describe complex bonding environments, whereas \textit{ab initio} methods offer high accuracy at prohibitive computational cost. Machine-learned interatomic potentials---particularly the Deep Potential Molecular Dynamics (DeepMD) framework---bridge this gap by achieving near--quantum mechanical accuracy with classical-MD performance.

The work by Andrés Cruz, titled \textit{``Deep Neural Networks Potentials for Scalable Molecular Dynamics Simulations on Accelerator Hardware''} \cite{cruz2025deepmd}, represents an important step toward integrating Deep Potential models into the high-performance GPU-accelerated simulation engine HALMD. His work reconstructs the DeePMD-kit inference pipeline inside HALMD, extracts network weights from a trained TensorFlow model, and validates energy and force predictions for a \textit{single-species Copper system}. In particular, his implementation focuses on the \textit{two-body embedding smooth edition, DeepPot-SE descriptor} and reproduces the filter and fitting networks \textit{only for a monoatomic system}, making it suitable for single metal.

However, the implementation in \cite{cruz2025deepmd} remains limited to the simplest case of DeepMD-v2 architectures. It does not include multi-species support.
%     \item multi-species support,
%     \item species-dependent neighbor counts $N_c(a)$,
%     \item type embedding networks,
%     \item species-aware embedding matrices $G_i$,
%     \item the general multi-species DeepPot-SE (se\_e2\_a) descriptor,
%     \item or complex multi-body interactions required for alloys or chemically diverse systems.
% \end{itemize}





Additionally, several intermediate steps present in the full DeepMD-v2 computational graph---such as normalization layers, ghost body extension for periodicity in boundary condition, species-wise descriptor partitioning, and certain derivative pathways---were simplified or omitted in the previous implementation. Cruz's work successfully established a working single-species DeepMD integration within HALMD.

This work builds directly on the work of Cruz \cite{cruz2025deepmd} and advances it substantially. The primary contributions of the present work are:
\begin{enumerate}
    \item Generalizing the HALMD Deep Potential integration to \textit{multi-species, multi-body DeepMD-v2 models}, enabling simulations of binary and multicomponent alloy systems.
    \item Reconstructing descriptor and fitting networks for the \textit{full multi-species DeepPot-SE architecture}, including multi-body descriptor terms.
    \item Improving the \textit{accuracy} of the HALMD implementation by adding several computational steps missing in the previous work, such as proper cutoff normalization, multi-species descriptor algebra and so on.
\end{enumerate}

Through these extensions, the present thesis transforms HALMD from supporting a prototype single-component DeepMD potential into a \textit{ high-accuracy, multi-species DeepMD-v2 engine}. This significantly broadens HALMD's applicability, enabling large-scale molecular dynamics simulations of technologically relevant multicomponent materials at near--quantum mechanical accuracy.

\subsection{Objectives and Scope}

The objective of this thesis is to extend and generalize the Deep Potential (DeePot) implementation in HALMD beyond the single-species, two-body descriptor developed by Cruz \cite{cruz2025deepmd}. While Cruz's work successfully demonstrated that HALMD can evaluate a DeePMD-v2 model for a monoatomic copper system, several components required for full multi-species DeepMD-v2 inference were missing. Most notably, the previous implementation did not include (i) the periodic coordinate extension and neighbor-list construction used in DeepMD \cite{wang2018deepmd}, (ii) normalization and scaling layers defined in the DP-v2 framework \cite{zeng2023deepmdv2}, (iii) species-dependent descriptors, or (iv) descriptor and filter weights that depend simultaneously on the central and neighbor species, as introduced in the multi-species DeepPot-SE descriptor \cite{zeng2023deepmdv2}.

This thesis addresses these limitations by implementing the complete multi-species DeepMD-v2 inference pipeline, including accurate periodic handling of atomic environments. Specifically, the thesis pursues the following objectives:

\begin{enumerate}

    \item \textbf{Implement coordinate normalization, ghost-cell extension, and multi-type neighbor list construction.}  
    The previous implementation did not include the canonical DeePMD preprocessing steps for periodic systems---wrapping coordinates into the primary simulation cell, generating ghost atoms to cover the cutoff radius, and constructing species-grouped neighbor lists---as described in the DeePMD methodology \cite{wang2018deepmd, zeng2023deepmdv2}.  
    Implementing these steps ensures that HALMD reproduces the correct local environments required by the DeepPot-SE descriptors under periodic boundary conditions.

    % \item \textbf{Generalize the descriptor pipeline to multi-species systems.}  
    % This includes implementing species-dependent neighbor counts $N_c(a)$, species-aware embedding matrices $G_i$, and the full multi-species DeepPot-SE (se\_e2\_a) descriptor introduced in DP-v2 \cite{zeng2023deepmdv2}.

    \item \textbf{Implement species-dependent filter networks.}  
    Extend the single-species embedding and filter networks used in \cite{cruz2025deepmd} to support arbitrary numbers of atomic types, following the species-indexed filter network formulation defined in the DP-v2 architecture \cite{zeng2023deepmdv2}.

    \item \textbf{Reproduce the full DeepMD-v2 inference procedure with higher accuracy.}  
    Incorporate several computational steps omitted in previous work, including normalization layers, descriptor scaling operations, smooth cutoff functions, and full force backpropagation through descriptor derivatives, consistent with the DeepMD-v2 formulation \cite{zeng2023deepmdv2, wang2018deepmd}.

    % \item \textbf{Validate energy and force predictions for multi-component systems.}  
    % Compare results from HALMD against the DeePMD-kit reference implementation for multi-species models and quantify numerical deviations, following standard validation procedures established in DeepMD literature \cite{wang2018deepmd, zeng2023deepmdv2}.

\end{enumerate}

\noindent\textbf{Scope:}  
This thesis focuses exclusively on the inference stage of DeepMD-v2, i.e., the computation of energies and forces from pre-trained models. Model training is outside the scope of this work. The implementation targets the multi-species DeepPot-SE descriptor and does not cover other descriptor families such as end-to-end or message-passing potentials. The work provides support for multi-species atomic systems relevant to alloys and chemically complex materials, while the underlying molecular dynamics algorithms (integrators, thermostats, barostats) rely on HALMD's existing infrastructure.



% % ===============================
\section{Background}
% \subsection{HALMD Software}

% The High-Accuracy Large-scale Molecular Dynamics (HALMD) package is a high-performance, open-source molecular dynamics (MD) simulation framework developed to study the microscopic dynamics of liquids, glasses, and other condensed matter systems. HALMD is designed around a modular C++ architecture with a strong emphasis on numerical precision, extensibility, and efficient parallel computation on graphics processing units (GPUs)\cite{colberg2011highly}. It provides a versatile platform for conducting large-scale classical molecular dynamics simulations involving millions of particles.

% HALMD employs the \textit{classical molecular dynamics} approach, in which atomic motion is governed by Newton’s equations of motion,
% \[
% m_i \frac{d^2 \mathbf{r}_i}{dt^2} = - \nabla_i U(\mathbf{r}_1, \mathbf{r}_2, \ldots, \mathbf{r}_N),
% \]
% where \( U \) is a predefined potential energy function describing interatomic interactions. The accuracy of a classical MD simulation is determined by the analytical form of this potential. HALMD provides several built-in potentials such as Lennard-Jones, Gaussian Core, and Yukawa models, and also supports tabulated pair potentials that can be customized by the user. These potentials are purely empirical and do not involve electronic structure calculations, distinguishing HALMD from \textit{ab initio} molecular dynamics (AIMD) methods, which derive forces directly from quantum-mechanical computations.

% One of HALMD’s defining features is its high-performance GPU implementation. The software utilizes NVIDIA’s CUDA and, to accelerate computationally intensive tasks such as force evaluation, neighbor-list construction, and time integration. Through domain decomposition and spatial cell binning, HALMD efficiently scales to very large systems, achieving orders-of-magnitude speedups compared to single-core CPU implementations. It also supports mixed-precision arithmetic, applying double precision selectively to ensure long-term numerical stability without sacrificing performance.

% HALMD follows a modular design philosophy. Core components—such as integrators, potential functions, and observables—are implemented as interchangeable modules, which can be dynamically configured through a Lua scripting interface. This modularity facilitates the addition of new functionalities, such as custom interaction potentials or data analysis routines, without modifying the main codebase. Simulation data and trajectories are stored in the H5MD format, an HDF5-based standard that ensures compatibility with a wide range of analysis tools.

% These characteristics make HALMD an ideal foundation for extending molecular dynamics simulations with machine-learned interatomic potentials. In this work, HALMD serves as the host MD engine into which the Deep Potential Molecular Dynamics (DeepMD) model is integrated. The goal is to replace conventional analytical potentials with neural-network-based force fields that reproduce \textit{ab initio}-level accuracy while leveraging HALMD’s GPU-accelerated framework for computational efficiency.


\subsection{HALMD Software}

The High-Accuracy Large-scale Molecular Dynamics (HALMD) package is a high-performance, open-source molecular dynamics (MD) simulation framework designed to study the microscopic dynamics of liquids, glasses, and other condensed matter systems. HALMD is built around a modular C++ architecture with a strong emphasis on numerical precision, extensibility, and efficient parallel computation on graphics processing units (GPUs) \cite{colberg2011highly}. It provides a versatile platform for conducting large-scale classical molecular dynamics simulations involving millions of particles.

HALMD employs the \textit{classical molecular dynamics} approach, in which atomic motion is governed by Newton’s equations of motion,
\[
m_i \frac{d^2 \mathbf{r}_i}{dt^2}
= - \nabla_i U(\mathbf{r}_1, \mathbf{r}_2, \ldots, \mathbf{r}_N),
\]
where \( U \) is a predefined analytical potential describing interatomic interactions. The accuracy of a classical MD simulation depends on the accuracy of this potential. HALMD supports several built-in empirical pair potentials, such as Lennard–Jones, Gaussian Core, and Yukawa models, and additionally allows users to supply tabulated pair potentials. These empirical models do not involve quantum-mechanical calculations and thus differ from \textit{ab initio} molecular dynamics (AIMD), where forces are computed directly from electronic structure methods.

One of HALMD’s defining strengths is its high-performance GPU backend. The software employs NVIDIA’s CUDA to accelerate computationally intensive tasks such as force evaluation, neighbor-list construction, and time integration \cite{colberg2011highly}, and more recent versions extend support to heterogeneous computing platforms through SYCL \cite{halmdGithub}. Spatial domain decomposition and cell binning enable efficient scaling on multi-GPU systems, achieving orders-of-magnitude speedups compared to CPU-only implementations. HALMD also supports mixed-precision arithmetic, selectively applying double precision to critical operations to ensure long-term numerical stability without compromising performance.

HALMD follows a modular design philosophy. Core components—integrators, interaction potentials, neighbor-list builders, and observables—are implemented as interchangeable modules configurable through a Lua scripting interface \cite{halmdManual}. This modularity makes it straightforward to extend HALMD with new functionality, including custom potentials, analysis routines, and data exporters. Simulation data are stored in the H5MD format \cite{debuyl2014h5md}, an HDF5-based standard widely supported by analysis tools in computational physics and chemistry.

These characteristics make HALMD an ideal platform for integrating machine-learned interatomic potentials. Its GPU-accelerated, modular architecture provides the necessary infrastructure to replace conventional analytical potentials with neural-network-based force fields. In this thesis, HALMD serves as the host MD engine into which the Deep Potential Molecular Dynamics (DeepMD) model is embedded. The goal is to achieve \textit{ab initio}-level accuracy within HALMD’s efficient large-scale simulation framework by implementing a full multi-species DeepMD-v2 inference pipeline.

% \subsection{Neural Network Potentials}

% In recent years, machine learning techniques—particularly deep neural networks (DNNs)—have revolutionized the construction of interatomic potentials for molecular dynamics simulations. Traditional analytical potentials, while computationally efficient, are often limited in their ability to accurately capture many-body interactions and chemical complexities across diverse atomic environments. Neural Network Potentials (NNPs) overcome these limitations by learning the potential energy surface (PES) directly from \textit{ab initio} reference data, typically obtained from density functional theory (DFT) or \textit{ab initio} molecular dynamics (AIMD).

% In the NNP framework, a neural network is trained to approximate the mapping between an atomic configuration and its corresponding total energy and atomic forces. The total potential energy of a system is expressed as a sum of atomic energy contributions predicted by the network:
% \[
% E = \sum_i E_i(\mathcal{R}_i),
% \]
% where \(E_i\) denotes the atomic energy associated with atom \(i\), and \(\mathcal{R}_i\) represents its local atomic environment within a cutoff radius. This decomposition ensures extensivity and locality of the potential, allowing the model to scale efficiently with system size.

% The training of a neural network potential involves minimizing a composite loss function that enforces agreement with the reference \textit{ab initio} data for both energies and forces, and optionally for virial stresses:
% \[
% \mathcal{L} = p_E \frac{1}{N_E} \sum_{n} \left| E_n^{\text{NN}} - E_n^{\text{DFT}} \right|^2
% + p_F \frac{1}{N_F} \sum_{n,i} \left| \mathbf{F}_{i,n}^{\text{NN}} - \mathbf{F}_{i,n}^{\text{DFT}} \right|^2
% + p_V \frac{1}{N_V} \sum_{n} \left| \mathbf{V}_n^{\text{NN}} - \mathbf{V}_n^{\text{DFT}} \right|^2,
% \]
% where \(p_E\), \(p_F\), and \(p_V\) are weighting factors that control the relative importance of energy, force, and virial matching during training. Including forces in the loss function is critical, as it allows the model to capture local curvature of the PES and improves its transferability across different atomic configurations.

% Once trained, the neural network potential can replace the analytical interatomic potential in molecular dynamics simulations. During the inference stage, atomic coordinates are transformed into symmetry-preserving descriptors that are invariant under translation, rotation, and permutation of atoms of the same type. These descriptors serve as inputs to the neural network, which predicts atomic energies and corresponding forces in real time. The resulting simulations can therefore achieve \textit{ab initio}-level accuracy at computational costs comparable to classical MD.

% A variety of NNP architectures have been proposed, including Behler–Parrinello networks, Gaussian approximation potentials (GAP), SchNet, and Neural Equivariant Interatomic Potentials (NequIP). Among these, the Deep Potential Molecular Dynamics (DeepMD) framework has emerged as one of the most widely adopted and computationally efficient implementations, owing to its smooth descriptor formulation, scalability, and native GPU support. The following subsection focuses specifically on \textbf{DeepMD-kit version 2}, which forms the theoretical and computational foundation for the present work.


\subsection{Neural Network Potentials}

In recent years, machine learning techniques—particularly deep neural networks (DNNs) have revolutionized the construction of interatomic potentials for molecular dynamics simulations. Traditional analytical potentials, while computationally efficient, are often limited in their ability to accurately capture many-body interactions and chemical complexities across diverse atomic environments. Neural Network Potentials (NNPs) overcome these limitations by learning the potential energy surface (PES) directly from \textit{ab initio} reference data \cite{behler2007generalized, noe2020mlreview}.

In the NNP framework, a neural network is trained to approximate the mapping between an atomic configuration and its corresponding total energy and atomic forces. The total potential energy of a system is commonly expressed as a sum of atomic contributions \cite{behler2007generalized}:
\[
E = \sum_i E_i(\mathcal{R}_i),
\]
where \(E_i\) denotes the atomic energy associated with atom \(i\), and \(\mathcal{R}_i\) represents its local environment within a cutoff radius. This decomposition ensures extensivity and locality and enables efficient scaling with system size.


Training a neural network potential involves minimizing a loss function that combines
the errors in energies, forces, and optionally virials. Following the formulation used in 
DeePMD-kit~\cite{wang2018deepmd, zeng2023deepmdv2}, the loss is written as
\begin{equation}
\mathcal{L} = p_E L_E + p_F L_F + p_V L_V ,
\end{equation}
where the terms are defined as
\begin{align}
L_E &= \frac{1}{N_E}
\sum_{n=1}^{N_E} 
\left( E_n^{\mathrm{NN}} - E_n^{\mathrm{DFT}} \right)^2, \\
L_F &= \frac{1}{3 N_F}
\sum_{n=1}^{N_F} \sum_{i=1}^{N_{\text{atoms}}}
\left\| \mathbf{F}_{i,n}^{\mathrm{NN}} - \mathbf{F}_{i,n}^{\mathrm{DFT}} \right\|^2, \\
L_V &= \frac{1}{9 N_V}
\sum_{n=1}^{N_V}
\left\| \mathbf{V}_n^{\mathrm{NN}} - \mathbf{V}_n^{\mathrm{DFT}} \right\|^2 .
\end{align}
The factors \(1/3\) and \(1/9\) arise from averaging the force and virial errors
over their three and nine Cartesian components, respectively, ensuring consistent 
normalization across all contributions to the loss.

During inference, atomic coordinates are transformed into symmetry-preserving descriptors that are invariant under translation, rotation, and permutation of atoms of the same type \cite{bartok2013representing, zaheer2017deepsets}. These descriptors serve as input to the neural network, which predicts atomic energies and corresponding forces in real time, achieving \textit{ab initio}-level accuracy at computational cost comparable to classical MD.

A variety of neural-network-based interatomic potentials have been proposed, including Behler–Parrinello networks \cite{behler2007generalized}, Gaussian Approximation Potentials (GAP) \cite{bartok2015gap}, SchNet \cite{schutt2017schnet}, and the E(3)-equivariant NequIP model \cite{batzner2022nequib}. Among these, the Deep Potential Molecular Dynamics (DeepMD) framework has emerged as one of the most widely adopted and computationally efficient implementations due to its smooth descriptor formulation and native GPU acceleration. The following subsection focuses specifically on \textbf{DeepMD-kit version 2}, which forms the theoretical and computational foundation for the present work.


% \subsection{Deep Potential Molecular Dynamics (DeepMD)}

% This work focuses exclusively on \textbf{Deep Potential Molecular Dynamics version 2 (DeepMD-kit v2)}, the second-generation implementation of the Deep Potential framework. DeepMD-kit v2 represents a major advancement over its predecessor, introducing smooth and transferable descriptor formulations, improved multi-species handling, and efficient GPU execution. These features make it particularly suitable for high-accuracy, large-scale molecular dynamics simulations of binary alloy systems within HALMD.

% DeepMD-kit v2 is a machine-learning-based approach that models interatomic interactions with near \textit{ab initio} accuracy at a computational cost comparable to classical molecular dynamics. It employs deep neural networks (DNNs) to learn the mapping between atomic configurations and their corresponding potential energies and forces from quantum-mechanical reference data, typically obtained from density functional theory (DFT).

% At the heart of the Deep Potential formalism lies the decomposition of the total potential energy into atomic energy contributions,
% \[
% E = \sum_i E_i(\mathcal{R}_i),
% \]
% where \(E_i\) is the atomic energy associated with atom \(i\), and \(\mathcal{R}_i\) denotes the local atomic environment of atom \(i\) within a cutoff radius \(r_c\). This locality assumption ensures linear scaling with the number of atoms and enables efficient parallelization on GPUs. Each atomic energy \(E_i\) is predicted by a neural network that takes as input a descriptor encoding the geometric arrangement of neighboring atoms in a manner invariant to translation, rotation, and permutation of atoms of the same type.

% In DeepMD-kit v2, the local environment of each atom is described using the \textit{Smooth Edition} (SE) descriptor, implemented in two variants: the angular form (\texttt{se\_e2\_a}) and the radial form (\texttt{se\_e2\_r}). These descriptors are built upon two intermediate geometric tensors—the \(R\)-matrix and the \(G\)-matrix—which capture both pairwise and many-body correlations. The \(R\)-matrix represents relative positions of neighboring atoms:
% \[
% R_{ij} = \mathbf{r}_j - \mathbf{r}_i,
% \]
% for all neighbors \(j\) within the cutoff radius \(r_c\). The pair distances \(r_{ij} = |R_{ij}|\) are modulated by a smooth cutoff function \(f_c(r_{ij})\) to ensure continuity of both energy and force at the cutoff boundary. From the \(R\)-matrix, a \(G\)-matrix is constructed to incorporate angular dependencies and normalize the geometric information, providing a rotationally and permutationally invariant representation of the local atomic environment.

% The Deep Potential architecture in version 2 consists of two coupled neural networks:
% \begin{enumerate}
%     \item \textbf{Embedding (Filter) Network:} Also referred to as the \textit{filter network}, this smaller feed-forward network acts individually on each neighbor of atom \(i\). It maps raw neighbor information (e.g., distances or angular components) into smooth, high-dimensional features that act as adaptive filters. Each atomic species is associated with its own filter network, enabling the model to capture distinct chemical interactions across multiple elements while preserving permutation invariance within a given species.

%     \item \textbf{Fitting Network:} A larger fully connected neural network that takes the aggregated descriptor vector \(\mathbf{D}_i\), constructed from the outputs of the filter network, and predicts the atomic energy \(E_i\). The same fitting network architecture is used for all atoms of a given species, while different parameter sets are used for different species, allowing accurate modeling of binary or multi-component alloy systems.
% \end{enumerate}

% The total potential energy is obtained as the sum of all atomic energies, and the atomic forces are derived as the negative gradients of the energy with respect to the atomic coordinates:
% \[
% \mathbf{F}_i = -\frac{\partial E}{\partial \mathbf{r}_i}.
% \]
% During training, these gradients are computed automatically through TensorFlow’s backpropagation, while during inference (the focus of this thesis) they are obtained via explicit application of the chain rule through the descriptor and neural network layers.

% A key strength of DeepMD-kit v2 is its explicit treatment of multi-species systems. Each atomic species is assigned its own filter (embedding) network, enabling the model to accurately capture both intra-species and inter-species interactions—such as Cu–Ag or Ni–Al bonds—while maintaining smoothness and transferability across configurations. This makes it especially powerful for modeling binary alloy systems.

% A trained DeepMD-kit v2 model is stored as a TensorFlow \texttt{frozen\_model.pb} file, accompanied by an \texttt{input.json} configuration file. The \texttt{.pb} file contains the trained parameters—weights, biases, and network topology—while the JSON file defines the descriptor type, cutoff radius, activation functions, and precision settings. During inference, atomic coordinates are transformed into descriptors, passed through the species-specific filter networks, and then processed by the fitting network to produce per-atom energies and forces.

% In this thesis, DeepMD-kit v2 serves as the machine-learned potential integrated into the HALMD simulation framework. All parameter extraction, descriptor reconstruction, and neural network inference follow the DeepMD-kit v2 specification precisely. By embedding this inference process into HALMD’s GPU-accelerated infrastructure, the present work enables efficient and scalable molecular dynamics simulations of binary alloy systems with near \textit{ab initio} accuracy.

\subsection{Deep Potential Molecular Dynamics (DeepMD)}

This work focuses on \textbf{Deep Potential Molecular Dynamics version~2 (DeepMD-kit v2)}, the second-generation implementation of the Deep Potential framework. DeepMD-kit v2 represents a major advancement over the original DeepMD formulation~\cite{wang2018deepmd}, introducing improved descriptor smoothness, enhanced multi-species handling, and a refined software architecture that enables highly efficient GPU execution~\cite{zeng2023deepmdv2}. These features make DeepMD-kit v2 particularly suitable for modeling chemically complex systems such as binary alloys within HALMD.

Deep Potential Molecular Dynamics is a machine-learning approach that approximates interatomic interactions with near \textit{ab initio} accuracy while retaining computational efficiency comparable to classical MD. A neural network is trained on quantum-mechanical reference data (typically from DFT or AIMD) to map atomic configurations to energies and forces. The total potential energy of the system is decomposed into atomic energy contributions~\cite{wang2018deepmd, zeng2023deepmdv2}:
\[
E = \sum_{i} E_i(\mathcal{R}_i),
\]
where \(E_i\) is the atomic energy of atom \(i\), and $\mathcal{R}_i$ denotes its local environment within a cutoff radius \(r_c\). This locality assumption yields linear scaling in the number of atoms and enables efficient parallelization on GPUs.

A defining element of DeepMD is the use of symmetry-preserving descriptors that are invariant under translations, rotations, and permutations of atoms of the same type. DeepMD-kit v2 employs the \textit{Smooth Edition} (SE) descriptor family, provided in angular (\texttt{se\_e2\_a}) variants~\cite{zeng2023deepmdv2}. It is constructed from two matrices:
\begin{itemize}
    \item the \textbf{R-matrix}, containing relative position vectors \( R_{ij} = \mathbf{r}_j - \mathbf{r}_i \) for neighbors \(j\) within the cutoff radius;
    \item the \textbf{G-matrix}, a normalized representation incorporating angular information and many-body geometric correlations.
\end{itemize}
Distances $r_{ij} = |R_{ij}|$ are modulated by a smooth cutoff function to ensure continuity of energies and forces at the cutoff boundary~\cite{wang2018deepmd}.

The Deep Potential model in both DP-v1 and DP-v2 consists of two neural networks: an \textit{embedding (filter) network} and a \textit{fitting network}~\cite{wang2018deepmd, zeng2023deepmdv2}.

\begin{enumerate}
    \item \textbf{Embedding (Filter) Network.}  
    This smaller feed-forward network transforms raw neighbor information into high-dimensional filter features. In DP-v2, a distinct embedding network is assigned to each atomic species, allowing the model to capture chemically specific interactions (e.g., Cu--Ag, Ni--Al) while preserving permutation invariance within each species~\cite{zeng2023deepmdv2}. The embedding network processes each neighbor independently.

    \item \textbf{Fitting Network.}  
    The outputs of the embedding networks are aggregated into a descriptor vector $\mathbf{D}_i$, which is passed to a larger fully connected fitting network that predicts the atomic energy $E_i$. Each species has its own fitting network parameters, enabling accurate modeling of multi-component materials.
\end{enumerate}

Forces are obtained as the negative gradients of the total energy:
\[
\mathbf{F}_i = -\frac{\partial E}{\partial \mathbf{r}_i}.
\]
During training, these derivatives are computed automatically through TensorFlow’s backpropagation engine. During inference---which is the focus of this thesis---the gradients are obtained by applying the chain rule explicitly through the descriptor and neural-network layers, following the DeePot-v2 computational graph~\cite{zeng2023deepmdv2}.

DeepMD-kit v2 introduces several enhancements that are essential for accurate multi-species modeling. These include species-dependent embedding networks, species-specific filter weights, descriptor normalization layers, and refined smooth cutoff schemes~\cite{zeng2023deepmdv2}. Together, these improvements enable DeePot-v2 to handle chemically diverse systems with greater smoothness, transferability, and numerical stability compared to its predecessor.

A trained DeepMD model is stored as a TensorFlow \textit{frozen\_model.pb} file trained with config file \texttt{input.json} file which specifies descriptor types, cutoff radii, hyperparameters, and precision settings. During inference, atomic coordinates are transformed into descriptors, processed by species-specific embedding networks, and fed into the fitting network to produce atomic energies and forces.

In this thesis, DeepMD-kit v2 serves as the machine-learned potential that is fully integrated into HALMD. All parameter extraction, descriptor reconstruction, and neural-network inference follow the DP-v2 specification precisely. By embedding this inference process into HALMD's GPU-accelerated architecture, the present work enables large-scale simulations of multi-species systems, such as binary alloys, with near \textit{ab initio} accuracy.



% ===============================
\section{Methodology}

\subsection{Overview of Implementation}

The integration of DeepMD-v2 into HALMD follows a modular workflow in which each
stage provides the necessary inputs for the next.  The process begins with
extracting all descriptor and neural-network parameters from the
\texttt{frozen\_model.pb} and \texttt{input.json} files and converting them into a
compact HDF5 format that HALMD can load efficiently.  Using these parameters,
HALMD reconstructs the local atomic environment in the exact DeepMD-v2
convention, including periodic wrapping, ghost-cell expansion, and
species-ordered neighbour selection.  From this environment, HALMD computes the
descriptor by forming the geometric matrix \(R\), evaluating species-dependent
embedding networks to obtain \(G\), and assembling the final descriptor vector
\(D_i\) that characterises the environment of each atom.  The descriptor is then
passed through the species-specific fitting network to compute atomic energies,
while forces are obtained by propagating derivatives through the descriptor and
networks using a combination of analytic expressions and automatic
differentiation.  Together, these steps enable HALMD to perform full DeepMD-v2
inference—energies and forces—within its native C++/CUDA simulation loop.


\subsection{Model Parameter Extraction}
\label{sec:model_parameter_extraction}

% \subsubsection{Frozen Model Structure}

% The trained DeepMD-v2 potential is distributed as a frozen TensorFlow graph
% (\texttt{frozen\_model.pb}) .  
% To perform manual inference inside HALMD, all relevant model parameters
% must be reconstructed from the computation graph.  
% The node list extracted from the model (see Appendix~X) contains all
% information required for this reconstruction.
% % %
% % \footnote{Node list extracted from the provided \texttt{frozen\_model.pb}.}

% \paragraph{Descriptor attributes.}
% The descriptor section of the graph exposes the physical and structural
% parameters needed to build the environment matrix:
% \begin{itemize}
%     \item cutoff radii (\texttt{descrpt\_attr/rcut}, \texttt{rcut\_smth}),
%     \item neighbour selection size (\texttt{descrpt\_attr/sel}, \texttt{original\_sel}),
%     \item type mapping and type statistics  
%         (\texttt{descrpt\_attr/t\_avg}, \texttt{t\_std}, \texttt{ntypes}).
% \end{itemize}
% These values uniquely determine the construction of $\hat{R}_i$ and ensure that
% the HALMD implementation reproduces the same geometric preprocessing as
% DeepMD.

% \paragraph{Embedding network.}
% The embedding (filter) network parameters appear under
% \texttt{filter\_type\_0/*}.  
% Each layer provides:
% \begin{itemize}
%     \item weight matrices (\texttt{matrix\_1\_0}, \texttt{matrix\_2\_0}, \texttt{matrix\_3\_0}),
%     \item bias vectors (\texttt{bias\_1\_0}, \texttt{bias\_2\_0}, \texttt{bias\_3\_0}),
%     \item activation functions (\texttt{Tanh} nodes).
% \end{itemize}
% From these nodes, the embedding transformation
% $G_i(\hat{R}_i)$ can be reconstructed exactly.

% \paragraph{Descriptor production.}
% The node \texttt{ProdEnvMatA} encapsulates the internal DeepMD operation that
% produces the environment matrix and its derivatives:
% \begin{itemize}
%     \item relative positions and angular components (\texttt{o\_rmat}),
%     \item pairwise distances (\texttt{o\_rij}),
%     \item neighbour list (\texttt{o\_nlist}),
%     \item geometric derivatives (\texttt{o\_rmat\_deriv}).
% \end{itemize}
% These outputs provide the ground truth for validating HALMD's geometric
% derivative implementation.

% \paragraph{Fitting network.}
% The atomic-energy fitting network is represented by nodes
% \texttt{layer\_0\_type\_0/*}, \texttt{layer\_1\_type\_0/*},
% \texttt{layer\_2\_type\_0/*}, and \texttt{final\_layer\_type\_0/*}.  
% Each layer contributes:
% \begin{itemize}
%     \item a weight matrix,
%     \item a bias vector,
%     \item optional residual coefficients (\texttt{idt} nodes),
%     \item nonlinear activations (\texttt{Tanh}).
% \end{itemize}
% Collectively, these nodes define the mapping from the descriptor vector
% $D_i$ to the atomic energy $E_i$.

% \paragraph{Energy and force outputs.}
% The frozen graph also contains the nodes that produce:
% \begin{itemize}
%     \item total energy (\texttt{o\_energy}),
%     \item per-atom energy (\texttt{o\_atom\_energy}),
%     \item forces (\texttt{o\_force}),
%     \item virials (\texttt{o\_virial}, \texttt{o\_atom\_virial}),
% \end{itemize}
% as well as all intermediate gradient nodes under \texttt{gradients/*},
% which DeepMD relies on for automatic differentiation.

% \medskip
% Together, these extracted components provide a complete specification of the
% trained potential: the descriptor parameters, the embedding transformation,
% the environment matrix construction, the full fitting network, and the final
% energy/force outputs.  Using these elements, manual inference inside HALMD can
% be implemented in a way that mirrors the DeepMD-v2 computational graph
% exactly.


\subsubsection{Frozen Model Structure}

The trained DeepMD-v2 potential is distributed as a frozen TensorFlow graph
(\texttt{frozen\_model.pb}).  
To perform manual inference inside HALMD, all relevant model parameters
must be reconstructed from the computation graph.  
The node list extracted from the model (see Appendix~X) contains all
information required for this reconstruction.

\paragraph{Descriptor attributes.}
The descriptor section of the graph exposes the physical and structural
parameters needed to build the environment matrix:
\begin{itemize}
    \item cutoff radii (\verb|descrpt_attr/rcut|, \verb|descrpt_attr/rcut_smth|),
    \item neighbour selection size (\verb|descrpt_attr/sel|, \verb|descrpt_attr/original_sel|),
    \item type mapping and normalization tensors  
          (\verb|descrpt_attr/t_avg|, \verb|descrpt_attr/t_std|, \verb|descrpt_attr/ntypes|).
\end{itemize}
These values uniquely determine the construction of $\hat{R}_i$ and ensure 
that HALMD reproduces the same geometric preprocessing as DeepMD.

\paragraph{Embedding network.}
The embedding (filter) network parameters appear under
\verb|filter_type_0/*|.  
The suffix \verb|_0| indicates that these parameters correspond to a central
atom of species \(a = 0\).  
DeepMD-v2 stores a separate filter (embedding) network for each central-atom
species, so in a multi-species system one would observe additional blocks such as
\verb|filter_type_1/*|, \verb|filter_type_2/*|, and so on.

Within each \verb|filter_type_a| group, DeepMD-v2 uses node names that follow the
pattern
\[
  \texttt{matrix\_}\ell\texttt{\_}b,
  \qquad
  \texttt{bias\_}\ell\texttt{\_}b,
\]
where:
\begin{itemize}
    \item $\ell$ is the layer index of the embedding MLP,
    \item $b$ is the \emph{neighbour-species index}.
\end{itemize}

Thus, nodes such as
\verb|matrix_1_0|, \verb|matrix_2_0|, \verb|matrix_3_0|
and their corresponding
\verb|bias_1_0|, \verb|bias_2_0|, \verb|bias_3_0|
represent the three-layer embedding network used when:
\[
\text{central species } a = 0, \qquad
\text{neighbour species } b = 0.
\]

In a multi-species model, additional blocks would appear, for example:
\begin{itemize}
    \item \verb|matrix_1_1|, \verb|matrix_2_1|, \dots  for neighbours of species \(b=1\),
    \item \verb|matrix_1_2|, \verb|matrix_2_2|, \dots  for neighbours of species \(b=2\), etc.
\end{itemize}

Each embedding layer provides:
\begin{itemize}
    \item a weight matrix (\texttt{matrix\_}$\ell$\texttt{\_}$b$),
    \item a bias vector (\texttt{bias\_}$\ell$\texttt{\_}$b$),
    \item a nonlinear activation function node (typically \verb|Tanh|).
\end{itemize}

Collectively, these nodes define the mapping
\[
  \hat{s}_{ij} \mapsto G_{ij},
\]
which transforms the normalized inverse distance into an embedding vector for
the descriptor.


\paragraph{Descriptor production.}
The node \verb|ProdEnvMatA| encapsulates the internal DeepMD operation that
produces the environment matrix and its derivatives:
\begin{itemize}
    \item relative positions and angular components (\verb|o_rmat|),
    \item pairwise distances (\verb|o_rij|),
    \item neighbour list (\verb|o_nlist|),
    \item geometric derivatives (\verb|o_rmat_deriv|).
\end{itemize}
These outputs provide the ground truth for validating HALMD's geometric
derivative implementation.

\paragraph{Fitting network.}
The atomic-energy fitting network appears under the node groups  
\verb|layer_0_type_0/*|, \verb|layer_1_type_0/*|,  
\verb|layer_2_type_0/*|, and \verb|final_layer_type_0/*|.  
Each layer contributes:
\begin{itemize}
    \item a weight matrix,
    \item a bias vector,
    \item optional residual-timestep coefficients (\verb|idt| nodes),
    \item a nonlinear activation function (\verb|Tanh|).
\end{itemize}
Together, these nodes define the mapping from the descriptor $D_i$ to the
atomic energy $E_i$.

\paragraph{Energy and force outputs.}
The frozen graph also includes nodes providing:
\begin{itemize}
    \item total energy (\verb|o_energy|),
    \item per-atom energy (\verb|o_atom_energy|),
    \item forces (\verb|o_force|),
    \item virials (\verb|o_virial|, \verb|o_atom_virial|),
\end{itemize}
as well as all intermediate gradient nodes under \verb|gradients/*|,
used by DeepMD's automatic differentiation engine.

\medskip
Together, these extracted components provide a complete specification of the
trained potential: descriptor parameters, embedding transformations,
environment-matrix construction, fitting-network architecture, and the final
energy/force outputs.  Using these elements, HALMD can reproduce DeepMD-v2
inference exactly, without relying on TensorFlow.


\subsubsection{Extraction Procedure}

DeepMD-kit stores trained neural network potentials as a TensorFlow \texttt{frozen\_model.pb} file, accompanied by an \texttt{input.json} file containing model hyperparameters. The \texttt{.pb} file encodes all numerical weights, biases, and auxiliary tensors in the form of TensorFlow computation graph nodes, while the JSON file specifies the descriptor type, cutoff radius, number of neurons per layer, activation functions, and species layout. Following the methodology of DeepMD-kit v1 and v2 \cite{wang2018deepmd, zeng2023deepmdv2}, this thesis extracts all necessary model parameters from these files and converts them into an HDF5 representation suitable for efficient inference inside HALMD.

The extraction process begins by loading the TensorFlow graph definition from the \texttt{.pb} file. All tensors of type \texttt{Const} are scanned, and those whose node names match descriptor or fitting-network layers are decoded using TensorFlow’s low-level \texttt{tensor\_util.MakeNdarray}. The DeepMD model contains one set of parameters per atomic species, and the species ordering in the output strictly follows the ordering defined in the DeepMD input configuration. For the work presented here, the descriptor type is always the angular Smooth Edition descriptor \texttt{se\_e2\_a}, which DeepMD-kit v2 uses for multi-species models requiring both radial and angular correlation encoding.

\paragraph{Descriptor Parameters.}
For each species, the descriptor section consists of a stack of fully connected layers with user-specified neuron counts, activation functions, and optional residual time-step connections. The extraction script identifies each descriptor layer via a naming pattern of the form
% \[
% \texttt{filter\_type\_}<s>/\texttt{matrix\_}<k>_<t_n>, \qquad
% \texttt{filter\_type\_}<s>/\texttt{bias\_}<k>_{t_n},
% \]
\[
\mathtt{filter\_type\_}\langle s \rangle/\mathtt{matrix\_}\langle k \rangle\_\langle  t_n \rangle, \qquad
\mathtt{filter\_type\_}\langle s \rangle/\mathtt{bias\_}\langle k \rangle \_{\langle t_n \rangle},
\]
where \(s\) indexes the species, \(k\) is the layer index, and \(t_n\) enumerates neighbor-type channels. For each layer, the script records:
\begin{itemize}
    \item weight matrices,
    \item bias vectors,
    \item number of neurons,
    \item activation function (typically \texttt{tanh} or \texttt{linear} in the present work),
    \item the presence of a residual network branch.
\end{itemize}
DeepMD-v2 allows descriptor layers to use residual updates when \texttt{resnet\_dt=true}. In that case, a per-layer time-step tensor is extracted and marked in the output structure, though the use of residual updates depends on the model’s training configuration.

\paragraph{Embedding (Filter) Network.}
In the DeepMD architecture, the descriptor network is conceptually equivalent to the “filter” or embedding network described in \cite{wang2018deepmd, zeng2023deepmdv2}. Each species has its own filter-network parameters to account for species-dependent geometric correlations. The extracted filter-network weights are reshaped into a row-major layout compatible with HALMD’s GPU evaluation kernels.

\paragraph{Fitting Network.}
For each species, the fitting network (also called the main network) predicts the atomic energy contribution \(E_i\). The extraction process retrieves:
\begin{itemize}
    \item all intermediate fitting layers,
    \item species-dependent activation functions,
    \item weight and bias tensors for each layer,
    \item the species-specific atomic energy bias term \texttt{bias\_atom\_e},
    \item the parameters of the final output layer.
\end{itemize}
The fitting-network layers are identified using a template of the form
\[
\texttt{layer\_LL\_type\_}<k>_<s>/\texttt{matrix}, \qquad 
\texttt{layer\_LL\_type\_}<k>_<s>/\texttt{bias},
\]
and similarly for the final layer \texttt{final\_layer\_type\_<s>}. The final layer uses a fixed activation function, typically \texttt{linear}, consistent with the DeepMD energy formulation.

\paragraph{Normalization Parameters.}
DeepMD-v2 introduces descriptor normalization tensors \texttt{t\_avg} and \texttt{t\_std}, which are required to maintain numerical stability and descriptor smoothness. These tensors are extracted from the nodes
\[
\texttt{descrpt\_attr/t\_avg}, \qquad
\texttt{descrpt\_attr/t\_std},
\]
and stored in the HDF5 output so that HALMD can apply the same normalization as the original DeepMD model.

\paragraph{Organization and Output Format.}
All extracted parameters are written into a structured HDF5 file using a hierarchical layout:
\begin{itemize}
    \item global descriptor constants (cutoff radii, smoothing radii, normalization tensors),
    \item per-species descriptor network parameters,
    \item per-species fitting network parameters.
\end{itemize}
This structure mirrors the multi-species design of DeepMD-kit v2 and makes the extracted parameters directly usable by HALMD for inference. Unlike the single-species extraction used in the previous HALMD implementation \cite{cruz2025deepmd}, the present work extracts and stores fully species-resolved descriptor and fitting-network data, which are required for accurate multi-species DeepMD-v2 inference.

The resulting HDF5 file serves as the unified model representation for HALMD. During the simulation, HALMD loads this file, reconstructs the descriptor and neural networks using GPU-optimized data structures, and performs inference without relying on TensorFlow. This enables the Deep Potential model to be evaluated natively within HALMD’s simulation loop with high performance and full multi-species support.



\subsection{Coordinate System Extension }
\label{sec:env_construction}

A central contribution of this thesis is the redesign of HALMD’s environment–construction 
pipeline so that it follows the exact conventions required by DeepMD-v2. The earlier 
implementation by Cruz~\cite{cruz2025deepmd} relied on HALMD’s built-in periodic boundary 
handling and neighbour list, which correctly satisfy the minimum-image convention used in 
classical MD~\cite{colberg2011highly}. However, this approach provides only the minimal 
displacement between atoms and does not reproduce the full periodic environment expected by 
the Deep Potential (DP) descriptor pipeline. DeepMD-v2 requires explicit coordinate wrapping, 
ghost-cell expansion, species-aware neighbour grouping, and descriptor normalization 
\cite{wang2018deepmd, zeng2023deepmdv2}. These steps are essential for reproducing the 
descriptor inputs used during training.

The new DeePMD-style environment constructor introduced in this work consists of the following
components.

\subsubsection{Explicit coordinate wrapping}

DeepMD requires that all atomic coordinates are expressed inside the
primary simulation cell before any descriptor quantities are computed.
This is stricter than the minimum-image convention normally used in
classical molecular dynamics, where only the *differences* between
coordinates need to be mapped back into the simulation box.  
DeepMD, however, expects the *absolute coordinates* of every atom to lie
within the interval
\[
[0,\,L_\alpha)
\qquad\text{for each Cartesian direction } \alpha\in\{x,y,z\}.
\]


To ensure consistency, each coordinate component is mapped explicitly
into the primary simulation box using
\[
\tilde{r}_{i\alpha}
=
r_{i\alpha}
-
L_\alpha 
\left\lfloor \frac{r_{i\alpha}}{L_\alpha} \right\rfloor .
\]

This operation performs the following steps:

\begin{itemize}
    \item Compute the integer number of box lengths by which the particle
          has moved:
          \[
          n_\alpha = \left\lfloor \frac{r_{i\alpha}}{L_\alpha} \right\rfloor .
          \]
          This may be positive (particle drifted to the right), negative
          (particle drifted to the left), or zero.

    \item Subtract exactly $n_\alpha L_\alpha$ from the coordinate so that
          the result lies in the interval $[0, L_\alpha)$:
          \[
          \tilde{r}_{i\alpha} = r_{i\alpha} - n_\alpha L_\alpha .
          \]

    \item The wrapped coordinates $\tilde{\mathbf{r}}_i$ now correspond to
          the representation DeepMD expects as input.
\end{itemize}

This differs from the minimum-image convention, which only wraps
\emph{relative displacements}.  
DeepMD’s descriptor depends on absolute coordinates (through the
ghost-cell extension and species grouping), so the wrapping step is
necessary to guarantee that the computed descriptor matches the
TensorFlow implementation exactly.

% In summary, explicit coordinate wrapping enforces:

% \begin{itemize}
%     \item strict consistency with DeepMD-v2’s coordinate assumptions,
%     \item reproducible descriptor construction regardless of diffusion or
%           long simulation times,
%     \item the correct behaviour of ghost-cell expansion and neighbour
%           grouping.
% \end{itemize}

\subsubsection{Ghost-cell periodic extension}

DeepMD constructs atomic environments by explicitly replicating the simulation
cell in all spatial directions before evaluating descriptor quantities.  
This ensures that every atom, including those close to a periodic boundary,
retains a complete neighbourhood within the cutoff radius \( r_c \).  
In contrast, HALMD’s native neighbour list returns only the nearest periodic
image, which is sufficient for classical MD but does not supply the full set of
geometric relations required by the DeepMD descriptor pipeline.

To achieve DeepMD-compatible behaviour, the present work implements full
ghost-cell tiling. For each wrapped coordinate \( \tilde{\mathbf{r}}_i \), the
periodic images are generated as

\[
\mathbf{r}_i^{(\mathbf{s})}
=
\tilde{\mathbf{r}}_i
+
s_x L_x\, \mathbf{e}_x
+
s_y L_y\, \mathbf{e}_y
+
s_z L_z\, \mathbf{e}_z,
\qquad
s_\alpha \in [-n_{\mathrm{buff},\alpha},\, n_{\mathrm{buff},\alpha}],
\]

where \(L_\alpha\) are the box lengths and  

\[
n_{\mathrm{buff},\alpha}
=
\Big\lceil \frac{r_c}{L_\alpha} \Big\rceil
\]

ensures full spatial coverage of the cutoff sphere.  
For a cubic cell with \( n_{\mathrm{buff}} = 1 \), this produces
\(3^3 = 27\) images (the central box and its neighbouring tiles).  
Neighbour search is then performed over these images, and only the atoms whose
replicas fall within the cutoff are retained.

This explicit tiling is necessary for descriptor correctness: it guarantees that
all physically relevant neighbours are included, that the distances \( r_{ij} \)
and direction vectors \( \mathbf{r}_{ij} \) match those used by DeepMD-kit, and
that atoms near cell boundaries obtain descriptor rows identical to those of
atoms in the interior.  
By reproducing DeepMD's environment construction exactly, the HALMD
implementation achieves full compatibility with the DeepMD-v2 descriptor
generation process.


% \subsubsection{Consistency with DeepMD}

% DeepMD performs such tiling internally before constructing descriptors.
% Matching this behaviour ensures:

% \begin{itemize}
%     \item identical neighbour sets between HALMD and DeepMD,
%     \item consistent species grouping and sorting,
%     \item matching $r_{ij}$ and geometric rows $R_{ij}$,
%     \item matching normalized rows $\hat{R}_{ij}$,
%     \item descriptor vectors $D_i$ identical to TensorFlow.
% \end{itemize}

% Therefore, ghost-cell periodic extension is a fundamental requirement for
% achieving exact equivalence between HALMD and DeepMD-v2 inference.

% \subsubsection{Species-aware neighbour grouping}
% DeepMD-v2 requires that neighbours be:
% \begin{enumerate}
%     \item grouped by species,
%     \item sorted by distance within each species group,
%     \item truncated according to the \texttt{sel} vector.
% \end{enumerate}

% The original HALMD neighbour list is species-blind.  
% The new implementation constructs:
% \[
% \text{neighbors\_by\_type}[a]
% =
% \{\, j \mid t_j = a,\ r_{ij} < r_c \,\},
% \]
% sorts each group by \(r_{ij}\), and fills descriptor rows in the ordering required by 
% DeepMD-v2. This is essential for multi-species descriptors.


\subsubsection{Environment-matrix construction interface}
Previously, HALMD computed environment quantities directly from the neighbour list using 
minimum-image displacements.  
The new implementation builds environments from:
\begin{itemize}
    \item wrapped coordinates,
    \item ghost-expanded coordinates,
    \item species-ordered neighbour lists.
\end{itemize}
The construction of the descriptor matrices (\(R\), \(G\)) occurs later and is
documented in Section~3.4.

\subsubsection{Normalization using \texorpdfstring{\(t_{\mathrm{avg}}, t_{\mathrm{std}}\)}{tavg, tstd}}
DeepMD-v2 normalizes descriptor features using training-time statistics:
\[
X' = \frac{X - t_{\mathrm{avg}}}{t_{\mathrm{std}}}.
\]
The previous HALMD implementation lacked this step; the new pipeline integrates normalization 
directly, ensuring full compatibility with DeepMD-v2.

\subsubsection{Summary of improvements}
The updated environment-construction step introduces:
\begin{itemize}
    \item explicit DeePMD-style coordinate wrapping,
    \item full periodic ghost-cell expansion,
    \item species-aware neighbour grouping based on the \texttt{sel} configuration,
    \item a redesigned environment interface for descriptor construction,
    \item descriptor normalization matching DeepMD-v2.
\end{itemize}

These extensions form the necessary foundation for computing the \(R\) and \(G\) 
descriptor matrices (Section~3.4) and enable HALMD to accurately evaluate 
multi-species DeepMD-v2 models.


\subsubsection{Species-aware neighbour grouping}

DeepMD-v2 requires that neighbour atoms be organised in a very specific
species-dependent format before any descriptor quantities are computed.
This is fundamentally different from HALMD’s native neighbour list, which
treats all neighbours uniformly and returns a single distance-sorted list
independent of species.  
However, in DeepMD-v2 the descriptor is constructed under the assumption that
each central atom has a fixed, preallocated number of neighbour slots for 
\emph{each species type}, as defined by the model parameter \texttt{sel}:
\[
\texttt{sel} = 
\bigl[
    N_c^{(1)},\,
    N_c^{(2)},\,
    \ldots,\,
    N_c^{(B)}
\bigr],
\]
where $N_c^{(b)}$ is the number of neighbours of species $b$ that the model
expects for each central atom.

This fixed layout is not optional: every block of neighbours associated with a
neighbour species is passed through a species-specific embedding network
$N_{\theta}^{(a,b)}$, and thus the descriptor depends critically on 
\emph{which neighbour occupies which row} of the $R$ and $G$ matrices.

\paragraph{Constructing species-specific neighbour lists.}
To reproduce this behaviour, the new implementation replaces HALMD’s
species-blind neighbour structure with species-partitioned lists.
For a central atom $i$ and each species label $a$, HALMD now constructs
\[
\text{neighbors\_by\_type}[a]
=
\bigl\{\, j 
\;\big|\;
t_j = a,\; r_{ij} < r_c 
\,\bigr\},
\]
where $t_j$ is the species of neighbour $j$.
Each such list contains only the neighbours belonging to species $a$.

\paragraph{Distance sorting within each species group.}
DeepMD requires each species block to be internally sorted by distance.
Thus, for every species $a$, the list
$\text{neighbors\_by\_type}[a]$ is sorted according to $r_{ij}$:
\[
\text{sort}\bigl(\text{neighbors\_by\_type}[a],\; \text{key} = r_{ij} \bigr).
\]

This ensures that:
\begin{itemize}
    \item the nearest neighbours of each species appear first,
    \item rows of the descriptor matrices corresponding to species $a$ match the
          exact order used by DeepMD-kit,
    \item neighbour-dependent neural networks receive inputs in the correct,
          deterministic order.
\end{itemize}

% \paragraph{Applying the \texttt{sel} constraint.}
% Each species block is then truncated or padded to the required capacity
% $N_c^{(a)}$:
% \[
% \widehat{\text{neighbors\_by\_type}}[a]
% =
% \begin{cases}
% \text{first } N_c^{(a)} \text{ entries}, & 
%     \text{if the species has too many neighbours}, \\[4pt]
% \text{padded using the last valid neighbour}, &
%     \text{if the species has too few neighbours}.
% \end{cases}
% \]

% Padding (repeating the last valid neighbour) is exactly how DeepMD-kit
% guarantees a fixed-size descriptor even in sparse environments.


\subsubsection{Applying the \texttt{sel} constraint}

For each central atom, DeepMD-v2 requires that the descriptor contains a fixed
number of neighbour entries for each species, as prescribed by the vector
\[
\texttt{sel} = \bigl[ N_c^{(0)},\, N_c^{(1)},\, \dots \bigr].
\]
After grouping neighbours by species and sorting them by distance, each species
block is adjusted to have exactly \(N_c^{(a)}\) rows:
\[
\widehat{\mathrm{neighbors\_by\_type}}[a]
=
\begin{cases}
\text{first } N_c^{(a)} \text{ neighbours}, &
    \text{if more than } N_c^{(a)} \text{ are available}, \\[6pt]
\text{zero-padded rows}, &
    \text{if fewer than } N_c^{(a)} \text{ neighbours exist}.
\end{cases}
\]
This behaviour matches the DeepMD-v2 implementation, where missing neighbours
are represented by zero-valued geometric entries in the environment matrix.
Invalid neighbour indices are masked, resulting in
\[
R_{ij} = 0, \qquad
\hat{s}_{ij} = 0, \qquad
G_{ij} = N^{(a,b)}_{\theta}(0),
\]
for all padded rows. No duplication of the last valid neighbour is performed.

Zero-padding ensures that the assembled \(R\) and \(G\) matrices have the
fixed, model-dependent shape required by DeepMD-v2 and that the fitting network
receives inputs consistent with the TensorFlow/JAX implementation.



\paragraph{Constructing the full neighbour ordering.}
The final neighbour list for descriptor construction is obtained by concatenating
species blocks in the fixed species order used by the model:
\[
\text{neighbors\_ordered}
=
\bigl[
    \widehat{\text{neighbors\_by\_type}}[1],\;
    \widehat{\text{neighbors\_by\_type}}[2],\;
    \ldots,\;
    \widehat{\text{neighbors\_by\_type}}[B]
\bigr].
\]

Every descriptor row in the matrices $R$ and $G$ now corresponds to a specific
species and a specific position within that species block, exactly as expected
by the DeepMD-v2 architecture.

\paragraph{Importance of species grouping.}
This organisation is not merely a convenience; it is required because:
\begin{itemize}
    \item each species pair $(a,b)$ uses a different embedding network
          $N_{\theta}^{(a,b)}$,
    \item descriptor rows must map one-to-one to embedding-network evaluations,
    \item the quadratic descriptor $D_i$ implicitly assumes this grouping when
          computing $G^\mathrm{T}\hat{R}$,
    \item the fitting network is trained on descriptors in this exact ordering.
\end{itemize}

A deviation from this layout would result in incorrect $R$ and $G$ matrices,
leading to mismatched descriptors $D_i$, incorrect predicted energies, and
incorrect forces.

Thus, the introduction of species-aware neighbour grouping is a key requirement
for enabling HALMD to evaluate multi-species DeepMD-v2 models correctly.




% \subsection{Computation of R and G Matrices}
% \subsubsection{R Matrix}

\subsection{Construction of the Configuration Matrix $R$}
\label{sec:R_matrix_construction}

Once wrapped coordinates, ghost-cell expansions, and species-ordered neighbour
lists have been obtained (Section~\ref{sec:env_construction}), the next task is to
construct the geometric matrix \(R\).  
This matrix encodes the local atomic environment of a central atom \(i\) in a way that
is translationally, rotationally, and permutation invariant
\cite{wang2018deepmd,zeng2023deepmdv2}.  
Each of the \(N_c^{\mathrm{(total)}}\) neighbour slots contributes one row of four geometric
quantities.

\subsubsection{Relative displacement and inverse-distance quantities}

For each neighbour selected into the fixed-capacity list, the relative displacement is computed
using the \emph{wrapped and ghost-extended coordinates}:
\[
\mathbf{r}_{ij} = \mathbf{r}_j^{(\mathbf{s})} - \tilde{\mathbf{r}}_i
\quad\in \mathbb{R}^3 , 
\qquad
r_{ij} = \lVert \mathbf{r}_{ij} \rVert \in \mathbb{R}.
\]

\subsubsection{Raw geometric row}

DeepMD defines the raw geometric features of neighbour \(j\) (row index \(k\)) as
\[
R^{\mathrm{raw}}_{k}
=
\left[
    s_{ij},\;
    \frac{x_{ij}}{r_{ij}^{2}},\;
    \frac{y_{ij}}{r_{ij}^{2}},\;
    \frac{z_{ij}}{r_{ij}^{2}}
\right]
\quad\in\mathbb{R}^{4},
\qquad
s_{ij} = \frac{1}{r_{ij}}.
\]
Collecting all rows yields the matrix
\[
R^{\mathrm{raw}} \in \mathbb{R}^{N_c^{\mathrm{(total)}} \times 4}.
\]

\subsubsection{Cutoff behaviour and switching function}

To ensure smooth decay near the cutoff radius, DeepMD applies a quintic switching function:
\[
f_{\mathrm{sw}}(r)=
\begin{cases}
1, & r \le r_{\mathrm{sm}},\\[2pt]
x^3(-6x^2+15x-10)+1, & r_{\mathrm{sm}}<r<r_c,\\[2pt]
0, & r \ge r_c,
\end{cases}
\qquad
x = \frac{r-r_{\mathrm{sm}}}{r_c - r_{\mathrm{sm}}}.
\]

The resulting weight vector is
\[
w_k = f_{\mathrm{sw}}(r_{ij(k)}),
\qquad
w \in \mathbb{R}^{N_c^{\mathrm{(total)}}}.
\]

Applying this weight yields the weighted geometric matrix
\[
R^{\mathrm{w}}_{k,\alpha} = R^{\mathrm{raw}}_{k,\alpha} \, w_k ,
\qquad
R^{\mathrm{w}} \in \mathbb{R}^{N_c^{\mathrm{(total)}} \times 4}.
\]

\subsubsection{Species-aware neighbour ordering}

DeepMD-v2 requires neighbours to be arranged according to:
\begin{enumerate}
    \item neighbour species,
    \item increasing neighbour distance within each species,
    \item a fixed per-species capacity determined by
    \[
        \texttt{sel} = [N_c^{(0)},\, N_c^{(1)},\dots ].
    \]
\end{enumerate}

After this procedure, every central atom receives a geometric matrix with the same
shape:
\[
R^{\mathrm{w}} \in \mathbb{R}^{N_c^{\mathrm{(total)}} \times 4}.
\]

\subsubsection{Normalization of geometric features}

DeepMD-v2 applies feature-wise normalization using tensors
\[
t_{\mathrm{avg}},\ t_{\mathrm{std}}
\in \mathbb{R}^{N_c^{\mathrm{(total)}} \times 4}.
\]

The normalized geometric matrix is
\[
\hat{R}_{k,\alpha}
=
\frac{
    R^{\mathrm{w}}_{k,\alpha} - (t_{\mathrm{avg}})_{k,\alpha}
}{
    (t_{\mathrm{std}})_{k,\alpha}
},
\qquad
\hat{R} \in \mathbb{R}^{N_c^{\mathrm{(total)}} \times 4}.
\]

The first column,
\[
\hat{s}_{ij} = \hat{R}_{ij,0},
\]
serves as the scalar input to the embedding network.

\subsubsection{Summary}

The constructed matrix
\[
\hat{R} \in \mathbb{R}^{N_c^{\mathrm{(total)}} \times 4}
\]
is bitwise compatible with DeepMD-v2 and provides the geometric inputs for the next
stage of the descriptor.


% ======================================================================
\subsection{Construction of the Embedding Matrix \texorpdfstring{$G$}{G}}
\label{sec:G_matrix_construction}

The second stage of the DeepPot-SE descriptor transforms each normalized inverse 
distance $\hat{s}_{ij} = \hat{R}_{ij,0}$ into a high-dimensional embedding vector through 
a species-pair–specific \emph{filter network}.  
Collecting the embeddings for all neighbour slots produces the matrix
\[
G \in \mathbb{R}^{N_c^{\mathrm{(total)}} \times M},
\]
where $M$ is the embedding dimension , and
$N_c^{\mathrm{(total)}} = \sum_b N_c^{(b)}$ is the total neighbour capacity specified by 
the model’s \texttt{sel} vector.

\subsubsection{Input and Output Dimensionality}

Each neighbour slot contributes a single scalar input
\[
\hat{s}_{ij} \in \mathbb{R},
\]
obtained from the first column of the normalized geometric matrix $\hat{R}$.

The embedding network maps this scalar to an $M$-dimensional output:
\[
G_{ij} \in \mathbb{R}^M.
\]

Stacking the embedding vectors for all neighbour rows yields
\[
G =
\begin{bmatrix}
G_{i1}^{T} \\
G_{i2}^{T} \\
\vdots \\
G_{iN_c^{(\mathrm{total})}}^{T}
\end{bmatrix}
\in \mathbb{R}^{N_c^{\mathrm{(total)}} \times M}.
\]
The ordering of the rows is identical to the ordering of the rows of $\hat{R}$:
neighbours are grouped by species and sorted by distance within each species block.

\subsubsection{Structure of the Filter Network}

For each central–neighbour species pair $(a,b)$, DeepMD-v2 defines a dedicated 
multi-layer perceptron (MLP)
\[
N^{(a,b)}_{\theta}: \mathbb{R} \to \mathbb{R}^M.
\]

Each of these MLPs consists of:
\begin{itemize}
    \item an input layer of width $1$ (the scalar $\hat{s}_{ij}$),
    \item $L$ hidden layers with species-pair–dependent widths,
    \item a final output layer of dimension $M$,
    \item Tanh activations between layers (except the output layer).
\end{itemize}

For a filter network with layer widths
\[
1 \;\to\; H_1 \;\to\; H_2 \;\to\; \cdots \;\to\; H_{L} \;\to\; M,
\]
the forward pass is
\[
\begin{aligned}
h^{(1)} &= \tanh\!\left(W^{(1)}_{a,b}\,\hat{s}_{ij} + b^{(1)}_{a,b}\right), \\
h^{(\ell)} &= \tanh\!\left(W^{(\ell)}_{a,b}\,h^{(\ell-1)} + b^{(\ell)}_{a,b}\right),
\qquad \ell = 2,\dots,L, \\
G_{ij} &= W^{(L+1)}_{a,b}\,h^{(L)} + b^{(L+1)}_{a,b}.
\end{aligned}
\]

These weights $W^{(\ell)}_{a,b}$ and biases $b^{(\ell)}_{a,b}$ are extracted from the Frozen 
Model under node names of the form  
`\texttt{filter\_type\_a/matrix\_l\_b}` and `\texttt{filter\_type\_a/bias\_l\_b}` 
(see Section~3.2).  
The subscript $b$ corresponds to the neighbour species, confirming explicitly that 
the network architecture depends on both the central and neighbour species.

\subsubsection{Species\-Pair–Dependent Embedding Networks}

For a central atom of species $a$ and a neighbour of species $b$, the embedding is 
computed using the matching filter network:
\[
G_{ij} = N^{(a,b)}_{\theta}\!\left( \hat{s}_{ij} \right).
\]

Different neighbour species therefore produce embeddings through different MLPs 
even when the geometric distances are identical.  
This mechanism is essential for modelling alloy systems, where cross-species interactions 
must be distinguishable in the descriptor.

HALMD stores the entire set of such networks in a two-dimensional structure:
\[
\texttt{neural\_networks}[a][b] \;\equiv\; N^{(a,b)}_{\theta}.
\]

\subsubsection{Runtime Network Selection and Evaluation}

After species-grouped neighbour lists are constructed 
(Section~\ref{sec:env_construction}), each neighbour row $j$ is associated with a 
neighbour-species label $b_{(j)}$.  
The embedding vector for that row is obtained via
\[
G_{ij} = N^{(a,b_{(j)})}_{\theta}\!\left( \hat{s}_{ij} \right).
\]

Thus:
\begin{itemize}
    \item row $j$ of $\hat{R}$ determines $\hat{s}_{ij}$,
    \item the species list determines $b_{(j)}$,
    \item the corresponding network $N^{(a,b_{(j)})}_{\theta}$ is applied,
    \item the resulting $G_{ij}$ is stored in row $j$ of $G$.
\end{itemize}

This ensures strict alignment between rows of $\hat{R}$ and rows of $G$, exactly as in the 
DeepMD-v2 computational graph.

\subsubsection{Summary}

The embedding matrix $G$ is constructed through the following steps:
\begin{itemize}
    \item extract the scalar geometric input $\hat{s}_{ij}$ from the normalized $R$-matrix,
    \item select the appropriate species-pair filter network $N^{(a,b)}_\theta$,
    \item evaluate its multi-layer structure using the trained weights and biases,
    \item assemble all embedding vectors into the matrix  
          $G \in \mathbb{R}^{N_c^{\mathrm{(total)}} \times M}$.
\end{itemize}

The resulting matrix provides the learned many-body representation that, together with 
$\hat{R}$, forms the complete input to the quadratic descriptor described in the next section.


\subsection{Descriptor Computation}
\label{sec:descriptor_computation}

The DeepPot-SE (\texttt{se\_e2\_a}) descriptor encodes the environment of a central atom \(i\)
through a combination of (i) normalized geometric features collected in the matrix
\(\hat{R}_i\) and (ii) learned nonlinear embeddings collected in the matrix \(G_i\).
These two matrices are then contracted into a quadratic descriptor \(D_i\), which forms the
input to the species-dependent fitting network predicting the atomic energy \(E_i\).

DeepMD-v2 assigns neighbour capacity on a per-species basis.  
Given species set \(\mathcal{S}\) and model parameters \(N_c^{(b)}\) for each species
\(b\in\mathcal{S}\), the total neighbour capacity is
\[
N_c^{\mathrm{(total)}} =
\sum_{b\in\mathcal{S}} N_c^{(b)} .
\tag{4.1}
\]

All matrices involved in descriptor computation are dimensioned using this total capacity.

% ------------------------------------------------------------
\subsubsection{Normalized geometric matrix}

Once the wrapped coordinates, periodic images, and species-ordered neighbour lists have been
constructed (Section~\ref{sec:env_construction}), the geometric features for atom \(i\) are
assembled into the matrix
\[
\hat{R}_i \in \mathbb{R}^{N_c^{\mathrm{(total)}} \times 4}.
\tag{4.2}
\]

Each row corresponds to a specific neighbour slot and contains four quantities:
\begin{enumerate}
    \item the normalized inverse distance \(\hat{s}_{ij}\),
    \item the three normalized angular components
          \(\hat{x}_{ij}/r_{ij}^2\), \(\hat{y}_{ij}/r_{ij}^2\), \(\hat{z}_{ij}/r_{ij}^2\).
\end{enumerate}

Normalization follows the statistics extracted from the trained model:
\[
\hat{R}_{ij,\alpha} =
\frac{R_{ij,\alpha} - (t_{\mathrm{avg}})_{\alpha}}
     {(t_{\mathrm{std}})_{\alpha}},
\qquad \alpha=0,\dots,3.
\tag{4.3}
\]

The first column is used as the scalar geometric input for the nonlinear embedding:
\[
\hat{s}_{ij} = \hat{R}_{ij,0}.
\tag{4.4}
\]

This matrix therefore represents a geometrically structured, normalized description of the
neighbourhood of atom \(i\), with rows aligned to the species-block ordering of
DeepMD-v2.

% ------------------------------------------------------------
\subsubsection{Embedding matrix}

Each neighbour slot \(j\) has a known neighbour species \(b(j)\).  
DeepMD-v2 assigns a distinct filter (embedding) network for every central–neighbour pair
\((a,b)\), where \(a\) is the central species:
\[
G_{ij} = N_{\theta}^{(a,b(j))}(\hat{s}_{ij}),
\tag{4.5}
\]
with \(N_{\theta}^{(a,b)} : \mathbb{R} \to \mathbb{R}^{M}\).

Evaluating all such networks yields
\[
G_i \in \mathbb{R}^{N_c^{\mathrm{(total)}} \times M}.
\tag{4.6}
\]

Only the first \(M'\) embedding channels are required for the descriptor:
\[
G_i^{\mathrm{trunc}} = G_i[:,\, 1:M'] \in
\mathbb{R}^{N_c^{\mathrm{(total)}} \times M'}.
\tag{4.7}
\]

This embedding stage transforms the purely geometric scalar inputs \(\hat{s}_{ij}\) into
learned, species-dependent representations, with rows aligned exactly to the rows of
\(\hat{R}_i\).

% ------------------------------------------------------------
\subsubsection{Quadratic descriptor construction}

The DeepPot-SE descriptor is obtained by combining the geometric and embedding matrices into
a quadratic tensor:
\[
D_i
=
\frac{1}{(N_c^{\mathrm{(total)}})^2}
\,
\bigl(G_i^\mathsf{T}\hat{R}_i\bigr)
\bigl(\hat{R}_i^\mathsf{T} G_i^{\mathrm{trunc}}\bigr).
\tag{4.8}
\]

The intermediate products have the following shapes:
\[
\begin{aligned}
G_i^\mathsf{T}\hat{R}_i
&\in \mathbb{R}^{M \times 4},\\[4pt]
\hat{R}_i^\mathsf{T}G_i^{\mathrm{trunc}}
&\in \mathbb{R}^{4 \times M'},\\[4pt]
D_i &\in \mathbb{R}^{M \times M'}.
\end{aligned}
\tag{4.9}
\]

DeepMD-v2 flattens this matrix in row-major order to obtain the final descriptor vector:
\[
\mathbf{D}_i \in \mathbb{R}^{M M'}.
\tag{4.10}
\]

This vector is the complete learned representation of the neighbourhood of atom \(i\),
structured and normalized exactly as required by the DeepMD fitting network.

% ------------------------------------------------------------
\subsubsection{Summary}

The descriptor pipeline implemented in HALMD reproduces the DeepMD-v2 formulation without
modification. In particular:
\begin{itemize}
    \item the neighbour axis matches the species-resolved capacities \(N_c^{(b)}\),
    \item the geometric matrix \(\hat{R}_i\) implements all preprocessing steps required by
          DeepMD-v2,
    \item the embedding matrix \(G_i\) is produced by the full set of species-pair networks
          \(N_{\theta}^{(a,b)}\),
    \item the quadratic descriptor \(D_i\) and its scaling follow the DeepMD-v2 definition,
    \item the final descriptor vector \(\mathbf{D}_i\) matches TensorFlow output to
          floating-point precision.
\end{itemize}

This yields a complete, dimensionally consistent descriptor pipeline suitable for accurate,
high-performance DeepMD-v2 inference inside HALMD.


\subsection{Potential Energy Calculation}
\label{sec:potential_energy}

In the Deep Potential (DeePot) formalism, the total potential energy of a system of
\(N\) atoms is written as a sum of atomic contributions,
\begin{equation}
    E = \sum_{i=1}^{N} E_i,
\end{equation}
where the atomic energy \(E_i\) is obtained by evaluating a \emph{species-dependent}
fitting neural network on the descriptor vector \(\mathbf{D}_i\).
For an atom of species \(s_i\), the energy is computed as
\begin{equation}
    E_i = \mathrm{NN}_{s_i}(\mathbf{D}_i),
\end{equation}
where \(\mathrm{NN}_{s_i}\) denotes the DeepMD-v2 fitting network associated with
species \(s_i\).  
HALMD reconstructs these networks directly from the TensorFlow frozen graph
(\texttt{frozen\_model.pb}), including all linear layers, activation functions,
residual branches, and output transformations.

% ------------------------------------------------------------
\subsubsection{Baseline Implementation Prior to This Work}

The earlier HALMD implementation \cite{cruz2025deepmd} supported inference only for
\emph{single-species} DeepMD models.  
It successfully reproduced DeepMD energies for monoatomic systems but lacked the structural
components required for DeepMD-v2 multi-species inference:
\begin{itemize}
    \item only one fitting network for all atoms,
    \item no integration with multi-species descriptors (\(\hat{R}\), \(G\), \(D\)),
    \item no per-species energy shift (\texttt{bias\_atom\_e}),
    \item no mechanism for mapping descriptor vectors to the appropriate network.
\end{itemize}

Therefore, multi-component DeepMD-v2 models could not be evaluated within HALMD.

% ------------------------------------------------------------
\subsubsection{Species-Dependent Fitting Networks}

DeepMD-v2 defines a separate fitting neural network for each atomic species.
In the TensorFlow graph, these networks appear under node groups of the form:
\[
\texttt{layer\_}\ell\texttt{\_type\_}a/\texttt{matrix}, \qquad
\texttt{layer\_}\ell\texttt{\_type\_}a/\texttt{bias}, \qquad
\texttt{final\_layer\_type\_}a/*,
\]
where:
\begin{itemize}
    \item \(a\) indexes the central-atom species,
    \item \(\ell\) is the layer index,
    \item each pair of \texttt{matrix} and \texttt{bias} nodes defines an affine layer.
\end{itemize}

HALMD reconstructs the complete set
\[
\bigl\{\,\mathrm{NN}_0,\ \mathrm{NN}_1,\ \dots,\ \mathrm{NN}_{S-1}\,\bigr\},
\]
and evaluates the appropriate network according to
\[
E_i = \mathrm{NN}_{s_i}(\mathbf{D}_i).
\]

The names of the corresponding TensorFlow nodes make the species mapping explicit:
\begin{itemize}
    \item for species \(a=0\):  
    \texttt{layer\_0\_type\_0/matrix},  
    \texttt{layer\_1\_type\_0/matrix},  
    \texttt{final\_layer\_type\_0/matrix}, etc.
    \item for species \(a=1\):  
    \texttt{layer\_0\_type\_1/matrix},  
    \texttt{layer\_1\_type\_1/matrix},  
    \texttt{final\_layer\_type\_1/matrix}, etc.
\end{itemize}

These node groups fully specify all weights and biases of each species-dependent fitting network.

% ------------------------------------------------------------
\subsubsection{Per-Species Energy Offsets}

DeepMD-v2 optionally includes constant energy offsets for each species, stored under:
\[
\texttt{bias\_atom\_e}.
\]
HALMD extracts these tensors and evaluates
\begin{equation}
    E_i = \mathrm{NN}_{s_i}(\mathbf{D}_i) + b_{s_i}.
\end{equation}
This ensures that the final energy scale matches the one used during DeepMD model training.

% ------------------------------------------------------------
\subsubsection{Integration with the Multi-Species Descriptor Pipeline}

The descriptor vector \(\mathbf{D}_i\) supplied to the fitting network has the exact structure
expected by the DeepMD-v2 TensorFlow implementation.  
Its construction incorporates:
\begin{itemize}
    \item the normalized geometric matrix  
    \(\hat{R}_i \in \mathbb{R}^{N_c^{\mathrm{(total)}} \times 4}\),
    \item neighbour-slot ordering based on the model's \texttt{sel} vector,
    \item embedding vectors from species-pair networks  
          \(N^{(a,b)}_\theta\!\left(\hat{s}_{ij}\right)\),
    \item truncation to the first \(M'\) embedding channels,
    \item the quadratic descriptor construction  
          \(D_i \in \mathbb{R}^{M \times M'}\),
    \item flattening into  
          \(\mathbf{D}_i \in \mathbb{R}^{MM'}\).
\end{itemize}

The ordering and dimensionality of \(\mathbf{D}_i\) must match the structure expected by
the fitting networks defined in  
\texttt{layer\_\*\_type\_a/*} and \texttt{final\_layer\_type\_a/*}.
The revised HALMD implementation enforces this consistency.

% ------------------------------------------------------------
\subsubsection{Reproduction of DeepMD-v2 Energies}

With the full multi-species pipeline integrated, HALMD reproduces DeepMD-v2 energy predictions
to floating-point precision across all tested systems:
\begin{itemize}
    \item monoatomic models (Cu),
    \item multi-species alloy models (HEA),
    % \item oxide and ionic systems (garnet),
    \item alternative Copper models with different normalization statistics.
\end{itemize}

Both total energies and per-atom energies match DeepMD’s TensorFlow-based outputs.

% ------------------------------------------------------------
\subsubsection{Final Formulation}

HALMD now evaluates the energy of each atom using the DeepMD-v2 expression
\begin{equation}
    E_i = \mathrm{NN}_{s_i}(\mathbf{D}_i) + b_{s_i},
\end{equation}
and accumulates the total energy as
\begin{equation}
    E_{\mathrm{tot}} = \sum_{i=1}^{N} E_i.
\end{equation}

This formulation is bitwise compatible with the DeepMD-v2 computational graph and is suitable
for high-performance molecular dynamics simulations of multi-species systems within HALMD.


\section{Force Computation}
\label{sec:forces}

In the Deep Potential (DeeP) framework, forces are obtained as the negative 
gradient of the total potential energy with respect to atomic coordinates,
\[
    \mathbf{F}_i = -\frac{\partial E}{\partial \mathbf{r}_i}.
\]
Because the energy is produced through a multi-stage mapping involving geometric 
quantities (\(R\)), species-dependent embedding networks (\(G\)), and a 
species-dependent fitting network, the force computation requires evaluating a 
sequence of nested derivatives. 

DeepMD expresses this as a structured chain rule passing through:

\[
\mathbf{r}
\;\xrightarrow{\text{geometry}}\; 
R
\;\xrightarrow{\text{embedding}}\;
G
\;\xrightarrow{\text{fitting network}}\;
E,
\]
so that each force component involves contributions from all atoms appearing in 
the local environment of the corresponding descriptor.

The remainder of this section describes the derivative computation in HALMD.  
We begin with an overview of automatic differentiation (AD), which is used to 
evaluate all neural-network derivatives. We then detail the analytic derivatives 
of the geometry matrix \(R\), the descriptor \(D\), and finally show how these 
quantities are combined with the fitting-network derivatives to assemble the 
total atomic forces.


\subsection{Automatic Differentiation (AD)}
\label{sec:autodiff}

The computation of atomic forces in DeepMD requires propagating derivatives
through a sequence of transformations involving geometric preprocessing,
species-pair embedding networks, quadratic descriptor construction, and finally
a species-dependent fitting network.  
While the geometric derivatives admit closed-form expressions, the neural
networks involve nonlinear activations and residual connections that make
analytic differentiation impractical.  
For these components, HALMD employs \emph{automatic differentiation} (AD).

Automatic differentiation evaluates derivatives by decomposing a computation
into elementary operations and applying the chain rule locally
\cite{griewank2008evaluating,baydin2018automatic}.  
Two modes of AD are commonly used:

\subsubsection{Forward-mode AD (current HALMD implementation)}

Forward-mode AD propagates derivatives \emph{together with the value} of every
intermediate variable.  
Intuitively, each quantity in the computation carries a ``derivative tag’’ that
is updated whenever the variable participates in an operation.  

This mode is efficient when:
\begin{itemize}
    \item the number of inputs is small, and
    \item each function has a scalar or low-dimensional output.
\end{itemize}

These conditions hold for the DeepMD \emph{embedding networks}
$N_\theta^{(a,b)}$, each of which maps a single scalar input $\hat{s}_{ij}$ to
an $M$-dimensional output.  
HALMD therefore evaluates both the value
\[
G_{ij}, \qquad \frac{\partial G_{ij}}{\partial \hat{s}_{ij}}
\]
using forward-mode AD provided by Boost.Autodiff \cite{falcou2023boostautodiff}.

Forward-mode AD is also used in the fitting network to compute
$\partial E_i/\partial D_{i,\alpha}$, since each descriptor component
$D_{i,\alpha}$ can be marked as a differentiable input.

\subsubsection{Reverse-mode AD (not yet implemented)}

Reverse-mode AD performs the computation in two sweeps:
\begin{enumerate}
    \item a forward sweep computes all intermediate values,
    \item a backward sweep accumulates derivatives from the output back through
          the computational graph.
\end{enumerate}

Intuitively, reverse-mode AD works like backpropagation in neural networks:
one starts from the scalar loss (or, in this case, the atomic energy $E_i$)
and distributes sensitivities backward across all intermediate variables.

Reverse-mode AD is most efficient when:
\begin{itemize}
    \item the function has a \emph{single scalar output}, and
    \item the number of inputs is large.
\end{itemize}

This matches the structure of the \emph{fitting network}:
\[
E_i = \mathrm{NN}_{s_i}(\mathbf{D}_i),
\]
where the input dimension $\dim(\mathbf{D}_i) = MM'$ may be several hundred.
In such a setting, reverse-mode AD would greatly reduce computational cost, as
the full Jacobian $\partial E_i/\partial D_{i,\alpha}$ could be computed in a
\emph{single} backward pass.

At present, HALMD implements only forward-mode AD.  
Although correct, this results in one forward-mode evaluation per descriptor
entry, which is more expensive than necessary for large multi-species models.
Incorporating reverse-mode AD in the fitting network is therefore a
promising future improvement for performance-critical simulations.

\subsubsection{Hybrid analytic--AD differentiation}

All geometric derivatives---including those of the wrapped coordinates,
ghost-cell transformations, inverse-distance expressions, and switching
functions---are evaluated analytically.  
AD is used exclusively inside the neural-network components.  
This hybrid strategy offers:
\begin{itemize}
    \item exact agreement with the DeepMD-v2 TensorFlow implementation,
    \item efficient evaluation of nonlinear network derivatives,
    \item clear separation between analytic geometric contributions and AD-driven
          neural contributions.
\end{itemize}

The following sections detail the analytic components required to complete the
force calculation: derivatives of the geometric mapping, descriptor derivatives,
and the assembly of the total force.

% \subsection{Derivatives of the Geometry Matrix \texorpdfstring{$R$}{R}}
% \label{sec:R_derivative}

% For force computation we need the derivative of each row of the geometric
% matrix \(R\) with respect to the coordinates of the central atom \(i\).
% Because each entry in \(R\) depends on both the pairwise distance \(r_{ij}\)
% and the relative displacement \(\mathbf{r}_{ij}\), the derivative naturally
% splits into:
% \begin{itemize}
%     \item a \emph{radial} contribution (how the feature changes when the
%           distance \(r_{ij}\) changes),
%     \item an \emph{angular} contribution (how the feature changes when
%           the direction of \(\mathbf{r}_{ij}\) changes at fixed distance).
% \end{itemize}
% We derive both contributions in a consistent way with the DeepMD-v2 definition
% of \(R\) from Section~\ref{sec:R_matrix_construction}.

% \subsubsection{Step 1: Structure of a Row of \texorpdfstring{$R$}{R}}

% For a central atom \(i\) and a neighbour \(j\), the raw geometric row is
% \[
% R_{ij}
% =
% \left[
%     s_{ij},\;
%     \frac{x_{ij}}{r_{ij}^{2}},\;
%     \frac{y_{ij}}{r_{ij}^{2}},\;
%     \frac{z_{ij}}{r_{ij}^{2}}
% \right]
% f_{\mathrm{sw}}(r_{ij}),
% \]
% where
% \[
% \mathbf{r}_{ij} = \mathbf{r}_j - \mathbf{r}_i
% = (x_{ij}, y_{ij}, z_{ij}),
% \qquad
% r_{ij} = \lVert \mathbf{r}_{ij} \rVert,
% \qquad
% s_{ij} = \frac{1}{r_{ij}}.
% \]
% It is convenient to introduce the \emph{switched inverse distance}
% \[
% \tilde{s}_{ij} = s_{ij} f_{\mathrm{sw}}(r_{ij})
% = \frac{1}{r_{ij}}\, f_{\mathrm{sw}}(r_{ij}).
% \]
% With this notation, the components of \(R_{ij}\) can be written as
% \begin{align*}
% R_{ij,0} &= \tilde{s}_{ij}, \\
% R_{ij,1} &= \tilde{s}_{ij} \frac{x_{ij}}{r_{ij}}, \\
% R_{ij,2} &= \tilde{s}_{ij} \frac{y_{ij}}{r_{ij}}, \\
% R_{ij,3} &= \tilde{s}_{ij} \frac{z_{ij}}{r_{ij}}.
% \end{align*}


% We also define the unit direction vector
% \[
% \hat{\mathbf{r}}_{ij} = \frac{\mathbf{r}_{ij}}{r_{ij}}
% = (\hat{r}_x, \hat{r}_y, \hat{r}_z),
% \]
% and use \(d_\alpha \in \{x_{ij}, y_{ij}, z_{ij}\}\) to denote the Cartesian
% components of \(\mathbf{r}_{ij}\).  
% Then the three angular components can be written compactly as
% \[
% S_\alpha
% =
% \tilde{s}_{ij} \frac{d_\alpha}{r_{ij}},
% \qquad
% \alpha \in \{x,y,z\},
% \]
% so that
% \[
% R_{ij,1} = S_x, \quad
% R_{ij,2} = S_y, \quad
% R_{ij,3} = S_z.
% \]

% % \subsubsection{Step 2: Derivative of the Distance \texorpdfstring{$r_{ij}$}{rij}}

% % We first compute how the scalar distance \(r_{ij}\) changes when the central atom
% % moves. From
% % \[
% % r_{ij} = \sqrt{x_{ij}^2 + y_{ij}^2 + z_{ij}^2}
% % = \sqrt{d_x^2 + d_y^2 + d_z^2} = (d_x^2 + d_y^2 + d_z^2)^{1/2},
% % \]

% % we compute the derivative with respect to a single displacement component
% % \(d_\alpha \in \{d_x, d_y, d_z\}\).

% % Let
% % \[
% % u = d_x^2 + d_y^2 + d_z^2,
% % \qquad
% % r_{ij} = u^{1/2}.
% % \]
% % Applying the chain rule,
% % \[
% % \frac{\partial r_{ij}}{\partial d_\alpha}
% % =
% % \frac{\partial r_{ij}}{\partial u}
% % \frac{\partial u}{\partial d_\alpha}.
% % \]

% % The outer derivative is
% % \[
% % \frac{\partial r_{ij}}{\partial u}
% % = \frac{1}{2} u^{-1/2}
% % = \frac{1}{2\sqrt{u}}
% % = \frac{1}{2 r_{ij}}.
% % \]

% % The inner derivative is
% % \[
% % \frac{\partial u}{\partial d_\alpha}
% % = \frac{\partial}{\partial d_\alpha}
% % (d_x^2 + d_y^2 + d_z^2)
% % = 2 d_\alpha,
% % \]
% % since only the \(d_\alpha^2\) term depends on \(d_\alpha\).

% % Combining both factors gives
% % \[
% % \frac{\partial r_{ij}}{\partial d_\alpha}
% % =
% % \frac{1}{2 r_{ij}} (2 d_\alpha)
% % =
% % \frac{d_\alpha}{r_{ij}},
% % \qquad
% % \alpha \in \{x,y,z\}.
% % \]

% % Since \(d_\alpha = r_{j,\alpha} - r_{i,\alpha}\), the derivative with respect to
% % the central-atom coordinate is
% % \[
% % \frac{\partial d_\alpha}{\partial \mathbf{r}_i}
% % = -\,\mathbf{e}_\alpha,
% % \]
% % where \(\mathbf{e}_\alpha\) is the unit vector along the \(\alpha\)-axis.
% % Combining both gives
% % \[
% % \frac{\partial r_{ij}}{\partial \mathbf{r}_i}
% % =
% % \sum_{\alpha}
% % \frac{\partial r_{ij}}{\partial d_\alpha}
% % \frac{\partial d_\alpha}{\partial \mathbf{r}_i}
% % =
% % \sum_{\alpha}
% % \frac{d_\alpha}{r_{ij}}\,(-\mathbf{e}_\alpha)
% % =
% % -\,\hat{\mathbf{r}}_{ij}.
% % \]
% % % This sign is physically intuitive: moving atom \(i\) along
% % % \(\hat{\mathbf{r}}_{ij}\) reduces the distance to atom \(j\).

% \subsubsection{Step 2: Derivative of the Distance \texorpdfstring{$r_{ij}$}{rij}}

% We now compute how the scalar distance \( r_{ij} \) changes when the central atom
% \( i \) moves.  
% The distance is defined as
% \[
% r_{ij}
% = \sqrt{x_{ij}^2 + y_{ij}^2 + z_{ij}^2}
% = \sqrt{d_x^2 + d_y^2 + d_z^2}
% = (d_x^2 + d_y^2 + d_z^2)^{1/2},
% \]
% where \( d_\alpha = x_{j,\alpha} - x_{i,\alpha} \) for
% \( \alpha \in \{x,y,z\} \).

% To compute \( \partial r_{ij}/\partial d_\alpha \), we introduce the auxiliary
% variable
% \[
% u = d_x^2 + d_y^2 + d_z^2,
% \qquad
% r_{ij} = u^{1/2}.
% \]

% Using the chain rule,
% \[
% \frac{\partial r_{ij}}{\partial d_\alpha}
% =
% \frac{\partial r_{ij}}{\partial u}
% \frac{\partial u}{\partial d_\alpha}.
% \]

% The first factor is
% \[
% \frac{\partial r_{ij}}{\partial u}
% = \frac{1}{2} u^{-1/2}
% = \frac{1}{2\sqrt{u}}
% = \frac{1}{2 r_{ij}}.
% \]

% The second factor is obtained by differentiating
% \( u = d_x^2 + d_y^2 + d_z^2 \) with respect to \( d_\alpha \):
% \[
% \frac{\partial u}{\partial d_\alpha}
% = 2 d_\alpha,
% \]
% since all other terms are constant with respect to \( d_\alpha \).

% Combining both gives the familiar expression
% \[
% \frac{\partial r_{ij}}{\partial d_\alpha}
% =
% \frac{1}{2 r_{ij}} (2 d_\alpha)
% =
% \frac{d_\alpha}{r_{ij}},
% \qquad
% \alpha \in \{x,y,z\}.
% \]

% Next, we compute the derivative with respect to the coordinates of the central
% atom.  
% Since each displacement component is
% \[
% d_\alpha = r_{j,\alpha} - r_{i,\alpha},
% \]
% its derivative is
% \[
% \frac{\partial d_\alpha}{\partial \mathbf{r}_i}
% = -\,\mathbf{e}_\alpha,
% \]
% where \( \mathbf{e}_\alpha \) denotes the unit vector along the
% \( \alpha \)-axis.

% Applying the chain rule again,
% \[
% \frac{\partial r_{ij}}{\partial \mathbf{r}_i}
% =
% \sum_{\alpha}
% \frac{\partial r_{ij}}{\partial d_\alpha}
% \frac{\partial d_\alpha}{\partial \mathbf{r}_i}
% =
% \sum_{\alpha}
% \frac{d_\alpha}{r_{ij}}\,(-\mathbf{e}_\alpha)
% =
% -\,\frac{1}{r_{ij}}(d_x,d_y,d_z)
% =
% -\,\hat{\mathbf{r}}_{ij}.
% \]

% Thus the gradient of the interatomic distance with respect to the position of
% the central atom points \emph{opposite} to the direction of the neighbour
% vector, as expected geometrically.



% \subsubsection{Step 3: Derivative of the Switched Inverse Distance}

% The first component is
% \[
% R_{ij,0} = \tilde{s}_{ij} = \frac{1}{r_{ij}}\,f_{\mathrm{sw}}(r_{ij}).
% \]
% Using the product rule with respect to \(r_{ij}\),
% \begin{equation}
% \frac{\partial \tilde{s}_{ij}}{\partial r_{ij}}
% =
% -\,\frac{1}{r_{ij}^{2}}\,f_{\mathrm{sw}}(r_{ij})
% +
% \frac{1}{r_{ij}}
% \frac{\mathrm{d} f_{\mathrm{sw}}}{\mathrm{d} r}(r_{ij}).
% \label{eq:dsdr_R}
% \end{equation}
% By the chain rule,
% \[
% \frac{\partial R_{ij,0}}{\partial \mathbf{r}_i}
% =
% \frac{\partial \tilde{s}_{ij}}{\partial r_{ij}}
% \frac{\partial r_{ij}}{\partial \mathbf{r}_i}
% =
% -\,\frac{\partial \tilde{s}_{ij}}{\partial r_{ij}}\,\hat{\mathbf{r}}_{ij}.
% \]

% \subsubsection{Step 4: Radial Part of the Angular Components}

% Consider one angular component
% \[
% S_\alpha
% =
% \tilde{s}_{ij}\,\frac{d_\alpha}{r_{ij}}.
% \]
% First, we differentiate with respect to \(r_{ij}\), treating \(d_\alpha\) as
% temporarily fixed:
% \[
% \frac{\partial S_\alpha}{\partial r_{ij}}
% =
% \frac{\partial \tilde{s}_{ij}}{\partial r_{ij}}
% \frac{d_\alpha}{r_{ij}}
% +
% \tilde{s}_{ij}\,
% \frac{\partial}{\partial r_{ij}}\left(\frac{d_\alpha}{r_{ij}}\right).
% \]
% Since
% \[
% \frac{\partial}{\partial r_{ij}}\left(\frac{d_\alpha}{r_{ij}}\right)
% =
% d_\alpha\,\frac{\partial}{\partial r_{ij}}\left(\frac{1}{r_{ij}}\right)
% =
% -\,\frac{d_\alpha}{r_{ij}^{2}},
% \]
% we obtain
% \begin{equation}
% \frac{\partial S_\alpha}{\partial r_{ij}}
% =
% \frac{d_\alpha}{r_{ij}}\,
% \frac{\partial \tilde{s}_{ij}}{\partial r_{ij}}
% -
% \tilde{s}_{ij}\,\frac{d_\alpha}{r_{ij}^{2}}.
% \label{eq:dS_dr_R}
% \end{equation}
% Using \(\partial r_{ij} / \partial \mathbf{r}_i = -\,\hat{\mathbf{r}}_{ij}\),
% the radial part of the central-atom derivative is
% \[
% \left(\frac{\partial S_\alpha}{\partial \mathbf{r}_i}\right)_{\mathrm{rad}}
% =
% \frac{\partial S_\alpha}{\partial r_{ij}}
% \frac{\partial r_{ij}}{\partial \mathbf{r}_i}
% =
% -\,\frac{\partial S_\alpha}{\partial r_{ij}}\,\hat{\mathbf{r}}_{ij}.
% \]

% \subsubsection{Step 5: Angular Part from Explicit Coordinate Dependence}

% The same component \(S_\alpha\) also depends explicitly on the displacement
% component \(d_\alpha\).  
% To isolate this effect, we regard \(r_{ij}\) as fixed and differentiate only with
% respect to \(d_\alpha\):
% \[
% S_\alpha
% =
% \tilde{s}_{ij}\,\frac{d_\alpha}{r_{ij}}
% \quad\Rightarrow\quad
% \frac{\partial S_\alpha}{\partial d_\alpha}
% =
% \tilde{s}_{ij}\,\frac{1}{r_{ij}}.
% \]
% Using
% \[
% \frac{\partial d_\alpha}{\partial \mathbf{r}_i}
% = -\,\mathbf{e}_\alpha,
% \]
% we obtain the angular contribution
% \begin{equation}
% \left(\frac{\partial S_\alpha}{\partial \mathbf{r}_i}\right)_{\mathrm{ang}}
% =
% \frac{\partial S_\alpha}{\partial d_\alpha}
% \frac{\partial d_\alpha}{\partial \mathbf{r}_i}
% =
% -\,\frac{\tilde{s}_{ij}}{r_{ij}}\,\mathbf{e}_\alpha.
% \label{eq:S_ang_R}
% \end{equation}
% Note that the first component \(R_{ij,0} = \tilde{s}_{ij}\) does not depend
% explicitly on \(d_\alpha\) and therefore has no angular part.

% \subsubsection{Step 6: Full Derivative of the Raw Geometry Row}

% We can now combine radial and angular pieces.  
% For the isotropic component (\(\alpha=0\)):
% \[
% \frac{\partial R_{ij,0}}{\partial \mathbf{r}_i}
% =
% -\,\frac{\partial \tilde{s}_{ij}}{\partial r_{ij}}\,\hat{\mathbf{r}}_{ij}.
% \]
% For the angular components (\(\alpha = 1,2,3\)), writing
% \(R_{ij,\alpha} = S_\alpha\) with \(d_\alpha \in \{x_{ij},y_{ij},z_{ij}\}\):
% \begin{equation}
% \frac{\partial R_{ij,\alpha}}{\partial \mathbf{r}_i}
% =
% \left(\frac{\partial S_\alpha}{\partial \mathbf{r}_i}\right)_{\mathrm{rad}}
% +
% \left(\frac{\partial S_\alpha}{\partial \mathbf{r}_i}\right)_{\mathrm{ang}}
% =
% -\,\frac{\partial S_\alpha}{\partial r_{ij}}\,\hat{\mathbf{r}}_{ij}
% -\,\frac{\tilde{s}_{ij}}{r_{ij}}\,\mathbf{e}_\alpha,
% \label{eq:R_full_raw}
% \end{equation}
% with \(\partial S_\alpha / \partial r_{ij}\) given by
% Eq.~\eqref{eq:dS_dr_R}.  
% Equations~\eqref{eq:dsdr_R} and \eqref{eq:R_full_raw} are the analytic
% counterparts of the radial and angular pieces assembled in the Python prototype
% (\texttt{deriv\_radial} and \texttt{angular\_contrib}).

% \subsubsection{Step 7: Derivative of the Normalized Geometry Row}

% DeepMD-v2 applies component-wise normalization to each row of \(R\) using
% \(\,t_{\mathrm{avg}}\) and \(\,t_{\mathrm{std}}\)
% (Section~\ref{sec:R_matrix_construction}):
% \[
% \hat{R}_{ij,\alpha}
% =
% \frac{
%     R_{ij,\alpha} - (t_{\mathrm{avg}})_{\alpha}
% }{
%     (t_{\mathrm{std}})_{\alpha}
% }.
% \]
% Since the normalization parameters are constants,
% \[
% \frac{\partial \hat{R}_{ij,\alpha}}{\partial \mathbf{r}_i}
% =
% \frac{1}{(t_{\mathrm{std}})_{\alpha}}
% \frac{\partial R_{ij,\alpha}}{\partial \mathbf{r}_i}.
% \]
% This is exactly the scaling implemented in the Python routine
% \texttt{normalize\_env\_mat}, where the normalized derivative
% \texttt{o\_rmat\_deriv\_central} is obtained by multiplying the raw derivative
% by \(1 / t_{\mathrm{std}}\).

\subsection{Derivatives of the Geometry Matrix \texorpdfstring{$R$}{R}}
\label{sec:R_derivative}

To compute forces, we require the derivative of each row of the geometry
matrix \(R\) with respect to the coordinates of the central atom \(i\).
Each row depends on the neighbour displacement
\[
\mathbf{r}_{ij} = \mathbf{r}_j - \mathbf{r}_i = (d_x, d_y, d_z),
\qquad
r_{ij} = \lVert \mathbf{r}_{ij} \rVert,
\]
and on the switched inverse distance
\[
\tilde{s}_{ij} = \frac{1}{r_{ij}}\, f_{\mathrm{sw}}(r_{ij}).
\]
We now derive all components of \(\partial R_{ij} / \partial \mathbf{r}_i\)
using only the multivariate chain rule.

% -------------------------------------------------------
\subsubsection{Step 1: Rewrite the Components of \texorpdfstring{$R$}{R}}

For neighbour \(j\), the geometry row is
\[
R_{ij}
=
\bigl[
    \tilde{s}_{ij},\;
    \tilde{s}_{ij}\tfrac{d_x}{r_{ij}},\;
    \tilde{s}_{ij}\tfrac{d_y}{r_{ij}},\;
    \tilde{s}_{ij}\tfrac{d_z}{r_{ij}}
\bigr].
\]
We denote the three directional components compactly as
\[
S_\alpha
=
\tilde{s}_{ij}\,\frac{d_\alpha}{r_{ij}},
\qquad
\alpha \in \{x,y,z\}.
\]

% -------------------------------------------------------
\subsubsection{Step 2: Derivative of the Distance \texorpdfstring{$r_{ij}$}{rij}}

Following the derivation in Section~\ref{sec:R_matrix_construction}, the
distance can be written as
\[
r_{ij} = (d_x^2 + d_y^2 + d_z^2)^{1/2}.
\]
Using the chain rule,
\[
\frac{\partial r_{ij}}{\partial d_\alpha}
=
\frac{d_\alpha}{r_{ij}},
\qquad
\alpha \in \{x,y,z\}.
\]
Since
\[
d_\alpha = r_{j,\alpha} - r_{i,\alpha},
\]
we also have
\[
\frac{\partial d_\alpha}{\partial \mathbf{r}_i}
= -\,\mathbf{e}_\alpha,
\]
and combining these yields
\[
\frac{\partial r_{ij}}{\partial \mathbf{r}_i}
=
\sum_\alpha
\frac{d_\alpha}{r_{ij}}(-\mathbf{e}_\alpha)
=
-\,\frac{1}{r_{ij}}(d_x, d_y, d_z)
=
-\,\hat{\mathbf{r}}_{ij}.
\]

% -------------------------------------------------------

\subsubsection{Step 3: Derivative of the Switching Function}

The switching function used in DeepMD is
\[
f_{\mathrm{sw}}(r)=
\begin{cases}
1, & r \le r_{\mathrm{sm}},\\[4pt]
x^{3}(-6x^{2} + 15x - 10)+1, & r_{\mathrm{sm}} < r < r_c,\\[4pt]
0, & r \ge r_c,
\end{cases}
\qquad
x = \dfrac{r - r_{\mathrm{sm}}}{r_c - r_{\mathrm{sm}}}.
\]

Its derivative with respect to the distance \(r\) is obtained using the chain rule,
\[
\frac{\mathrm{d}f_{\mathrm{sw}}}{\mathrm{d}r}
=
\begin{cases}
0, & r \le r_{\mathrm{sm}},\\[6pt]
\dfrac{\mathrm{d}f_{\mathrm{sw}}}{\mathrm{d}x}
\cdot
\dfrac{1}{\,r_c - r_{\mathrm{sm}}\,},
& r_{\mathrm{sm}} < r < r_c,\\[10pt]
0, & r \ge r_c,
\end{cases}
\]
where
\[
\frac{\mathrm{d}f_{\mathrm{sw}}}{\mathrm{d}x}
=
x^{2}\left( -30x^{2} + 60x - 30 \right).
\]

This expression enters the derivative of the switched inverse distance
\(\tilde{s}_{ij} = r_{ij}^{-1} f_{\mathrm{sw}}(r_{ij})\),
via
\[
\frac{\partial \tilde{s}_{ij}}{\partial r_{ij}}
=
-\,\frac{1}{r_{ij}^{2}} f_{\mathrm{sw}}(r_{ij})
+
\frac{1}{r_{ij}}
\frac{\mathrm{d}f_{\mathrm{sw}}}{\mathrm{d}r}(r_{ij}),
\]
which is subsequently used in the derivatives of all components of the
geometry matrix \(R\).


\subsubsection{Step 4: Derivative of the Switched Inverse Distance}

The first component of the geometry row is
\[
R_{ij,0} = \tilde{s}_{ij}
= \frac{1}{r_{ij}} f_{\mathrm{sw}}(r_{ij}).
\]
Differentiating with respect to \(r_{ij}\),
\[
\frac{\partial \tilde{s}_{ij}}{\partial r_{ij}}
=
-\,\frac{1}{r_{ij}^{2}} f_{\mathrm{sw}}(r_{ij})
+
\frac{1}{r_{ij}}\, \frac{\mathrm{d}f_{\mathrm{sw}}}{\mathrm{d}r}(r_{ij}).
\]
Applying the chain rule with
\(\partial r_{ij}/\partial \mathbf{r}_i = -\,\hat{\mathbf{r}}_{ij}\),
\[
\frac{\partial R_{ij,0}}{\partial \mathbf{r}_i}
=
\frac{\partial \tilde{s}_{ij}}{\partial r_{ij}}
\frac{\partial r_{ij}}{\partial \mathbf{r}_i}
=
-\,\frac{\partial \tilde{s}_{ij}}{\partial r_{ij}}\,\hat{\mathbf{r}}_{ij}.
\]

% -------------------------------------------------------
\subsubsection{Step 5: Derivative of the Directional Components}

Consider a single directional term
\[
S_\alpha
=
\tilde{s}_{ij}\,\frac{d_\alpha}{r_{ij}}.
\]
We differentiate it with respect to \(\mathbf{r}_i\) using the product rule.
First differentiate with respect to distance:
\[
\frac{\partial}{\partial r_{ij}}
\left(\frac{d_\alpha}{r_{ij}}\right)
=
d_\alpha\left(-\frac{1}{r_{ij}^{2}}\right)
=
-\,\frac{d_\alpha}{r_{ij}^{2}}.
\]
Then
\[
\frac{\partial S_\alpha}{\partial r_{ij}}
=
\frac{\partial \tilde{s}_{ij}}{\partial r_{ij}}
\frac{d_\alpha}{r_{ij}}
-
\tilde{s}_{ij}\,\frac{d_\alpha}{r_{ij}^{2}}.
\]

Next we differentiate \(S_\alpha\) with respect to the displacement component
\(d_\alpha\):
\[
\frac{\partial S_\alpha}{\partial d_\alpha}
=
\tilde{s}_{ij}\,\frac{1}{r_{ij}}.
\]
Using \(\partial d_\alpha / \partial \mathbf{r}_i = -\,\mathbf{e}_\alpha\), the
chain rule gives
\[
\frac{\partial S_\alpha}{\partial \mathbf{r}_i}
=
\frac{\partial S_\alpha}{\partial r_{ij}}
\frac{\partial r_{ij}}{\partial \mathbf{r}_i}
+
\frac{\partial S_\alpha}{\partial d_\alpha}
\frac{\partial d_\alpha}{\partial \mathbf{r}_i}.
\]

Substituting all expressions:
\[
\frac{\partial S_\alpha}{\partial \mathbf{r}_i}
=
-
\left(
\frac{\partial \tilde{s}_{ij}}{\partial r_{ij}}
\frac{d_\alpha}{r_{ij}}
-
\tilde{s}_{ij}\,\frac{d_\alpha}{r_{ij}^{2}}
\right)
\hat{\mathbf{r}}_{ij}
-
\frac{\tilde{s}_{ij}}{r_{ij}}\,\mathbf{e}_\alpha.
\]

This formula directly produces the derivative of
\(R_{ij,1}, R_{ij,2}, R_{ij,3}\).

% -------------------------------------------------------
\subsubsection{Step 6: Normalized Geometry Derivative}

DeepMD-v2 uses component-wise normalization:
\[
\hat{R}_{ij,\alpha}
=
\frac{
    R_{ij,\alpha} - (t_{\mathrm{avg}})_\alpha
}{
    (t_{\mathrm{std}})_\alpha
}.
\]
Since the normalization parameters are constants,
\[
\frac{\partial \hat{R}_{ij,\alpha}}{\partial \mathbf{r}_i}
=
\frac{1}{(t_{\mathrm{std}})_\alpha}
\frac{\partial R_{ij,\alpha}}{\partial \mathbf{r}_i},
\]


\subsubsection{Step 7: Derivative with Respect to the Neighbour Coordinate \texorpdfstring{$\mathbf{r}_j$}{rj}}

All formulas above were derived for the derivative with respect to the
\emph{central} atom \(i\).  
The derivative with respect to the \emph{neighbour} atom \(j\) is obtained
immediately from the identity
\[
\mathbf{r}_{ij} = \mathbf{r}_j - \mathbf{r}_i,
\]
which implies
\[
\frac{\partial d_\alpha}{\partial \mathbf{r}_j}
= +\,\mathbf{e}_\alpha,
\qquad
\text{and}
\qquad
\frac{\partial d_\alpha}{\partial \mathbf{r}_i}
= -\,\mathbf{e}_\alpha.
\]

Using the chain rule for the distance \(r_{ij}\),
\[
\frac{\partial r_{ij}}{\partial \mathbf{r}_j}
=
\sum_\alpha
\frac{\partial r_{ij}}{\partial d_\alpha}
\frac{\partial d_\alpha}{\partial \mathbf{r}_j}
=
\sum_\alpha
\frac{d_\alpha}{r_{ij}}\,\mathbf{e}_\alpha
=
+\,\hat{\mathbf{r}}_{ij},
\]
which is exactly the opposite of the central-atom derivative
\(\partial r_{ij}/\partial \mathbf{r}_i = -\hat{\mathbf{r}}_{ij}\).

\vspace{1em}

\noindent\textbf{As a consequence, every derivative with respect to}
\(\mathbf{r}_j\)
\textbf{is simply the negative of the corresponding derivative with respect to}
\(\mathbf{r}_i\):
\[
\frac{\partial R_{ij,\alpha}}{\partial \mathbf{r}_j}
=
-\,
\frac{\partial R_{ij,\alpha}}{\partial \mathbf{r}_i},
\qquad
\alpha = 0,1,2,3.
\]

This follows because every dependence of \(R_{ij}\) on atomic positions appears
only through the displacement vector \(\mathbf{r}_{ij}\), and exchanging the
differentiation variable from \(\mathbf{r}_i\) to \(\mathbf{r}_j\) reverses the
sign of every chain-rule factor:
\[
\frac{\partial}{\partial \mathbf{r}_j}
(\mathbf{r}_j - \mathbf{r}_i)
= +\mathbf{I},
\qquad
\frac{\partial}{\partial \mathbf{r}_i}
(\mathbf{r}_j - \mathbf{r}_i)
= -\mathbf{I}.
\]

Thus, once the derivative with respect to the central atom is known, the
neighbour-atom derivative requires no further computation.


\subsection{Descriptor Derivative}
\label{sec:descriptor_derivative}

The DeepPot-SE descriptor is constructed from the interaction between the
embedding matrix \(G \in \mathbb{R}^{N_c \times M}\) and the normalized geometric
matrix \(\hat{R} \in \mathbb{R}^{N_c \times 4}\).  
Their contraction enters through the intermediate matrix
\[
C = G^{\mathsf T}\, \hat{R} 
\qquad 
C \in \mathbb{R}^{M \times 4}.
\]
The matrix \(C\) aggregates all geometry–embedding interactions and forms the
foundation of the quadratic descriptor used in DeepMD.

\medskip

The full (extended) descriptor is defined as the scaled Gram matrix
\[
D_{\mathrm{ext}}
=
\frac{1}{N_c^2}\, C C^{\mathsf T}
\qquad
D_{\mathrm{ext}} \in \mathbb{R}^{M \times M}.
\]
This construction ensures permutation invariance of neighbours and couples all
embedding channels quadratically.

However, the actual DeepPot-SE descriptor used by the fitting network consists
only of the first \(M'\) columns of \(D_{\mathrm{ext}}\):
\[
D = D_{\mathrm{ext}}[:,\,1\!:\!M'], 
\qquad
D \in \mathbb{R}^{M \times M'}.
\]
Thus, the descriptor truncation is implemented by a simple column restriction.

\subsubsection*{Derivative with respect to the intermediate matrix \(C\)}

Because \(D_{\mathrm{ext}}\) is a symmetric quadratic form in \(C\), its matrix
derivative follows directly from standard results:
\[
\frac{\partial D_{\mathrm{ext}}}{\partial C}
=
\frac{2}{N_c^2}\, C.
\]
This expression indicates that each column of \(C\) contributes linearly to the
gradient, scaled by the normalization factor \(N_c^{-2}\).

Since the final descriptor \(D\) keeps only the first \(M'\) columns of
\(D_{\mathrm{ext}}\), the derivative of \(D\) with respect to \(C\) is obtained by
restricting the above gradient to these columns:
\[
\frac{\partial D}{\partial C}
=
\left(
\frac{\partial D_{\mathrm{ext}}}{\partial C}
\right)[:,\,1\!:\!M']
=
\left(
\frac{2}{N_c^2}\, C
\right)[:,\,1\!:\!M'].
\]

\subsubsection*{Derivative with respect to the normalized geometric matrix}

The intermediate matrix \(C\) depends linearly on \(\hat{R}\),
\[
C = G^{\mathsf T}\, \hat{R},
\]
so its derivative with respect to \(\hat{R}\) is simply the transpose of the
embedding matrix:
\[
\frac{\partial C}{\partial \hat{R}}
=
G^{\mathsf T}.
\]

Applying the chain rule yields the descriptor derivative with respect to the
normalized geometry:
\[
\boxed{
\frac{\partial D}{\partial \hat{R}}
=
\frac{\partial D}{\partial C}
\,
\frac{\partial C}{\partial \hat{R}}
=
\left(
\frac{2}{N_c^2}\, C
\right)[:,\,1\!:\!M']
G^{\mathsf T}.
}
\]

This expression plays a central role in the force derivation, as it couples the
geometric derivatives of \(\hat{R}\) (Section~\ref{sec:R_derivative})
with the embedding and descriptor structures, enabling full propagation of
gradients from the energy model back to atomic coordinates.



\subsection{Fitting Network Derivative}
\label{sec:fitting_network_derivative}

The final stage of the DeepMD--v2 force pipeline is the differentiation of the
species-dependent fitting network that maps the descriptor of atom~$i$ to its atomic
energy,
\[
E_i = \mathrm{NN}_{s_i}(\mathbf{D}_i),
\]
where $s_i$ denotes the species of atom~$i$ and
$\mathbf{D}_i \in \mathbb{R}^{M'}$ is the descriptor vector constructed in
Section~\ref{sec:descriptor_computation}.  
To assemble forces, we require the gradient (Jacobian row)
\[
\frac{\partial E_i}{\partial D_{i,\alpha}},
\qquad
\alpha = 1,\dotsc,M',
\]
which measures the sensitivity of the atomic energy with respect to each descriptor
component.

In the frozen TensorFlow graph, the fitting network for species $s$ is encoded by
nodes such as
% \[
% \texttt{layer\_0\_type\_s/matrix},\quad
% \texttt{layer\_0\_type\_s/bias},\quad
% \dots,\quad
% \texttt{final\_layer\_type\_s/matrix},\quad
% \texttt{final\_layer\_type\_s/bias},
% \]

\[
\begin{aligned}
\texttt{layer\_0\_type\_s/matrix},\quad
\texttt{layer\_0\_type\_s/bias}, \\
\dots,\quad
\texttt{final\_layer\_type\_s/matrix},\quad
\texttt{final\_layer\_type\_s/bias}.
\end{aligned}
\]

together with optional residual-scaling nodes \texttt{idt} and the per-species energy
offset \texttt{bias\_atom\_e}.  
These parameters are extracted as described in
Section~\ref{sec:model_parameter_extraction} and reconstructed in HALMD’s
neural-network module.

\subsubsection{Forward-mode automatic differentiation of the fitting network}

The fitting networks in DeepMD--v2 consist of several fully connected layers with
nonlinear activation functions, optional residual connections, and timestep-related
scalings.  
The resulting mapping
\[
\mathbf{D}_i \longmapsto E_i
\]
is highly nonlinear, and an explicit hand-derived expression for
$\partial E_i / \partial D_{i,\alpha}$ would be lengthy and error-prone.

Instead, HALMD evaluates these derivatives using \emph{forward-mode automatic
differentiation} (AD) provided by Boost.Autodiff~\cite{falcou2023boostautodiff}.  
Conceptually, forward-mode AD augments each input component with an ``infinitesimal''
tangent and propagates both value and derivative through all primitive operations.

For the fitting network, this proceeds as follows for each descriptor component
$\alpha$:

\begin{enumerate}
    \item The descriptor vector $\mathbf{D}_i$ is promoted to an array of
          autodiff variables, where the $\alpha$-th component is seeded with unit
          tangent and all others with zero tangent.

    \item This augmented input is propagated forward through all layers of the
          network, using the same weights, biases, activation functions, and
          residual connections as in the standard inference.

    \item At the output, the autodiff scalar representing $E_i$ contains both the
          energy value and its derivative with respect to $D_{i,\alpha}$.  The
          derivative part is read out as
          $\partial E_i / \partial D_{i,\alpha}$.
\end{enumerate}

Repeating this procedure for $\alpha = 1,\dotsc,M'$ yields the full gradient
\[
\boxed{
\nabla_{\mathbf{D}_i} E_i
=
\left(
\frac{\partial E_i}{\partial D_{i,1}},
\ldots,
\frac{\partial E_i}{\partial D_{i,M'}}
\right)
}
\]
which HALMD stores as the Jacobian row associated with atom~$i$.

From an algorithmic point of view, this is equivalent to computing one
forward pass per descriptor component, i.e.\ the cost scales linearly with $M'$.
For the descriptor sizes considered in this work, this is acceptable and greatly
simplifies the implementation because no explicit backpropagation code is required.

\subsubsection{Relation to reverse-mode AD and future improvements}

For scalar outputs and high-dimensional inputs, \emph{reverse-mode} AD is
theoretically more efficient than forward mode, as it computes the entire gradient
$\nabla_{\mathbf{D}_i} E_i$ in a single backward sweep.  
This is the strategy employed by deep-learning frameworks such as
TensorFlow~\cite{abadi2016tensorflow} and PyTorch~\cite{paszke2019pytorch}.

In the present HALMD implementation, only forward-mode AD has been realized for the
fitting networks.  
Extending the neural-network module to support reverse-mode AD would reduce the
asymptotic cost of gradient evaluation and is therefore a natural target for future
optimisation.  
Nevertheless, the forward-mode approach used here already provides exact
derivatives of the reconstructed fitting networks and is sufficient to achieve
bitwise agreement with DeepMD--v2 energies and forces for all tested models.


% \subsection{Final Force Assembly}
% \label{sec:final_force_assembly}

% The total force on an atom \(k\) is obtained from the gradient of the total
% potential energy with respect to its Cartesian coordinates:
% \[
% \mathbf{F}_k
% =
% -\,\frac{\partial E}{\partial \mathbf{r}_k},
% \qquad
% E = \sum_{i} E_i(D_i),
% \]
% where each atomic energy \(E_i\) depends on the descriptor \(D_i\) of atom \(i\).

% \subsubsection*{Chain rule structure}

% Applying the multivariate chain rule gives
% \[
% \mathbf{F}_k
% =
% -\,\sum_{i}
% \frac{\partial E_i}{\partial D_i}
% :
% \frac{\partial D_i}{\partial \mathbf{r}_k},
% \]
% where \(A:B = \mathrm{Tr}(A^{T}B)\) denotes the Frobenius contraction.
% The force is therefore assembled entirely from two ingredients:

% \begin{enumerate}
%     \item the derivative of the fitting network,
%     \[
%         \frac{\partial E_i}{\partial D_i},
%     \]
%     which is computed automatically by the autodiff machinery
%     inside the neural-network evaluator (Section~\ref{sec:autodiff});

%     \item the derivative of the descriptor,
%     \[
%         \frac{\partial D_i}{\partial \mathbf{r}_k},
%     \]
%     which is constructed by combining the geometric and embedding-network
%     derivatives:
%     \[
%     \frac{\partial D_i}{\partial \mathbf{r}_k}
%     =
%     \frac{\partial D_i}{\partial \hat{R}_i}
%     :\frac{\partial \hat{R}_i}{\partial \mathbf{r}_k}
%     \;+\;
%     \frac{\partial D_i}{\partial G_i}
%     :\frac{\partial G_i}{\partial \mathbf{r}_k}.
%     \]
% \end{enumerate}

% The two inner derivatives have already been obtained:
% \begin{itemize}
%     \item \(\partial \hat{R}_i / \partial \mathbf{r}_k\)  
%           is the analytic geometric derivative derived in
%           Section~\ref{sec:R_derivative},
%     \item \(\partial G_i / \partial \mathbf{r}_k\)  
%           follows from autodifferentiation of the filter networks
%           (Section~\ref{sec:autodiff}),
%     \item \(\partial D_i / \partial \hat{R}_i\) and
%           \(\partial D_i / \partial G_i\)  
%           are provided by the descriptor derivative formulas in
%           Section~\ref{sec:descriptor_derivative}.
% \end{itemize}

% \subsubsection*{Final expression for the force}

% Substituting all components produces the complete DeepMD-v2 force expression:
% \[
% \boxed{
% \mathbf{F}_k
% =
% -\sum_{i}
% \frac{\partial E_i}{\partial D_i}
% :\left[
% \frac{\partial D_i}{\partial \hat{R}_i}
% :\frac{\partial \hat{R}_i}{\partial \mathbf{r}_k}
% \;+\;
% \frac{\partial D_i}{\partial G_i}
% :\frac{\partial G_i}{\partial \mathbf{r}_k}
% \right].
% }
% \]

% This formula matches the force evaluation pipeline of DeepMD-kit~v2 exactly.
% All model-specific components


\subsection{Final Force Assembly}
\label{sec:final_force_assembly}

We now combine all derivative components derived in the preceding sections to obtain
the explicit DeepMD--v2 force expression.  
The total potential energy is
\[
E = \sum_{i} E_i(D_i),
\]
so the force on atom $k$ follows from
\[
\mathbf{F}_k
=
-\,\frac{\partial E}{\partial \mathbf{r}_k}
=
-\,\sum_{i}
\frac{\partial E_i}{\partial D_i}
\frac{\partial D_i}{\partial \mathbf{r}_k}.
\tag{4.1}
\]

This expression backpropagates the fitting-network derivative through the descriptor
construction pathway
\[
\mathbf{r}
\;\longrightarrow\;
\hat{R}
\;\longrightarrow\;
G
\;\longrightarrow\;
D
\;\longrightarrow\;
E.
\]

% ---------------------------------------------------------
\subsubsection{Contribution from the fitting network}

From Section~\ref{sec:fitting_network_derivative}, forward-mode AD yields the
Jacobian
\[
\boxed{
\frac{\partial E_i}{\partial D_i}
=
J_i \in \mathbb{R}^{M'}
},
\]
where $J_{i,\alpha}$ is the sensitivity of the atomic energy to the $\alpha$-th
descriptor component.  
This vector forms the final gradient that propagates backward through the descriptor.

% ---------------------------------------------------------
\subsubsection{Contribution from the descriptor}

From Section~\ref{sec:descriptor_derivative}, the descriptor derivative is
\[
\boxed{
\frac{\partial D_i}{\partial \mathbf{r}_k}
=
\biggl[
\frac{\partial D_i}{\partial \hat{R}_i}
\;+\;
\frac{\partial D_i}{\partial G_i}
\,
\frac{\partial G_i}{\partial \hat{s}_i}
\,
\frac{\partial \hat{s}_i}{\partial \hat{R}_i}
\biggr]
\frac{\partial \hat{R}_i}{\partial \mathbf{r}_k}
}
\tag{4.2}
\]
where
\begin{itemize}
    \item $\hat{s}_i$ denotes the vector of normalized inverse distances (first column of $\hat{R}_i$),
    \item $\partial \hat{s}_i / \partial \hat{R}_i$ is the projector selecting the first column,
    \item ``$:$'' denotes Frobenius contraction over neighbour and feature indices.
\end{itemize}

No further expansion is necessary: all factors have been derived previously.

% ---------------------------------------------------------
\subsubsection{Contribution from geometric derivatives}

The only dependence on atomic coordinates appears through the normalized geometry,
whose derivative
\[
\frac{\partial \hat{R}_i}{\partial \mathbf{r}_k}
\]
was derived analytically in Section~\ref{sec:R_derivative}, including contributions
from:
\begin{itemize}
    \item relative displacements $\mathbf{r}_{ij}$,
    \item inverse distances $1/r_{ij}$,
    \item directional components $d_\alpha/r_{ij}^2$,
    \item and the switching function $f_{\mathrm{sw}}(r_{ij})$.
\end{itemize}

This tensor provides the geometric part of the chain rule.

% ---------------------------------------------------------
\subsubsection{Final force expression}

Substituting Eq.~(4.2) into Eq.~(4.1) gives the complete DeepMD--v2 force:
\[
\boxed{
\mathbf{F}_k
=
-
\sum_i
\left( \frac{\partial E_i}{\partial D_i} \right)
\biggl[
\frac{\partial D_i}{\partial \hat{R}_i}
+
\frac{\partial D_i}{\partial G_i}
\,
\frac{\partial G_i}{\partial \hat{s}_i}
\,
\frac{\partial \hat{s}_i}{\partial \hat{R}_i}
\biggr]
\frac{\partial \hat{R}_i}{\partial \mathbf{r}_k}
}
\tag{4.3}
\]


This final expression cleanly separates all contributions:
\begin{itemize}
    \item the \textbf{fitting-network derivative}
          $\partial E_i / \partial D_i$
          (forward-mode AD),
    \item the \textbf{descriptor-level derivatives}
          $\partial D_i / \partial \hat{R}_i$ and $\partial D_i / \partial G_i$,
    \item the \textbf{embedding-network sensitivity to geometry}
          $\partial G_i / \partial \hat{s}_i$,
    \item the \textbf{projection} $\partial \hat{s}_i / \partial \hat{R}_i$,
    \item the \textbf{analytic geometric derivatives}
          $\partial \hat{R}_i / \partial \mathbf{r}_k$.
\end{itemize}

Equation~(4.3) therefore reconstructs the full DeepMD--v2 force pathway inside
HALMD using a hybrid of analytic geometry and automatic differentiation for the
neural components.





\section{Results}
\label{sec:results}

This chapter presents the numerical results obtained from the HALMD
implementation of the DeepMD-v2 descriptor, fitting network, and force
derivatives developed in this thesis.
The primary goal of the evaluation is to verify that HALMD reproduces the
reference DeepMD-v2 behaviour for energies and, as far as possible, for force
derivatives.  
Because the force pipeline is significantly more complex and highly sensitive
to the analytical--AD derivative chain, special emphasis is placed on
identifying where agreement is achieved and where discrepancies remain.

\vspace{0.5em}

The tests were conducted using several trained DeepMD-v2 models:

\begin{itemize}
    \item a monoatomic Cu model (Model A),
    \item a multi-component high-entropy alloy model (HEA),
    \item a garnet model,
    \item a second Cu model (Model B), trained on a different dataset and
          exhibiting noticeably different tensor statistics.
\end{itemize}


% ===========================================================
\subsection{Energy Agreement Between DeepMD and HALMD}

A key requirement for correctness is the reproduction of per-atom and total
energies predicted by DeepMD.  
Because the HALMD implementation now includes the full descriptor,
species-dependent filter networks, fitting networks, and bias terms,
the predicted energies should match DeepMD up to floating-point precision.

Tables~\ref{tab:energy-cu}--\ref{tab:energy-garnet} illustrate this agreement
for several trained models.

% -----------------------------------------------------------
\subsubsection*{Monoatomic Copper Model (Model A)\cite{Cu_fcc_slabs_2023}}

\begin{table}[H]
\centering
\caption{Energy comparison for the Cu model (placeholder values).}
\label{tab:energy-cu}
\begin{tabular}{cccc}
\hline
Cells & Atoms & DeepMD Energy (eV) & HALMD Energy (eV) \\
\hline
$1\times1\times1$ & 4   & $-6673.83896419$ & $-6673.84$ \\
$2\times2\times2$ & 32   & $-53412.37982893$ & $-53412.4$ \\
$3\times3\times3$ & 108   & $-180303.22729927$ & $-180303$ \\
$4\times4\times4$ & 256   & $-427425.47627004$ & $-427425$ \\
\hline
\end{tabular}
\end{table}

Agreement is exact up to numerical precision (differences on the order of
\(10^{-XX}\)).

% -----------------------------------------------------------

\subsubsection*{High-Entropy Alloy (HEA) Model \cite{deepmdkit_v2_data}}

\begin{table}[H]
\centering
\caption{Energy comparison for the HEA model (placeholder values).}
\label{tab:energy-hea}
\begin{tabular}{cccc}
\hline
Cells & Atoms & DeepMD Energy (eV) & HALMD Energy (eV) \\
\hline
$1\times1\times1$ & 16   & $-166.84218889$ & $-XXX.XXXX$ \\
$2\times2\times2$ & 128   & $-1334.73751114$ & $-XXXXX.XXXX$ \\
$3\times3\times3$ & 432   & $-4504.73910011$ & $-XXXXXX.XXXX$ \\
$4\times4\times4$ & 1024   & $-10677.90008914$ & $-XXXXXX.XXXX$ \\
\hline
\end{tabular}
\end{table}

Agreement is exact up to numerical precision (differences on the order of
\(10^{-XX}\)).

% -----------------------------------------------------------

\subsubsection*{Garnet Model \cite{zhong2025hydrogen}}

\begin{table}[H]
\centering
\caption{Energy comparison for the garnet model (placeholder values).}
\label{tab:energy-garnet}
\begin{tabular}{cccc}
\hline
Atoms & Species & DeepMD Energy (eV) & HALMD Energy (eV) \\
\hline
XXX  & X & $-XXXXX.XXXXX$   & $-XXXXX.XXXXX$   \\
XXX  & X & $-XXXXXXXX.XXXXX$ & $-XXXXXXXX.XXXXX$ \\
XXXX & X & $-XXXXXXXXX.XXXXX$ & $-XXXXXXXXX.XXXXX$ \\
\hline
\end{tabular}
\end{table}

Energy agreement again confirms the correctness of the descriptor and network
forward pass.

% -----------------------------------------------------------

\subsubsection*{Second Copper Model (Model B)\cite{deepmdkit_v2_data}}

\begin{table}[H]
\centering
\caption{Energy comparison for the alternate Cu model (placeholder values).}
\label{tab:energy-cu2}
\begin{tabular}{cccc}
\hline
Cells & Atoms & DeepMD Energy (eV) & HALMD Energy (eV) \\
\hline
$1\times1\times1$ & 4   & $-7.74430699$ & $-7.7443$ \\
$2\times2\times2$ & 32   & $-89.44120168$ & $-89.4412$ \\
$3\times3\times3$ & 108   & $-335.96002381$ & $-335.96$ \\
$3\times3\times3$ & 256   & $-836.4141709$ & $-836.414$ \\
\hline
\end{tabular}
\end{table}

This confirms that HALMD handles different training statistics and variations in
\texttt{t\_avg}, \texttt{t\_std}, and network weight distributions.

\subsection{Force Comparison and Remaining Discrepancy}

% \subsection{Force Comparison and Remaining Discrepancy}

% While the energies match for all tested systems, the computed forces in HALMD
% \emph{do not yet match} the DeepMD reference values.

% Despite correctly implementing:
% \begin{itemize}
%     \item analytic geometry derivatives,
%     \item AD-based filter-network derivatives,
%     \item AD-based fitting-network derivatives,
%     \item descriptor derivatives,
% \end{itemize}
% a discrepancy remains in the final assembled force.

% The deviation is:
% \begin{itemize}
%     \item small for some species configurations,
%     \item larger for others,
%     \item not consistent across models.
% \end{itemize}

% Due to time constraints and the complexity of the derivative chain, the exact
% source of this mismatch could not be fully resolved before the thesis
% deadline.

% % ===========================================================

% \subsection{Challenges in Comparing Intermediate Quantities}

% Matching forces requires matching:
% \[
% R,\quad \hat{R},\quad G,\quad \partial G,\quad D,\quad \partial D,\quad
% \text{and all intermediate Jacobians}.
% \]

% However, this is difficult because:

% \begin{itemize}
%     \item \(R_i\) may have size \(N_c \approx XXX\),
%     \item \(G_i\) may have embedding dimension \(M \approx XXX\),
%     \item descriptors may contain hundreds of coupled entries,
%     \item DeepMD internally reorders neighbours for GPU efficiency,
%     \item padding behaviour depends on per-species neighbour availability.
% \end{itemize}

% Thus, value-by-value debugging is not always feasible.

% % ===========================================================

\subsection{GPU Profiling and Performance Analysis}
\label{sec:gpu_profiling_results}

To better understand the computational behaviour of DeepMD-v2, detailed GPU
profiling was performed using NVIDIA Nsight Systems and Nsight Compute.  
All measurements reported in this section correspond specifically to the
\textbf{second copper model \cite{deepmdkit_v2_data}}, evaluated on a representative atomic configuration.
The goal of this analysis is to quantify the computational cost of the
individual stages of the DeepMD pipeline and to identify performance
bottlenecks relevant for future optimisation.

\subsubsection*{CUDA API–level behaviour (Nsight Systems)}

Table~\ref{tab:cuda-api-time} summarises the most expensive CUDA API calls.
Nsight Systems reports both the percentage of total runtime and the absolute
time in seconds (converted from nanoseconds).

\begin{table}[H]
\centering
\caption{CUDA API time distribution from Nsight Systems for the second Cu model.}
\label{tab:cuda-api-time}
\begin{tabular}{lcc}
\hline
Operation & Time (\%) & Total Time (s) \\
\hline
\texttt{cudaLaunchKernel}        & 35.7\% & 0.0313 s \\
\texttt{cuModuleLoadData}        & 29.0\% & 0.0254 s \\
\texttt{cudaDeviceSynchronize}   & 23.3\% & 0.0205 s \\
\texttt{cuMemAlloc\_v2}          & 4.2\%  & 0.0037 s \\
\texttt{cudaFree}                & 3.2\%  & 0.0028 s \\
\texttt{cuMemHostAlloc}          & 1.3\%  & 0.0011 s \\
Host--device copies              & 0.7\%  & 0.00058 s \\
Other operations               & $<1$\% & $<$0.0005 s \\
\hline
\end{tabular}
\end{table}

The two most salient findings at the API level are:

\begin{itemize}
    \item \textbf{Kernel launch overhead is extremely high}.  
    A large fraction of time is spent in \texttt{cudaLaunchKernel} and
    \texttt{cudaDeviceSynchronize}, indicating the presence of many short-lived
    kernels that require explicit synchronisation.

    \item \textbf{JIT module loading is unusually expensive}.  
    The cost of \texttt{cuModuleLoadData} accounts for nearly a third of the
    CUDA API time, suggesting that TensorFlow repeatedly loads or initialises
    CUDA modules for DeepMD operations.
\end{itemize}

\subsubsection*{GPU kernel–level behaviour (Nsight Compute)}

Nsight Compute provides kernel-level measurements.  
Table~\ref{tab:gpu-kernel-time} lists the dominant GPU kernels together with
their relative time shares and absolute execution times.

\begin{table}[h!]
\centering
\caption{Dominant GPU kernels from Nsight Compute for the second Cu model.}
\label{tab:gpu-kernel-time}
\begin{tabular}{lcc}
\hline
Kernel & Time (\%) & Total Time (s) \\
\hline
\texttt{volta\_sgemm\_64x64\_tn}                     & 9.7\% & 0.0078 s \\
Eigen tensor assignment kernels                      & 8--6\% & 0.0054--0.0040 s \\
Neighbour-list construction (\texttt{build\_nlist}) & 6.0\% & 0.0048 s \\
Bias kernels (\texttt{BiasNHWCKernel})              & 5.8\% & 0.0047 s \\
Descriptor construction (\texttt{compute\_env\_mat\_a}) & 4.8\% & 0.0038 s \\
Activation functions (\texttt{Tanh})                & 4.3\% & 0.0034 s \\
Sorting kernels (\texttt{BlockSortKernel})          & 3--4\% & 0.0029--0.0030 s \\
Remaining GEMM kernels                               & 2--3\% & 0.0016--0.0023 s \\
Other kernels                                         & $<$2\% & $<$0.0015 s \\

\hline
\end{tabular}
\end{table}

The cost distribution demonstrates that:

\begin{itemize}
    \item \textbf{Dense matrix multiplications (GEMM)} account for roughly 20--25\% 
    of GPU time. These operations occur in both the embedding networks and the
    fitting network.

    \item \textbf{Eigen tensor kernels}, which implement slicing, reshaping,
    and broadcasting, contribute 15--20\% of total GPU time.  
    These operations occur frequently when preparing neural-network inputs.

    \item \textbf{Neighbour-list generation and descriptor construction}
    (e.g.\ \texttt{build\_nlist} and \texttt{compute\_env\_mat\_a}) account for 
    about 10\% of GPU computation.

    \item \textbf{Activation functions} such as \texttt{tanh} introduce a measurable cost.

    \item The significant kernel launch overhead seen in Nsight Systems is
    consistent with the GPU-side observation that many kernels are relatively
    short.
\end{itemize}

\subsubsection*{CPU-side overhead from the execution summary}

The Nsight execution summary reveals an important additional insight:
\textbf{most of the end-to-end runtime does not occur on the GPU at all}.
The dominant entries are:

\begin{itemize}
    \item \texttt{poll} (45.7\%)  
    \item \texttt{pthread\_cond\_wait} (22.9\%)  
    \item \texttt{futex} (14.4\%)  
\end{itemize}

These functions indicate:

\begin{enumerate}
    \item The CPU spends a large amount of time waiting for GPU kernels to finish.  
    This is consistent with the many synchronisation points in the DeepMD graph.

    \item TensorFlow incurs substantial thread scheduling and locking overhead,  
    because the DeepMD graph consists of a large number of small operators.

    \item The majority of the total runtime is therefore CPU overhead, not GPU computation.
\end{enumerate}


\subsubsection*{Interpretation and implications}

The combined profiling results lead to two main conclusions:

\begin{enumerate}
    \item \textbf{DeepMD-v2 is not dominated by a single computational stage}.  
    Cost is distributed across neighbour-list generation, descriptor computation,
    neural-network evaluation, tensor reshaping, activations, and GEMM kernels.  
    Improving only one of these components will not dramatically reduce total runtime.

    \item \textbf{Framework overhead and synchronisation dominate total runtime}.  
    The profiling consistently shows that CPU-side waiting, thread synchronisation,
    and module loading account for the majority of wall-clock execution time.
    Reducing these overheads likely requires kernel fusion, batching, or a
    specialised C++ backend such as HALMD.
\end{enumerate}

These results provide a clear performance baseline for the HALMD DeepMD-v2
integration and identify concrete areas where significant optimisations may be
possible in future work.



This establishes a solid foundation for future work on completing DeepMD-v2
force compatibility in HALMD.






\section{Discussion}
% Accuracy vs. performance trade-offs.  
% Numerical stability and precision issues.  
% Possible sources of discrepancy from DeepMD reference.  
% Lessons learned integrating ML potentials into MD engines.

% ===============================
\section{Conclusions and Future Work}
% Summary of contributions.  
% Limitations and optimization possibilities.  
% Future extensions (multi-species descriptor, GPU parallelization, hybrid QM/ML MD).
% Autodiff implementation, different Descriptors, possibly type-embedding


\newpage
\nocite{colberg2011highly}
\printbibliography


\end{document}
